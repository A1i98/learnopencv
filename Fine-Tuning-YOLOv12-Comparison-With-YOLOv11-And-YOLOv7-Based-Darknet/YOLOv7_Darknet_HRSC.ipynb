{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to keep this notebook just outside the built darknet directory\n",
    "\n",
    "import os\n",
    "!pip install kagglehub --quiet\n",
    "import kagglehub\n",
    "dataset_path = \"/present/working/directory/HRSC2016-MS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_dataset():\n",
    "    print(\"Downloading HRSC2016-MS dataset from Kaggle Hub...\")\n",
    "    path = kagglehub.dataset_download(\"weiming97/hrsc2016-ms-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "        os.system(f\"cp -r {path}/* {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the dataset xml's annotations and store them in .txt files\n",
    "\n",
    "import os\n",
    "!pip install tqdm\n",
    "!pip install pillow\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def make_yolo_dir(cwd):\n",
    "    yolo_dir = os.path.join(cwd, 'HRSC-YOLO')\n",
    "    os.mkdir(yolo_dir)\n",
    "    for i in ['train', 'val', 'test']:\n",
    "        folder = os.path.join(yolo_dir, i)\n",
    "        os.mkdir(folder)\n",
    "        for j in ['images', 'labels']:\n",
    "            subfolder = os.path.join(folder, j)\n",
    "            os.mkdir(subfolder)\n",
    "    return yolo_dir\n",
    "\n",
    "def xml2txt(xml_file_path, txt_file, w, h):\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    objs = root.findall('object')\n",
    "\n",
    "    if not objs:\n",
    "        print(f\"Warning: No objects found in {xml_file_path}. Creating an empty annotation file.\")\n",
    "        with open(txt_file, 'w') as f:\n",
    "            f.write(\"\\n\")  # Empty file for compatibility\n",
    "        return\n",
    "\n",
    "    with open(txt_file, 'w') as f:\n",
    "        for obj in objs:\n",
    "            try:\n",
    "                xmin = int(obj.find('bndbox/xmin').text)\n",
    "                ymin = int(obj.find('bndbox/ymin').text)\n",
    "                xmax = int(obj.find('bndbox/xmax').text)\n",
    "                ymax = int(obj.find('bndbox/ymax').text)\n",
    "\n",
    "                # Convert to YOLO format (normalized values)\n",
    "                x_center = (xmin + xmax) / (2.0 * w)\n",
    "                y_center = (ymin + ymax) / (2.0 * h)\n",
    "                box_w = (xmax - xmin) / w\n",
    "                box_h = (ymax - ymin) / h\n",
    "\n",
    "                # Save in YOLO format\n",
    "                f.write(f\"0 {x_center:.6f} {y_center:.6f} {box_w:.6f} {box_h:.6f}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {xml_file_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def read_list_file(cwd):\n",
    "    with open(os.path.join(cwd, 'ImageSets/train.txt'), 'r') as f:\n",
    "        train_list = f.read().splitlines()\n",
    "    with open(os.path.join(cwd, 'ImageSets/val.txt'), 'r') as f:\n",
    "        val_list = f.read().splitlines()\n",
    "    with open(os.path.join(cwd, 'ImageSets/test.txt'), 'r') as f:\n",
    "        test_list = f.read().splitlines()\n",
    "    return train_list, val_list, test_list\n",
    "\n",
    "def construct_path(file_name, cwd, yolo_dir, mode):\n",
    "    if mode == 'train':\n",
    "        train_file_path = os.path.join(cwd, f'AllImages/{file_name}.bmp')\n",
    "        train_xml_path = os.path.join(cwd, f'Annotations/{file_name}.xml')\n",
    "        save_txt_path = os.path.join(yolo_dir, f'train/labels/{file_name}.txt')\n",
    "        save_png_path = os.path.join(yolo_dir, f'train/images/{file_name}.png')\n",
    "    elif mode == 'val':\n",
    "        train_file_path = os.path.join(cwd, f'AllImages/{file_name}.bmp')\n",
    "        train_xml_path = os.path.join(cwd, f'Annotations/{file_name}.xml')\n",
    "        save_txt_path = os.path.join(yolo_dir, f'val/labels/{file_name}.txt')\n",
    "        save_png_path = os.path.join(yolo_dir, f'val/images/{file_name}.png')\n",
    "    elif mode == 'test':\n",
    "        train_file_path = os.path.join(cwd, f'AllImages/{file_name}.bmp')\n",
    "        train_xml_path = os.path.join(cwd, f'Annotations/{file_name}.xml')\n",
    "        save_txt_path = os.path.join(yolo_dir, f'test/labels/{file_name}.txt')\n",
    "        save_png_path = os.path.join(yolo_dir, f'test/images/{file_name}.png')\n",
    "    else:\n",
    "        print(f\"Unrecognized mode {mode}!\")\n",
    "    return train_file_path, train_xml_path, save_txt_path, save_png_path\n",
    "\n",
    "\n",
    "def parser():\n",
    "    cwd = os.getcwd()\n",
    "    yolo_dir = make_yolo_dir(cwd)\n",
    "\n",
    "    #yolo_dir = \"/content/HRSC-YOLO\"\n",
    "    train_list, val_list, test_list = read_list_file(\"/home/opencvuniv/Work/Shubham/install_darknet/HRSC2016-MS/\")\n",
    "\n",
    "    for train_file in tqdm(train_list):\n",
    "        bmp_path, xml_path, txt_path, png_path = construct_path(train_file, \"/home/opencvuniv/Work/Shubham/install_darknet/HRSC2016-MS/\", yolo_dir, 'train')\n",
    "\n",
    "        img = Image.open(bmp_path)\n",
    "        w, h = img.size\n",
    "        xml2txt(xml_path, txt_path, w, h)\n",
    "\n",
    "        img.save(png_path, format='PNG')\n",
    "\n",
    "    for val_file in tqdm(val_list):\n",
    "        bmp_path, xml_path, txt_path, png_path = construct_path(val_file, \"/home/opencvuniv/Work/Shubham/install_darknet/HRSC2016-MS/\", yolo_dir, 'val')\n",
    "\n",
    "        img = Image.open(bmp_path)\n",
    "        w, h = img.size\n",
    "        xml2txt(xml_path, txt_path, w, h)\n",
    "\n",
    "        img.save(png_path, format='PNG')\n",
    "\n",
    "    for test_file in tqdm(test_list):\n",
    "        bmp_path, xml_path, txt_path, png_path = construct_path(test_file, \"/home/opencvuniv/Work/Shubham/install_darknet/HRSC2016-MS/\", yolo_dir, 'test')\n",
    "\n",
    "        img = Image.open(bmp_path)\n",
    "        w, h = img.size\n",
    "        xml2txt(xml_path, txt_path, w, h)\n",
    "\n",
    "        img.save(png_path, format='PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "source_dir = \"/present/working/directory/HRSC-YOLO/\"\n",
    "dest_dir = \"/present/working/directory/experimentals/\"\n",
    "\n",
    "# Create the 'experimentals' directory with 'images' and 'labels' subdirectories\n",
    "\n",
    "os.makedirs('experimentals', exist_ok=True)\n",
    "os.makedirs(os.path.join(dest_dir, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(dest_dir, \"labels\"), exist_ok=True)\n",
    "\n",
    "# Define dataset splits\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "# Copy all images and labels from HRSC-YOLO to experimentals\n",
    "def copy_files():\n",
    "    for split in splits:\n",
    "        image_source = os.path.join(source_dir, split, \"images\")\n",
    "        label_source = os.path.join(source_dir, split, \"labels\")\n",
    "        \n",
    "        for file in os.listdir(image_source):\n",
    "            src_path = os.path.join(image_source, file)\n",
    "            dest_path = os.path.join(dest_dir, \"images\", file)\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "        \n",
    "        for file in os.listdir(label_source):\n",
    "            src_path = os.path.join(label_source, file)\n",
    "            dest_path = os.path.join(dest_dir, \"labels\", file)\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "\n",
    "copy_files()\n",
    "print(\"All images and labels copied successfully to experimentals/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = \"/present/working/directory/experimentals\"\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "labels_dir = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "# Output directories\n",
    "output_dirs = {\n",
    "    \"train\": \"train\",\n",
    "    \"val\": \"val\",\n",
    "    \"test\": \"test\"\n",
    "}\n",
    "\n",
    "# Create train, val, and test directories with images and labels subfolders\n",
    "for split in output_dirs.values():\n",
    "    os.makedirs(os.path.join(split, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(split, \"labels\"), exist_ok=True)\n",
    "\n",
    "# Get all image filenames\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(\".png\")]\n",
    "random.shuffle(image_files)  # Shuffle dataset\n",
    "\n",
    "# Define split sizes\n",
    "train_size, val_size, test_size = 610, 460, 610\n",
    "\n",
    "# Split dataset\n",
    "train_images = image_files[:train_size]\n",
    "val_images = image_files[train_size:train_size + val_size]\n",
    "test_images = image_files[train_size + val_size:train_size + val_size + test_size]\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files2(image_list, split):\n",
    "    for img_name in image_list:\n",
    "        label_name = img_name.replace(\".png\", \".txt\")  # Corresponding label\n",
    "        shutil.copy(os.path.join(images_dir, img_name), os.path.join(split, \"images\", img_name))\n",
    "        shutil.copy(os.path.join(labels_dir, label_name), os.path.join(split, \"labels\", label_name))\n",
    "\n",
    "# Copy files to respective folders\n",
    "copy_files2(train_images, \"train\")\n",
    "copy_files2(val_images, \"val\")\n",
    "copy_files2(test_images, \"test\")\n",
    "\n",
    "print(\"Dataset split and copied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_root = \"/present/working/directory\"\n",
    "dest_root = \"present/working/directory\"\n",
    "\n",
    "# for creating text files having images and their corresponding labels together to be used by the Darknet\n",
    "def copy_files3(source_root, dest_root):\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        src_image_dir = os.path.join(source_root, split, 'images')\n",
    "        src_label_dir = os.path.join(source_root, split, 'labels')\n",
    "\n",
    "        dest_dir = os.path.join(dest_root, f\"{split}2\")\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(src_image_dir) or not os.path.exists(src_label_dir):\n",
    "            print(f\"Skipping missing directory: {src_image_dir} or {src_label_dir}\")\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(src_image_dir):\n",
    "            src_image_path = os.path.join(src_image_dir, filename)\n",
    "            src_label_path = os.path.join(src_label_dir, filename.replace('.png', '.txt'))\n",
    "\n",
    "            if os.path.isfile(src_image_path):\n",
    "                shutil.copy(src_image_path, dest_dir)\n",
    "            if os.path.isfile(src_label_path):\n",
    "                shutil.copy(src_label_path, dest_dir)\n",
    "\n",
    "copy_files3(source_root, dest_root)\n",
    "print(\"Copying completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /present/working/directory/darknet/\n",
    "!wget https://github.com/AlexeyAB/darknet/releases/download/yolov4/yolov7x.conv.147\n",
    "!wget https://github.com/AlexeyAB/darknet/releases/download/yolov4/yolov7x.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prepare_darknet_image_txt_paths.py\n",
    "import os\n",
    "\n",
    "DATA_ROOT_TRAIN = os.path.join(\n",
    "    '/directory/path/just/outside/the/built/darknet', 'train2'\n",
    ")\n",
    "DATA_ROOT_VALID = os.path.join(\n",
    "    '/directory/path/just/outside/the/built/darknet', 'val2'\n",
    ")\n",
    "\n",
    "DATA_ROOT_TEST = os.path.join(\n",
    "    '/directory/path/just/outside/the/built/darknet', 'test2'\n",
    ")\n",
    "\n",
    "train_image_files_names = os.listdir(os.path.join(DATA_ROOT_TRAIN))\n",
    "with open('train.txt', 'w') as f:\n",
    "    for file_name in train_image_files_names:\n",
    "        if not '.txt' in file_name:\n",
    "            write_name = os.path.join(DATA_ROOT_TRAIN, file_name)\n",
    "            f.writelines(write_name+'\\n')\n",
    "\n",
    "valid_data_files__names = os.listdir(os.path.join(DATA_ROOT_VALID))\n",
    "with open('valid.txt', 'w') as f:\n",
    "    for file_name in valid_data_files__names:\n",
    "        if not '.txt' in file_name:\n",
    "            write_name = os.path.join(DATA_ROOT_VALID, file_name)\n",
    "            f.writelines(write_name+'\\n')\n",
    "\n",
    "test_data_files__names = os.listdir(os.path.join(DATA_ROOT_TEST))\n",
    "with open('test.txt', 'w') as f:\n",
    "    for file_name in test_data_files__names:\n",
    "        if not '.txt' in file_name:\n",
    "            write_name = os.path.join(DATA_ROOT_TEST, file_name)\n",
    "            f.writelines(write_name+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_darknet_image_txt_paths.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cfg/yolov7-darknet-hrsc2016-ms.cfg\n",
    "[net]\n",
    "# Testing\n",
    "#batch=64\n",
    "#subdivisions=64\n",
    "# Training\n",
    "batch=64\n",
    "subdivisions=64\n",
    "width=640\n",
    "height=640\n",
    "channels=3\n",
    "momentum=0.9\n",
    "decay=0.0005\n",
    "angle=0\n",
    "saturation = 1.5\n",
    "exposure = 1.5\n",
    "hue=.1\n",
    "\n",
    "learning_rate=0.00261\n",
    "burn_in=1000\n",
    "\n",
    "max_batches = 2000\n",
    "policy=steps\n",
    "steps=1600,1800\n",
    "scales=.1,.1\n",
    "\n",
    "\n",
    "# 0\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=40\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "# 1\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=80\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=80\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "# 3\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 14\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "[maxpool]\n",
    "size=2\n",
    "stride=2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-3\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "# 20\n",
    "[route]\n",
    "layers = -1,-4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 31\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "[maxpool]\n",
    "size=2\n",
    "stride=2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-3\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "# 37\n",
    "[route]\n",
    "layers = -1,-4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 48\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1280\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "[maxpool]\n",
    "size=2\n",
    "stride=2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-3\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "# 54\n",
    "[route]\n",
    "layers = -1,-4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 65\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1280\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "##################################\n",
    "\n",
    "### SPPCSP ###\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=640\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "### SPP ###\n",
    "[maxpool]\n",
    "stride=1\n",
    "size=5\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[maxpool]\n",
    "stride=1\n",
    "size=9\n",
    "\n",
    "[route]\n",
    "layers=-4\n",
    "\n",
    "[maxpool]\n",
    "stride=1\n",
    "size=13\n",
    "\n",
    "[route]\n",
    "layers=-6,-5,-3,-1\n",
    "### End SPP ###\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=640\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1, -13\n",
    "\n",
    "# 80\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[upsample]\n",
    "stride=2\n",
    "\n",
    "[route]\n",
    "layers = 48\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 96\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[upsample]\n",
    "stride=2\n",
    "\n",
    "[route]\n",
    "layers = 31\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 112\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "[maxpool]\n",
    "size=2\n",
    "stride=2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-3\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=160\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-4,96\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 129\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "\n",
    "[maxpool]\n",
    "size=2\n",
    "stride=2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-3\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=320\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-4,80\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers=-2\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "[route]\n",
    "layers = -1,-3,-5,-7,-9\n",
    "\n",
    "# 146\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=640\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=swish\n",
    "\n",
    "#############################\n",
    "\n",
    "# ============ End of Neck ============ #\n",
    "\n",
    "# ============ Head ============ #\n",
    "\n",
    "\n",
    "# P3\n",
    "[route]\n",
    "layers = 112\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=320\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=18\n",
    "#activation=linear\n",
    "activation=logistic\n",
    "\n",
    "[yolo]\n",
    "mask = 0,1,2\n",
    "anchors = 12,16, 19,36, 40,28, 36,75, 76,55, 72,146, 142,110, 192,243, 459,401\n",
    "classes=1\n",
    "num=9\n",
    "jitter=.1\n",
    "scale_x_y = 2.0\n",
    "objectness_smooth=1\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "#random=1\n",
    "resize=1.5\n",
    "iou_thresh=0.2\n",
    "iou_normalizer=0.05\n",
    "cls_normalizer=0.5\n",
    "obj_normalizer=1.0\n",
    "iou_loss=ciou\n",
    "nms_kind=diounms\n",
    "beta_nms=0.6\n",
    "new_coords=1\n",
    "max_delta=2\n",
    "\n",
    "\n",
    "# P4\n",
    "[route]\n",
    "layers = 129\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=640\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=18\n",
    "#activation=linear\n",
    "activation=logistic\n",
    "\n",
    "[yolo]\n",
    "mask = 3,4,5\n",
    "anchors = 12,16, 19,36, 40,28, 36,75, 76,55, 72,146, 142,110, 192,243, 459,401\n",
    "classes=1\n",
    "num=9\n",
    "jitter=.1\n",
    "scale_x_y = 2.0\n",
    "objectness_smooth=1\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "#random=1\n",
    "resize=1.5\n",
    "iou_thresh=0.2\n",
    "iou_normalizer=0.05\n",
    "cls_normalizer=0.5\n",
    "obj_normalizer=1.0\n",
    "iou_loss=ciou\n",
    "nms_kind=diounms\n",
    "beta_nms=0.6\n",
    "new_coords=1\n",
    "max_delta=2\n",
    "\n",
    "\n",
    "# P5\n",
    "[route]\n",
    "layers = 146\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=1280\n",
    "activation=swish\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=18\n",
    "#activation=linear\n",
    "activation=logistic\n",
    "\n",
    "[yolo]\n",
    "mask = 6,7,8\n",
    "anchors = 12,16, 19,36, 40,28, 36,75, 76,55, 72,146, 142,110, 192,243, 459,401\n",
    "classes=1\n",
    "num=9\n",
    "jitter=.1\n",
    "scale_x_y = 2.0\n",
    "objectness_smooth=1\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "#random=1\n",
    "resize=1.5\n",
    "iou_thresh=0.2\n",
    "iou_normalizer=0.05\n",
    "cls_normalizer=0.5\n",
    "obj_normalizer=1.0\n",
    "iou_loss=ciou\n",
    "nms_kind=diounms\n",
    "beta_nms=0.6\n",
    "new_coords=1\n",
    "max_delta=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build/darknet/x64/data/ship.names\n",
    "ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build/darknet/x64/data/hrsc2016-ms-yolov7.data\n",
    "classes = 1\n",
    "train  = train.txt\n",
    "valid  = valid.txt\n",
    "names = build/darknet/x64/data/ship.names\n",
    "backup = backup_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build/darknet/x64/data/ship_test.data\n",
    "classes = 1\n",
    "train  = train.txt\n",
    "valid  = test.txt\n",
    "names = build/darknet/x64/data/ship.names\n",
    "backup = backup_test_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('backup_2000', exist_ok=True)\n",
    "print('Backup directory created for YOLOv7-based Darknet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('backup_test_2000', exist_ok=True)\n",
    "print('Test Backup directory created for YOLOv7-based Darknet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the command in terminal to be opened in the built darknet directory itself\n",
    "# to start the darknet training process\n",
    "\n",
    "./darknet detector train build/darknet/x64/data/hrsc2016-ms-yolov7.data cfg/yolov7-darknet-hrsc2016-ms.cfg yolov7x.conv.147 -map -dont_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the command in terminal to be opened in the built darknet directory itself\n",
    "# to get the mAP scores at various IoU thresholds\n",
    "\n",
    "!./darknet detector map build/darknet/x64/data/ship_test.data cfg/yolov7-darknet-hrsc2016-ms.cfg backup_2000/yolov7-hrsc2016-ms-final.weights\n",
    "!./darknet detector map build/darknet/x64/data/ship_test.data cfg/yolov7-darknet-hrsc2016-ms.cfg backup_2000/yolov7-hrsc2016-ms-final.weights -iou_thresh 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('backup_2000', exist_ok=True)\n",
    "print('Directory to store detections on test images has been created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add below 2 code lines in darknet_images.py script and then run the below command in terminal to see detections on test images\n",
    "# place where to add these two lines has been shown in the article\n",
    "\n",
    "output_image_path = os.path.join(\"yolov7_darknet_test_results\", image_name.split(os.path.sep)[-1])\n",
    "cv2.imwrite(output_image_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the detections on the test images\n",
    "\n",
    "# we can visualize darknet detections on train as well as validation images, we just need to\n",
    "# change the path of the images directory in the below command as well as we need to create \n",
    "# separate directories as shown in the last third command\n",
    "\n",
    "# command to be executed in terminal\n",
    "\n",
    "python darknet_images.py --data_file build/darknet/x64/data/ship_test.data --input /path/to/the/directory/containing/train/images --config_file cfg/yolov7-darknet-hrsc2016-ms.cfg --weights backup_2000/yolov7-darknet-hrsc2016-ms_final.weights --thresh 0.25 --ext_output --save_labels --dont_show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the darknet detections as bounding boxes values\n",
    "\n",
    "./darknet detector test build/darknet/x64/data/hrsc2016-ms-yolov7.data cfg/yolov7-darknet-hrsc2016-ms.cfg backup_2000/yolov7-darknet-hrsc2016-ms_final.weights -dont_show -ext_output < test.txt > results_darknet_test.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veb_pytorchx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
