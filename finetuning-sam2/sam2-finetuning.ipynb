{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXa3Fh8m9koo",
    "outputId": "95e58637-e194-4186-f949-d7658ace61dd"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/segment-anything-2\n",
    "%cd segment-anything-2\n",
    "!pip install -q -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "V25AqEhi-pVr",
    "outputId": "c696a23d-cafb-4b5a-918e-42b6f9456738"
   },
   "outputs": [],
   "source": [
    "# get dataset from Kaggle\n",
    "from google.colab import files\n",
    "files.upload()  # This will prompt you to upload the kaggle.json file\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d ankanghosh651/leaf-sengmentation-dataset-sam2-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get install zip unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrNLY01j_tFn",
    "outputId": "260f2691-a8f1-471c-8a55-73daed6e4f0d"
   },
   "outputs": [],
   "source": [
    "!unzip leaf-sengmentation-dataset-sam2-format.zip -d ./leaf-seg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VIhhTetADIF",
    "outputId": "4f2c5ad7-4a82-4653-a70e-b3b3b5cb466b"
   },
   "outputs": [],
   "source": [
    "!wget -O sam2_hiera_tiny.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\"\n",
    "!wget -O sam2_hiera_small.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt\"\n",
    "!wget -O sam2_hiera_base_plus.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\"\n",
    "!wget -O sam2_hiera_large.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd segment-anything-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds():\n",
    "    SEED_VALUE = 42\n",
    "    random.seed(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED_VALUE)\n",
    "        torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "        torch.backends.cudnn.benchmark=True\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmXIJFc8A9px",
    "outputId": "4a55607f-f0c6-48f8-c2f0-4c7c389c9fdd"
   },
   "outputs": [],
   "source": [
    "# Path to the chest-ct-segmentation dataset folder\n",
    "data_dir = \"../leaf-seg/leaf-seg\"\n",
    "images_dir = os.path.join(data_dir, \"images\")\n",
    "masks_dir = os.path.join(data_dir, \"masks\")\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "\n",
    "# Split the data into two halves: one for training and one for testing\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the training data list\n",
    "train_data = []\n",
    "for index, row in train_df.iterrows():\n",
    "   image_name = row['imageid']\n",
    "   mask_name = row['maskid']\n",
    "\n",
    "   # Append image and corresponding mask paths\n",
    "   train_data.append({\n",
    "       \"image\": os.path.join(images_dir, image_name),\n",
    "       \"annotation\": os.path.join(masks_dir, mask_name)\n",
    "   })\n",
    "\n",
    "# Prepare the testing data list (if needed for inference or evaluation later)\n",
    "test_data = []\n",
    "for index, row in test_df.iterrows():\n",
    "   image_name = row['imageid']\n",
    "   mask_name = row['maskid']\n",
    "\n",
    "   # Append image and corresponding mask paths\n",
    "   test_data.append({\n",
    "       \"image\": os.path.join(images_dir, image_name),\n",
    "       \"annotation\": os.path.join(masks_dir, mask_name)\n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "PPYF32R3CzRZ",
    "outputId": "213ef57d-6db6-4cbb-eb6e-abd2269a0cd8"
   },
   "outputs": [],
   "source": [
    "def read_batch(data, visualize_data=True):\n",
    "   # Select a random entry\n",
    "   ent = data[np.random.randint(len(data))]\n",
    "\n",
    "   # Get full paths\n",
    "   Img = cv2.imread(ent[\"image\"])[..., ::-1]  # Convert BGR to RGB\n",
    "   ann_map = cv2.imread(ent[\"annotation\"], cv2.IMREAD_GRAYSCALE)  # Read annotation as grayscale\n",
    "\n",
    "   if Img is None or ann_map is None:\n",
    "       print(f\"Error: Could not read image or mask from path {ent['image']} or {ent['annotation']}\")\n",
    "       return None, None, None, 0\n",
    "\n",
    "   # Resize image and mask\n",
    "   r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]])  # Scaling factor\n",
    "   Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "   ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "   ### Continuation of read_batch() ###\n",
    "\n",
    "   # Initialize a single binary mask\n",
    "   binary_mask = np.zeros_like(ann_map, dtype=np.uint8)\n",
    "   points = []\n",
    "\n",
    "   # Get binary masks and combine them into a single mask\n",
    "   inds = np.unique(ann_map)[1:]  # Skip the background (index 0)\n",
    "   for ind in inds:\n",
    "       mask = (ann_map == ind).astype(np.uint8)  # Create binary mask for each unique index\n",
    "       binary_mask = np.maximum(binary_mask, mask)  # Combine with the existing binary mask\n",
    "\n",
    "   # Erode the combined binary mask to avoid boundary points\n",
    "   eroded_mask = cv2.erode(binary_mask, np.ones((5, 5), np.uint8), iterations=1)\n",
    "\n",
    "   # Get all coordinates inside the eroded mask and choose a random point\n",
    "   coords = np.argwhere(eroded_mask > 0)\n",
    "   if len(coords) > 0:\n",
    "       for _ in inds:  # Select as many points as there are unique labels\n",
    "           yx = np.array(coords[np.random.randint(len(coords))])\n",
    "           points.append([yx[1], yx[0]])  # Corrected order for y, x\n",
    "\n",
    "   points = np.array(points)\n",
    "\n",
    "   ### Continuation of read_batch() ###\n",
    "\n",
    "   if visualize_data:\n",
    "       # Plotting the images and points\n",
    "       plt.figure(figsize=(15, 5))\n",
    "\n",
    "       # Original Image\n",
    "       plt.subplot(1, 3, 1)\n",
    "       plt.title('Original Image')\n",
    "       plt.imshow(Img)\n",
    "       plt.axis('off')\n",
    "\n",
    "       # Segmentation Mask (binary_mask)\n",
    "       plt.subplot(1, 3, 2)\n",
    "       plt.title('Binarized Mask')\n",
    "       plt.imshow(binary_mask, cmap='gray')\n",
    "       plt.axis('off')\n",
    "\n",
    "       # Mask with Points in Different Colors\n",
    "       plt.subplot(1, 3, 3)\n",
    "       plt.title('Binarized Mask with Points')\n",
    "       plt.imshow(binary_mask, cmap='gray')\n",
    "\n",
    "       # Plot points in different colors\n",
    "       colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "       for i, point in enumerate(points):\n",
    "           plt.scatter(point[0], point[1], c=colors[i % len(colors)], s=100, label=f'Point {i+1}')  # Corrected to plot y, x order\n",
    "\n",
    "       # plt.legend()\n",
    "       plt.axis('off')\n",
    "\n",
    "       plt.tight_layout()\n",
    "       plt.show()\n",
    "\n",
    "   binary_mask = np.expand_dims(binary_mask, axis=-1)  # Now shape is (1024, 1024, 1)\n",
    "   binary_mask = binary_mask.transpose((2, 0, 1))\n",
    "   points = np.expand_dims(points, axis=1)\n",
    "\n",
    "   # Return the image, binarized mask, points, and number of masks\n",
    "   return Img, binary_mask, points, len(inds)\n",
    "\n",
    "# Visualize the data\n",
    "Img1, masks1, points1, num_masks = read_batch(train_data, visualize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdfWoSGQHBuU"
   },
   "outputs": [],
   "source": [
    "sam2_checkpoint = \"../sam2_hiera_tiny.pt\"  # @param [\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"]\n",
    "model_cfg = \"sam2_hiera_t.yaml\" # @param [\"sam2_hiera_t.yaml\", \"sam2_hiera_s.yaml\", \"sam2_hiera_b+.yaml\", \"sam2_hiera_l.yaml\"]\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvoPrPjPHJVX",
    "outputId": "5dad0c10-677c-455f-908b-1d7a235e876f"
   },
   "outputs": [],
   "source": [
    "# Train mask decoder.\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "\n",
    "# Train prompt encoder.\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "\n",
    "# Mix precision.\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# No. of steps to train the model.\n",
    "NO_OF_STEPS = 6000 # @param\n",
    "\n",
    "# Fine-tuned model name.\n",
    "FINE_TUNED_MODEL_NAME = \"fine_tuned_sam2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "WRIKeKk4HQji",
    "outputId": "3838ddae-b78e-4a81-b68c-1fe1333f317f"
   },
   "outputs": [],
   "source": [
    "# Configure optimizer.\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-4, weight_decay = 1e-4) #1e-5, weight_decay = 4e-5\n",
    "\n",
    "# Initialize scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.6) # 500 , 250, gamma = 0.1\n",
    "\n",
    "accumulation_steps = 4  # Number of steps to accumulate gradients before updating\n",
    "# mean_iou = 0  # Start with a reasonable value\n",
    "\n",
    "for step in range(1, NO_OF_STEPS + 1):\n",
    "   with torch.amp.autocast(device_type='cuda'):\n",
    "       image, mask, input_point, num_masks = read_batch(train_data, visualize_data=False)\n",
    "       if image is None or mask is None or num_masks == 0:\n",
    "           continue\n",
    "\n",
    "       input_label = np.ones((num_masks, 1))\n",
    "       if not isinstance(input_point, np.ndarray) or not isinstance(input_label, np.ndarray):\n",
    "           continue\n",
    "\n",
    "       if input_point.size == 0 or input_label.size == 0:\n",
    "           continue\n",
    "\n",
    "       predictor.set_image(image)\n",
    "       mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n",
    "       if unnorm_coords is None or labels is None or unnorm_coords.shape[0] == 0 or labels.shape[0] == 0:\n",
    "           continue\n",
    "\n",
    "       sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "           points=(unnorm_coords, labels), boxes=None, masks=None,\n",
    "       )\n",
    "\n",
    "       batched_mode = unnorm_coords.shape[0] > 1\n",
    "       high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "       low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "           image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "           image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "           sparse_prompt_embeddings=sparse_embeddings,\n",
    "           dense_prompt_embeddings=dense_embeddings,\n",
    "           multimask_output=True,\n",
    "           repeat_image=batched_mode,\n",
    "           high_res_features=high_res_features,\n",
    "       )\n",
    "       prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "       gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n",
    "       prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "       \n",
    "       seg_loss = (-gt_mask * torch.log(prd_mask + 0.000001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n",
    "       # dice_loss = 1 - (2 * (gt_mask * prd_mask).sum() + 1e-6) / (gt_mask.sum() + prd_mask.sum() + 1e-6)\n",
    "       # loss = seg_loss + dice_loss\n",
    "\n",
    "       # seg_loss = F.binary_crossprd_mask_entropy(prd_mask, gt_mask)\n",
    "       inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "       iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "       score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "       loss = seg_loss + score_loss * 0.05\n",
    "\n",
    "       # Apply gradient accumulation\n",
    "       loss = loss / accumulation_steps\n",
    "       scaler.scale(loss).backward()\n",
    "\n",
    "       # Clip gradients\n",
    "       torch.nn.utils.clip_grad_norm_(predictor.model.parameters(), max_norm=1.0)\n",
    "\n",
    "       \n",
    "\n",
    "       if step % accumulation_steps == 0:\n",
    "           scaler.step(optimizer)\n",
    "           scaler.update()\n",
    "           scheduler.step()\n",
    "           predictor.model.zero_grad()\n",
    "           \n",
    "       # Update scheduler\n",
    "       # scheduler.step()\n",
    "\n",
    "       if step % 500 == 0:\n",
    "           FINE_TUNED_MODEL = FINE_TUNED_MODEL_NAME + \"_\" + str(step) + \".pt\"\n",
    "           torch.save(predictor.model.state_dict(), FINE_TUNED_MODEL)\n",
    "\n",
    "       if step == 1:\n",
    "           mean_iou = 0\n",
    "\n",
    "       mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "\n",
    "       # if step % 100 == 0:\n",
    "       #     print(\"Step \" + str(step) + \":\\t\", \"Accuracy (IoU) = \", mean_iou)\n",
    "           \n",
    "       current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "       \n",
    "       if step % 100 == 0:  # Log every 100 steps\n",
    "            print(f\"Step {step}: Current LR = {current_lr:.6f}, IoU = {mean_iou:.6f}, Seg Loss = {seg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVOlqMTEIssK"
   },
   "outputs": [],
   "source": [
    "def read_image(image_path, mask_path):  # read and resize image and mask\n",
    "   img = cv2.imread(image_path)[..., ::-1]  # Convert BGR to RGB\n",
    "   mask = cv2.imread(mask_path, 0)\n",
    "   r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])\n",
    "   img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))\n",
    "   mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "   return img, mask\n",
    "\n",
    "def get_points(mask, num_points):  # Sample points inside the input mask\n",
    "   points = []\n",
    "   coords = np.argwhere(mask > 0)\n",
    "   for i in range(num_points):\n",
    "       yx = np.array(coords[np.random.randint(len(coords))])\n",
    "       points.append([[yx[1], yx[0]]])\n",
    "   return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-ZWH8cENVQ5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Randomly select a test image from the test_data\n",
    "selected_entry = random.choice(test_data)\n",
    "print(selected_entry)\n",
    "image_path = selected_entry['image']\n",
    "mask_path = selected_entry['annotation']\n",
    "print(mask_path,'mask path')\n",
    "\n",
    "# Load the selected image and mask\n",
    "image, target_mask = read_image(image_path, mask_path)\n",
    "\n",
    "# Generate random points for the input\n",
    "num_samples = 30  # Number of points per segment to sample\n",
    "input_points = get_points(target_mask, num_samples)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "FINE_TUNED_MODEL_WEIGHTS = \"fine_tuned_sam2_4500.torch\"\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "\n",
    "# Build net and load weights\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "predictor.model.load_state_dict(torch.load(FINE_TUNED_MODEL_WEIGHTS))\n",
    "\n",
    "\n",
    "# Perform inference and predict masks\n",
    "with torch.no_grad():\n",
    "   predictor.set_image(image)\n",
    "   masks, scores, logits = predictor.predict(\n",
    "       point_coords=input_points,\n",
    "       point_labels=np.ones([input_points.shape[0], 1])\n",
    "   )\n",
    "\n",
    "# Process the predicted masks and sort by scores\n",
    "np_masks = np.array(masks[:, 0])\n",
    "np_scores = scores[:, 0]\n",
    "sorted_masks = np_masks[np.argsort(np_scores)][::-1]\n",
    "\n",
    "# Initialize segmentation map and occupancy mask\n",
    "seg_map = np.zeros_like(sorted_masks[0], dtype=np.uint8)\n",
    "occupancy_mask = np.zeros_like(sorted_masks[0], dtype=bool)\n",
    "\n",
    "# Combine masks to create the final segmentation map\n",
    "for i in range(sorted_masks.shape[0]):\n",
    "   mask = sorted_masks[i]\n",
    "   if (mask * occupancy_mask).sum() / mask.sum() > 0.15:\n",
    "       continue\n",
    "\n",
    "   mask_bool = mask.astype(bool)\n",
    "   mask_bool[occupancy_mask] = False  # Set overlapping areas to False in the mask\n",
    "   seg_map[mask_bool] = i + 1  # Use boolean mask to index seg_map\n",
    "   occupancy_mask[mask_bool] = True  # Update occupancy_mask\n",
    "\n",
    "# Visualization: Show the original image, mask, and final segmentation side by side\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Test Image')\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Original Mask')\n",
    "plt.imshow(target_mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Final Segmentation')\n",
    "plt.imshow(seg_map, cmap='jet')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
