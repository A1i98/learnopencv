{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bffdc053f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "MIN_WORD_FREQ = 1\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "IMAGES_DIR = \"flickr8k/Images\"\n",
    "TOKENS_FILE = \"flickr8k/captions.txt\"\n",
    "\n",
    "BEST_CHECKPOINT_PATH = \"best_checkpoint.pth\"  # Checkpoint w/ epoch, model, optimizer, best_val_loss\n",
    "FINAL_MODEL_PATH = \"final_model.pth\"          # Final model weights only (saved at the end)\n",
    "VOCAB_PATH = \"vocab.pkl\"                      # Where we save the vocabulary\n",
    "\n",
    "# Set this to True if you want to resume from the best checkpoint\n",
    "RESUME = False\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        # self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.itos = {0: \"pad\", 1: \"startofseq\", 2: \"endofseq\", 3: \"unk\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "        self.index = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer(self, text):\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r\"\\w+\", text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        for sentence in sentence_list:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            frequencies.update(tokens)\n",
    "\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.stoi[word] = self.index\n",
    "                self.itos[self.index] = word\n",
    "                self.index += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenizer(text)\n",
    "        numericalized = []\n",
    "        for token in tokens:\n",
    "            if token in self.stoi:\n",
    "                numericalized.append(self.stoi[token])\n",
    "            else:\n",
    "                numericalized.append(self.stoi[\"<unk>\"])\n",
    "        return numericalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_flickr_tokens(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with columns: image,caption\n",
    "    Returns dict: {image_filename: [captions]}\n",
    "    \"\"\"\n",
    "    imgid2captions = {}\n",
    "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        # Skip header row: \"image,caption\"\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "            img_id, caption = row[0], row[1]\n",
    "            if img_id not in imgid2captions:\n",
    "                imgid2captions[img_id] = []\n",
    "            imgid2captions[img_id].append(caption)\n",
    "    return imgid2captions\n",
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, imgid2captions, vocab, transform=None):\n",
    "        self.imgid2captions = []\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Flatten each (img_id, [cap1, cap2, ...]) into multiple examples\n",
    "        for img_id, caps in imgid2captions.items():\n",
    "            for c in caps:\n",
    "                self.imgid2captions.append((img_id, c))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgid2captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id, caption = self.imgid2captions[idx]\n",
    "        # print('CAPTION START...', caption, 'CAPTION END\\n')\n",
    "        img_path = os.path.join(IMAGES_DIR, img_id)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # with open('val_file.txt', 'a') as f:\n",
    "        #     f.writelines(f\"{img_path}: {caption}\\n\")\n",
    "\n",
    "        # Numericalize caption\n",
    "        # numerical_caption = [self.vocab.stoi[\"<start>\"]]\n",
    "        numerical_caption = [self.vocab.stoi[\"startofseq\"]]\n",
    "        numerical_caption += self.vocab.numericalize(caption)\n",
    "        numerical_caption.append(self.vocab.stoi[\"endofseq\"])\n",
    "\n",
    "        return image, torch.tensor(numerical_caption, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images = [item[0] for item in batch]\n",
    "    captions = [item[1] for item in batch]\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, padded_captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = True\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(embed_dim, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)  # (batch_size, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "        features = self.batch_norm(features)\n",
    "        return features\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # remove the last token for input\n",
    "        captions_in = captions[:, :-1]\n",
    "        emb = self.embedding(captions_in)\n",
    "        features = features.unsqueeze(1)\n",
    "        lstm_input = torch.cat((features, emb), dim=1)\n",
    "        outputs, _ = self.lstm(lstm_input)\n",
    "        logits = self.fc(outputs)\n",
    "        return logits\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, vocab_size, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    for images, captions, _lengths in progress_bar:\n",
    "        images = images.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, captions)\n",
    "        # outputs: (batch_size, seq_len, vocab_size)\n",
    "        # compare outputs[:, 1:, :] with captions[:, 1:]\n",
    "        outputs = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "        targets = captions[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "        # targets = captions.contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, dataloader, criterion, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions, _lengths in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            captions = captions.to(DEVICE)\n",
    "            outputs = model(images, captions)\n",
    "            outputs = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "            targets = captions[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "            # targets = captions.contiguous().view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    avg_val_loss = total_loss / len(dataloader)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary saved to: vocab.pkl\n",
      "Vocabulary size: 8492\n",
      "32,140,396 total parameters.\n",
      "32,140,396 training parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████| 506/506 [01:03<00:00,  8.03batch/s, loss=1.9518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Loss: 2.1531 | Val Loss: 1.7488\n",
      "New best model saved -> best_checkpoint.pth (val_loss=1.7488)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████| 506/506 [01:12<00:00,  6.96batch/s, loss=1.5735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/50] Train Loss: 1.5907 | Val Loss: 1.6098\n",
      "New best model saved -> best_checkpoint.pth (val_loss=1.6098)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████| 506/506 [01:29<00:00,  5.66batch/s, loss=1.5610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/50] Train Loss: 1.4014 | Val Loss: 1.5693\n",
      "New best model saved -> best_checkpoint.pth (val_loss=1.5693)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████| 506/506 [01:59<00:00,  4.23batch/s, loss=1.1057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/50] Train Loss: 1.2610 | Val Loss: 1.5548\n",
      "New best model saved -> best_checkpoint.pth (val_loss=1.5548)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████| 506/506 [02:27<00:00,  3.44batch/s, loss=1.1601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50] Train Loss: 1.1506 | Val Loss: 1.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████| 506/506 [02:39<00:00,  3.18batch/s, loss=0.9398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/50] Train Loss: 1.0472 | Val Loss: 1.5793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████| 506/506 [02:59<00:00,  2.82batch/s, loss=0.8513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50] Train Loss: 0.9527 | Val Loss: 1.5987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████| 506/506 [02:56<00:00,  2.87batch/s, loss=0.8883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50] Train Loss: 0.8673 | Val Loss: 1.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████| 506/506 [03:12<00:00,  2.63batch/s, loss=0.9457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50] Train Loss: 0.7909 | Val Loss: 1.6655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████| 506/506 [03:15<00:00,  2.59batch/s, loss=0.8881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/50] Train Loss: 0.7207 | Val Loss: 1.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|███████████████| 506/506 [01:43<00:00,  4.87batch/s, loss=0.8048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/50] Train Loss: 0.6612 | Val Loss: 1.7375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|███████████████| 506/506 [01:03<00:00,  7.91batch/s, loss=0.7152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/50] Train Loss: 0.6079 | Val Loss: 1.7777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|███████████████| 506/506 [01:04<00:00,  7.90batch/s, loss=0.7057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] Train Loss: 0.5552 | Val Loss: 1.8179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|███████████████| 506/506 [01:04<00:00,  7.90batch/s, loss=0.6347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50] Train Loss: 0.5104 | Val Loss: 1.8590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|███████████████| 506/506 [01:04<00:00,  7.90batch/s, loss=0.5567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/50] Train Loss: 0.4731 | Val Loss: 1.8958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|███████████████| 506/506 [01:04<00:00,  7.90batch/s, loss=0.5493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50] Train Loss: 0.4396 | Val Loss: 1.9452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|███████████████| 506/506 [01:04<00:00,  7.90batch/s, loss=0.5551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/50] Train Loss: 0.4119 | Val Loss: 1.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|███████████████| 506/506 [01:04<00:00,  7.89batch/s, loss=0.5444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/50] Train Loss: 0.3813 | Val Loss: 2.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|███████████████| 506/506 [01:04<00:00,  7.90batch/s, loss=0.3425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/50] Train Loss: 0.3577 | Val Loss: 2.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|███████████████| 506/506 [01:04<00:00,  7.89batch/s, loss=0.4681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/50] Train Loss: 0.3387 | Val Loss: 2.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|███████████████| 506/506 [01:04<00:00,  7.88batch/s, loss=0.4049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/50] Train Loss: 0.3210 | Val Loss: 2.1473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|███████████████| 506/506 [01:04<00:00,  7.88batch/s, loss=0.4092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/50] Train Loss: 0.3052 | Val Loss: 2.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|███████████████| 506/506 [01:04<00:00,  7.88batch/s, loss=0.3519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/50] Train Loss: 0.2885 | Val Loss: 2.2195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|███████████████| 506/506 [01:04<00:00,  7.88batch/s, loss=0.3367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/50] Train Loss: 0.2768 | Val Loss: 2.2561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|███████████████| 506/506 [01:04<00:00,  7.87batch/s, loss=0.3849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/50] Train Loss: 0.2663 | Val Loss: 2.2874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|███████████████| 506/506 [01:02<00:00,  8.04batch/s, loss=0.3225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/50] Train Loss: 0.2552 | Val Loss: 2.3186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|███████████████| 506/506 [01:04<00:00,  7.89batch/s, loss=0.2837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27/50] Train Loss: 0.2472 | Val Loss: 2.3476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|███████████████| 506/506 [01:04<00:00,  7.85batch/s, loss=0.3356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28/50] Train Loss: 0.2401 | Val Loss: 2.3801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|███████████████| 506/506 [01:02<00:00,  8.08batch/s, loss=0.2567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29/50] Train Loss: 0.2356 | Val Loss: 2.4041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|███████████████| 506/506 [01:02<00:00,  8.07batch/s, loss=0.2823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30/50] Train Loss: 0.2275 | Val Loss: 2.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|███████████████| 506/506 [01:02<00:00,  8.06batch/s, loss=0.3002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31/50] Train Loss: 0.2219 | Val Loss: 2.4667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|███████████████| 506/506 [01:02<00:00,  8.06batch/s, loss=0.3304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32/50] Train Loss: 0.2182 | Val Loss: 2.4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|███████████████| 506/506 [01:02<00:00,  8.04batch/s, loss=0.2663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33/50] Train Loss: 0.2128 | Val Loss: 2.5088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|███████████████| 506/506 [01:02<00:00,  8.04batch/s, loss=0.2344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34/50] Train Loss: 0.2075 | Val Loss: 2.5383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|███████████████| 506/506 [01:02<00:00,  8.07batch/s, loss=0.2358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/50] Train Loss: 0.2059 | Val Loss: 2.5546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|███████████████| 506/506 [01:02<00:00,  8.05batch/s, loss=0.3523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36/50] Train Loss: 0.1997 | Val Loss: 2.5940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|███████████████| 506/506 [01:02<00:00,  8.06batch/s, loss=0.3118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37/50] Train Loss: 0.1980 | Val Loss: 2.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|███████████████| 506/506 [01:02<00:00,  8.04batch/s, loss=0.1935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38/50] Train Loss: 0.1958 | Val Loss: 2.6273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|███████████████| 506/506 [01:02<00:00,  8.05batch/s, loss=0.2617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39/50] Train Loss: 0.1930 | Val Loss: 2.6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|███████████████| 506/506 [01:02<00:00,  8.06batch/s, loss=0.2713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40/50] Train Loss: 0.1913 | Val Loss: 2.6656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|███████████████| 506/506 [01:02<00:00,  8.06batch/s, loss=0.2048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41/50] Train Loss: 0.1893 | Val Loss: 2.6810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|███████████████| 506/506 [01:02<00:00,  8.04batch/s, loss=0.2636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42/50] Train Loss: 0.1881 | Val Loss: 2.6959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|███████████████| 506/506 [01:02<00:00,  8.06batch/s, loss=0.2773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43/50] Train Loss: 0.1857 | Val Loss: 2.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|███████████████| 506/506 [01:02<00:00,  8.05batch/s, loss=0.2763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44/50] Train Loss: 0.1832 | Val Loss: 2.7352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|███████████████| 506/506 [01:02<00:00,  8.05batch/s, loss=0.2291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45/50] Train Loss: 0.1811 | Val Loss: 2.7456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|███████████████| 506/506 [01:02<00:00,  8.06batch/s, loss=0.2526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46/50] Train Loss: 0.1786 | Val Loss: 2.7612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|███████████████| 506/506 [01:02<00:00,  8.07batch/s, loss=0.2322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/50] Train Loss: 0.1791 | Val Loss: 2.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|███████████████| 506/506 [01:03<00:00,  7.96batch/s, loss=0.2378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48/50] Train Loss: 0.1768 | Val Loss: 2.7922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|███████████████| 506/506 [01:04<00:00,  7.84batch/s, loss=0.1990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49/50] Train Loss: 0.1756 | Val Loss: 2.8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|███████████████| 506/506 [01:04<00:00,  7.83batch/s, loss=0.2131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 50/50] Train Loss: 0.1747 | Val Loss: 2.8259\n",
      "\n",
      "Final model weights saved to final_model.pth\n",
      "Best val_loss=1.5548 (checkpoint at best_checkpoint.pth)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# (A) Parse tokens and build vocabulary\n",
    "# ---------------------------------------\n",
    "if not RESUME:\n",
    "    # If not resuming, parse and build vocab from scratch, and create pkl\n",
    "    imgid2captions = parse_flickr_tokens(TOKENS_FILE)\n",
    "\n",
    "    all_captions = []\n",
    "    for caps in imgid2captions.values():\n",
    "        all_captions.extend(caps)\n",
    "\n",
    "    vocab = Vocabulary(freq_threshold=MIN_WORD_FREQ)\n",
    "    vocab.build_vocabulary(all_captions)\n",
    "\n",
    "    with open(VOCAB_PATH, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(\"Vocabulary saved to:\", VOCAB_PATH)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "    img_ids = list(imgid2captions.keys())\n",
    "    random.shuffle(img_ids)\n",
    "    split_idx = int(0.8 * len(img_ids))\n",
    "    train_ids = img_ids[:split_idx]\n",
    "    val_ids = img_ids[split_idx:]\n",
    "\n",
    "    train_dict = {iid: imgid2captions[iid] for iid in train_ids}\n",
    "    val_dict = {iid: imgid2captions[iid] for iid in val_ids}\n",
    "\n",
    "else:\n",
    "    # If resuming, we assume vocab has been built already, so load it\n",
    "    with open(VOCAB_PATH, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Resuming training. Vocab size: {vocab_size}\")\n",
    "\n",
    "    # Also, parse the tokens again\n",
    "    imgid2captions = parse_flickr_tokens(TOKENS_FILE)\n",
    "    # or you can store train/val splits in a file if you'd like, but let's do it again\n",
    "    img_ids = list(imgid2captions.keys())\n",
    "    random.shuffle(img_ids)\n",
    "    split_idx = int(0.8 * len(img_ids))\n",
    "    train_ids = img_ids[:split_idx]\n",
    "    val_ids = img_ids[split_idx:]\n",
    "\n",
    "    train_dict = {iid: imgid2captions[iid] for iid in train_ids}\n",
    "    val_dict = {iid: imgid2captions[iid] for iid in val_ids}\n",
    "\n",
    "# ---------------------------------------\n",
    "# (B) Create datasets & loaders\n",
    "# ---------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = Flickr8kDataset(train_dict, vocab, transform=transform)\n",
    "val_dataset = Flickr8kDataset(val_dict, vocab, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# (C) Create model, optimizer, etc.\n",
    "# ---------------------------------------\n",
    "encoder = ResNetEncoder(EMBED_DIM)\n",
    "decoder = DecoderLSTM(EMBED_DIM, HIDDEN_DIM, vocab_size)\n",
    "model = ImageCaptioningModel(encoder, decoder).to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"pad\"])\n",
    "parameters = list(model.decoder.parameters()) + list(model.encoder.fc.parameters()) + list(model.encoder.batch_norm.parameters())\n",
    "optimizer = optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "# If we want to resume from an existing checkpoint\n",
    "if RESUME and os.path.exists(BEST_CHECKPOINT_PATH):\n",
    "    print(\"Resuming from checkpoint:\", BEST_CHECKPOINT_PATH)\n",
    "    checkpoint = torch.load(BEST_CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "    print(f\"Resuming at epoch {start_epoch}, best_val_loss so far: {best_val_loss:.4f}\")\n",
    "elif RESUME:\n",
    "    print(f\"Warning: {BEST_CHECKPOINT_PATH} not found. Starting fresh...\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# (D) Training Loop\n",
    "# ---------------------------------------\n",
    "try:\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, vocab_size, epoch)\n",
    "        val_loss = validate(model, val_loader, criterion, vocab_size)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if it's the best so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            checkpoint_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"best_val_loss\": best_val_loss\n",
    "            }\n",
    "            torch.save(checkpoint_dict, BEST_CHECKPOINT_PATH)\n",
    "            print(f\"New best model saved -> {BEST_CHECKPOINT_PATH} (val_loss={val_loss:.4f})\")\n",
    "\n",
    "        final_checkpoint_dict = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "            }\n",
    "        torch.save(final_checkpoint_dict, FINAL_MODEL_PATH)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user. Best checkpoint is already saved if it improved during training.\")\n",
    "\n",
    "print(f\"\\nFinal model weights saved to {FINAL_MODEL_PATH}\")\n",
    "print(f\"Best val_loss={best_val_loss:.4f} (checkpoint at {BEST_CHECKPOINT_PATH})\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
