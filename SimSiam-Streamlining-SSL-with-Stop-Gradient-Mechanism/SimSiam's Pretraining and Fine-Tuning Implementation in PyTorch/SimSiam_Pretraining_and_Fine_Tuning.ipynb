{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pretraining SimSiam"
      ],
      "metadata": {
        "id": "IgDMNTjFooKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNedOvbYO09d"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFilter\n",
        "import random\n",
        "\n",
        "\n",
        "class TwoCropsTransform:\n",
        "    \"\"\"\n",
        "    A transformation class to create two different random crops of the same image.\n",
        "    This is used to generate a query (q) and key (k) pair for contrastive learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_transform):\n",
        "        # Initialize with a base transformation (e.g., augmentation pipeline).\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Apply the base transformation twice to produce two augmented views.\n",
        "        q = self.base_transform(x)  # First crop (query)\n",
        "        k = self.base_transform(x)  # Second crop (key)\n",
        "        return [q, k]  # Return as a list of query and key\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"\n",
        "    Apply Gaussian blur as an augmentation.\n",
        "    This is inspired by SimCLR: https://arxiv.org/abs/2002.05709.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sigma=[.1, 2.]):\n",
        "        # Define the range for the standard deviation (sigma) of the Gaussian kernel.\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Randomly select a sigma value within the defined range.\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        # Apply Gaussian blur with the selected radius to the input image.\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnIx3N0xO09f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from lib.normalize import Normalize\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic building block for ResNet.\n",
        "    Implements two convolutional layers with Batch Normalization and ReLU activation.\n",
        "    Includes a shortcut connection to handle dimensionality changes.\n",
        "    \"\"\"\n",
        "    expansion = 1  # Defines how the number of output channels expands\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Shortcut connection for matching dimensions if necessary\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers and shortcut connection\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)  # Add shortcut connection\n",
        "        out = F.relu(out)  # Final ReLU activation\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    Bottleneck block for ResNet.\n",
        "    Implements a three-layer structure to reduce computation while maintaining performance.\n",
        "    \"\"\"\n",
        "    expansion = 4  # Output channels are 4x the input channels\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        # First convolutional layer (1x1)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Second convolutional layer (3x3)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Third convolutional layer (1x1)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "        # Shortcut connection for matching dimensions if necessary\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the bottleneck layers and shortcut connection\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)  # Add shortcut connection\n",
        "        out = F.relu(out)  # Final ReLU activation\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet model definition.\n",
        "    Builds the full network by stacking blocks and applying transformations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block, num_blocks, low_dim=128):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64  # Initial number of input channels\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Stacked layers using blocks\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        # Fully connected layer for output\n",
        "        self.fc = nn.Linear(512 * block.expansion, low_dim)\n",
        "        # self.l2norm = Normalize(2)  # Optional normalization (commented out)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        Create a layer by stacking multiple blocks.\n",
        "        Handles downsampling when stride > 1.\n",
        "        \"\"\"\n",
        "        strides = [stride] + [1] * (num_blocks - 1)  # First block handles downsampling\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion  # Update input channels for the next block\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through all layers of the network\n",
        "        out = F.relu(self.bn1(self.conv1(x)))  # Initial layer\n",
        "        out = self.layer1(out)  # Layer 1\n",
        "        out = self.layer2(out)  # Layer 2\n",
        "        out = self.layer3(out)  # Layer 3\n",
        "        out = self.layer4(out)  # Layer 4\n",
        "        out = F.avg_pool2d(out, 4)  # Global average pooling\n",
        "        out = out.view(out.size(0), -1)  # Flatten\n",
        "        out = self.fc(out)  # Fully connected layer\n",
        "        # out = self.l2norm(out)  # Optional normalization (commented out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet variants with different depths\n",
        "def ResNet18(low_dim=128):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], low_dim)\n",
        "\n",
        "def ResNet34(low_dim=128):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], low_dim)\n",
        "\n",
        "def ResNet50(low_dim=128):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], low_dim)\n",
        "\n",
        "def ResNet101(low_dim=128):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], low_dim)\n",
        "\n",
        "def ResNet152(low_dim=128):\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], low_dim)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cUM2-VXO09g"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class SimSiamLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the SimSiam loss function.\n",
        "    This loss is designed for self-supervised learning by comparing the similarity\n",
        "    between pairs of projections and predictions from two augmented views of the same image.\n",
        "\n",
        "    Reference:\n",
        "    SimSiam: Exploring Simple Siamese Representation Learning (https://arxiv.org/abs/2011.10566)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, version='simplified'):\n",
        "        \"\"\"\n",
        "        Initialize the SimSiam loss module.\n",
        "\n",
        "        Args:\n",
        "            version (str): Specifies the version of the loss.\n",
        "                           'original' uses the original dot-product-based formulation,\n",
        "                           'simplified' uses cosine similarity (default).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ver = version\n",
        "\n",
        "    def asymmetric_loss(self, p, z):\n",
        "        \"\"\"\n",
        "        Compute the asymmetric loss between the prediction (p) and the projection (z).\n",
        "        This enforces similarity between the two while detaching the gradient from `z`.\n",
        "\n",
        "        Args:\n",
        "            p (torch.Tensor): Prediction vector.\n",
        "            z (torch.Tensor): Projection vector.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Computed loss.\n",
        "        \"\"\"\n",
        "        if self.ver == 'original':\n",
        "            # Detach z to stop gradient flow\n",
        "            z = z.detach()\n",
        "\n",
        "            # Normalize vectors\n",
        "            p = nn.functional.normalize(p, dim=1)\n",
        "            z = nn.functional.normalize(z, dim=1)\n",
        "\n",
        "            # Original formulation: negative dot product\n",
        "            return -(p * z).sum(dim=1).mean()\n",
        "\n",
        "        elif self.ver == 'simplified':\n",
        "            # Detach z to stop gradient flow\n",
        "            z = z.detach()\n",
        "\n",
        "            # Simplified formulation: negative cosine similarity\n",
        "            return -nn.functional.cosine_similarity(p, z, dim=-1).mean()\n",
        "\n",
        "    def forward(self, z1, z2, p1, p2):\n",
        "        \"\"\"\n",
        "        Compute the SimSiam loss for two pairs of projections and predictions.\n",
        "\n",
        "        Args:\n",
        "            z1 (torch.Tensor): Projection vector from the first augmented view.\n",
        "            z2 (torch.Tensor): Projection vector from the second augmented view.\n",
        "            p1 (torch.Tensor): Prediction vector corresponding to z1.\n",
        "            p2 (torch.Tensor): Prediction vector corresponding to z2.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Averaged SimSiam loss.\n",
        "        \"\"\"\n",
        "        # Compute the loss for each pair (p1, z2) and (p2, z1)\n",
        "        loss1 = self.asymmetric_loss(p1, z2)\n",
        "        loss2 = self.asymmetric_loss(p2, z1)\n",
        "\n",
        "        # Average the two losses\n",
        "        return 0.5 * loss1 + 0.5 * loss2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR1qJP0hO09h"
      },
      "outputs": [],
      "source": [
        "# https://github.com/zhirongw/lemniscate.pytorch/blob/master/test.py\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class KNNValidation(object):\n",
        "    \"\"\"\n",
        "    Perform K-Nearest Neighbors (KNN) validation for self-supervised learning.\n",
        "    This evaluates the learned representations by checking how well the model\n",
        "    can classify images using KNN on feature embeddings.\n",
        "\n",
        "    Args:\n",
        "        args (Namespace): Configuration arguments including dataset paths, batch size, etc.\n",
        "        model (nn.Module): The feature extraction model to evaluate.\n",
        "        K (int): Number of neighbors to consider in KNN. Default is 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, model, K=1):\n",
        "        self.model = model\n",
        "        self.device = torch.device('cuda' if next(model.parameters()).is_cuda else 'cpu')\n",
        "        self.args = args\n",
        "        self.K = K\n",
        "\n",
        "        # Define base transformations for preprocessing CIFAR-10 dataset\n",
        "        base_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # CIFAR-10 normalization\n",
        "        ])\n",
        "\n",
        "        # Load CIFAR-10 training dataset\n",
        "        train_dataset = datasets.CIFAR10(root=args.data_root,\n",
        "                                         train=True,\n",
        "                                         download=True,\n",
        "                                         transform=base_transforms)\n",
        "\n",
        "        self.train_dataloader = DataLoader(train_dataset,\n",
        "                                           batch_size=args.batch_size,\n",
        "                                           shuffle=False,  # No shuffle for consistent feature extraction\n",
        "                                           num_workers=args.num_workers,\n",
        "                                           pin_memory=True,\n",
        "                                           drop_last=True)\n",
        "\n",
        "        # Load CIFAR-10 validation dataset\n",
        "        val_dataset = datasets.CIFAR10(root=args.data_root,\n",
        "                                       train=False,\n",
        "                                       download=True,\n",
        "                                       transform=base_transforms)\n",
        "\n",
        "        self.val_dataloader = DataLoader(val_dataset,\n",
        "                                         batch_size=args.batch_size,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=args.num_workers,\n",
        "                                         pin_memory=True,\n",
        "                                         drop_last=True)\n",
        "\n",
        "    def _topk_retrieval(self):\n",
        "        \"\"\"\n",
        "        Extract features from the validation dataset and perform KNN search on\n",
        "        the training dataset features to classify validation images.\n",
        "\n",
        "        Returns:\n",
        "            float: Top-1 accuracy of KNN classification on the validation dataset.\n",
        "        \"\"\"\n",
        "        # Number of training data points\n",
        "        n_data = self.train_dataloader.dataset.data.shape[0]\n",
        "        feat_dim = self.args.feat_dim  # Feature dimension from the model\n",
        "\n",
        "        self.model.eval()  # Set model to evaluation mode\n",
        "        if str(self.device) == 'cuda':\n",
        "            torch.cuda.empty_cache()  # Clear GPU cache for efficient memory usage\n",
        "\n",
        "        # Create tensor to store all training features\n",
        "        train_features = torch.zeros([feat_dim, n_data], device=self.device)\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, _) in enumerate(self.train_dataloader):\n",
        "                inputs = inputs.to(self.device)\n",
        "                batch_size = inputs.size(0)\n",
        "\n",
        "                # Forward pass to extract features\n",
        "                features = self.model(inputs)\n",
        "                features = nn.functional.normalize(features)  # Normalize feature vectors\n",
        "                train_features[:, batch_idx * batch_size:batch_idx * batch_size + batch_size] = features.data.t()\n",
        "\n",
        "            # Get training labels\n",
        "            train_labels = torch.LongTensor(self.train_dataloader.dataset.targets).cuda()\n",
        "\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(self.val_dataloader):\n",
        "                targets = targets.cuda(non_blocking=True)\n",
        "                batch_size = inputs.size(0)\n",
        "\n",
        "                # Extract features for validation inputs\n",
        "                features = self.model(inputs.to(self.device))\n",
        "\n",
        "                # Compute pairwise cosine similarity between validation and training features\n",
        "                dist = torch.mm(features, train_features)\n",
        "\n",
        "                # Retrieve top-K neighbors\n",
        "                yd, yi = dist.topk(self.K, dim=1, largest=True, sorted=True)\n",
        "\n",
        "                # Get corresponding labels of top-K neighbors\n",
        "                candidates = train_labels.view(1, -1).expand(batch_size, -1)\n",
        "                retrieval = torch.gather(candidates, 1, yi)\n",
        "\n",
        "                # Take the most likely label (top-1 retrieval)\n",
        "                retrieval = retrieval.narrow(1, 0, 1).clone().view(-1)\n",
        "\n",
        "                # Update total and correct predictions\n",
        "                total += targets.size(0)\n",
        "                correct += retrieval.eq(targets.data).sum().item()\n",
        "\n",
        "        # Compute top-1 accuracy\n",
        "        top1 = correct / total\n",
        "        return top1\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Public method to evaluate the model using KNN validation.\n",
        "\n",
        "        Returns:\n",
        "            float: Top-1 accuracy.\n",
        "        \"\"\"\n",
        "        return self._topk_retrieval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fPpeaaXwJ3Z"
      },
      "outputs": [],
      "source": [
        "class projection_MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron (MLP) for projection in SimSiam.\n",
        "    This module projects the backbone's output to a feature space for contrastive learning.\n",
        "\n",
        "    Args:\n",
        "        in_dim (int): Input feature dimension.\n",
        "        out_dim (int): Output feature dimension.\n",
        "        num_layers (int): Number of layers in the MLP (default: 2).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim, num_layers=2):\n",
        "        super().__init__()\n",
        "        hidden_dim = out_dim  # Hidden layer dimension\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # First layer: Fully connected + BatchNorm + ReLU\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Second layer: Fully connected + BatchNorm + ReLU (optional)\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Third layer: Fully connected + BatchNorm without learnable affine parameters\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "            nn.BatchNorm1d(out_dim, affine=False)  # See SimSiam paper (Page 5, Paragraph 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the projection MLP.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input features.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Projected features.\n",
        "        \"\"\"\n",
        "        if self.num_layers == 2:\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer3(x)\n",
        "        elif self.num_layers == 3:\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class prediction_MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP for prediction in SimSiam.\n",
        "    Maps the projected features to the prediction space.\n",
        "\n",
        "    Args:\n",
        "        in_dim (int): Input feature dimension (default: 2048).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=2048):\n",
        "        super().__init__()\n",
        "        out_dim = in_dim  # Output dimension matches input dimension\n",
        "        hidden_dim = int(out_dim / 4)  # Reduce feature dimension in the hidden layer\n",
        "\n",
        "        # First layer: Fully connected + BatchNorm + ReLU\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Second layer: Fully connected (no activation)\n",
        "        self.layer2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the prediction MLP.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input features.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted features.\n",
        "        \"\"\"\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimSiam(nn.Module):\n",
        "    \"\"\"\n",
        "    SimSiam network implementation.\n",
        "    Combines a backbone, a projection MLP, and a prediction MLP for self-supervised learning.\n",
        "\n",
        "    Args:\n",
        "        args (Namespace): Configuration arguments for the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        super(SimSiam, self).__init__()\n",
        "        # Initialize the backbone (e.g., ResNet variants)\n",
        "        self.backbone = SimSiam.get_backbone(args.arch)\n",
        "        out_dim = self.backbone.fc.weight.shape[1]  # Feature dimension from the backbone\n",
        "        self.backbone.fc = nn.Identity()  # Remove the fully connected layer from the backbone\n",
        "\n",
        "        # Initialize the projection MLP\n",
        "        self.projector = projection_MLP(out_dim, args.feat_dim, args.num_proj_layers)\n",
        "\n",
        "        # Combine backbone and projector into a single encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            self.backbone,\n",
        "            self.projector\n",
        "        )\n",
        "\n",
        "        # Initialize the prediction MLP\n",
        "        self.predictor = prediction_MLP(args.feat_dim)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_backbone(backbone_name):\n",
        "        \"\"\"\n",
        "        Retrieve the backbone model based on the specified architecture.\n",
        "\n",
        "        Args:\n",
        "            backbone_name (str): Name of the backbone architecture.\n",
        "\n",
        "        Returns:\n",
        "            nn.Module: Backbone model instance.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'resnet18': ResNet18(),\n",
        "            'resnet34': ResNet34(),\n",
        "            'resnet50': ResNet50(),\n",
        "            'resnet101': ResNet101(),\n",
        "            'resnet152': ResNet152()\n",
        "        }[backbone_name]\n",
        "\n",
        "    def forward(self, im_aug1, im_aug2):\n",
        "        \"\"\"\n",
        "        Forward pass through the SimSiam model.\n",
        "\n",
        "        Args:\n",
        "            im_aug1 (torch.Tensor): Augmented view 1 of the input image batch.\n",
        "            im_aug2 (torch.Tensor): Augmented view 2 of the input image batch.\n",
        "\n",
        "        Returns:\n",
        "            dict: Output projections and predictions for both views.\n",
        "                  Keys: 'z1', 'z2', 'p1', 'p2'\n",
        "        \"\"\"\n",
        "        # Pass the first augmented view through the encoder\n",
        "        z1 = self.encoder(im_aug1)\n",
        "        # Pass the second augmented view through the encoder\n",
        "        z2 = self.encoder(im_aug2)\n",
        "\n",
        "        # Predict features for both views\n",
        "        p1 = self.predictor(z1)\n",
        "        p2 = self.predictor(z2)\n",
        "\n",
        "        # Return projections and predictions\n",
        "        return {'z1': z1, 'z2': z2, 'p1': p1, 'p2': p2}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEc7rSEZO09j"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Adjust `sys.argv` for compatibility with Jupyter Notebook or IPython environments.\n",
        "if 'ipykernel_launcher' in sys.argv[0]:\n",
        "    sys.argv = [sys.argv[0]]  # Reset `sys.argv` to prevent parsing issues.\n",
        "\n",
        "# Define configuration parameters for the SimSiam experiment using argparse.Namespace.\n",
        "args = argparse.Namespace(\n",
        "    data_root='./data',          # Path to the root directory containing dataset.\n",
        "    exp_dir='./experiments',     # Directory for saving experimental results (e.g., checkpoints, logs).\n",
        "    trial='1',                   # Identifier for the experiment trial.\n",
        "    img_dim=32,                  # Dimension of the input images (e.g., 32x32 for CIFAR-10).\n",
        "    arch='resnet18',             # Backbone architecture to use (e.g., ResNet18).\n",
        "    feat_dim=2048,               # Dimensionality of the projected features.\n",
        "    num_proj_layers=2,           # Number of layers in the projection MLP.\n",
        "    batch_size=512,              # Batch size for training and validation.\n",
        "    num_workers=4,               # Number of data loading workers.\n",
        "    epochs=800,                  # Number of training epochs.\n",
        "    gpu=0,                       # GPU index to use for training (e.g., 0 for the first GPU).\n",
        "    loss_version='original',   # Version of the loss function ('simplified' or 'original').\n",
        "    print_freq=10,               # Frequency (in batches) to print training progress.\n",
        "    eval_freq=5,                 # Frequency (in epochs) to perform KNN evaluation.\n",
        "    save_freq=50,                # Frequency (in epochs) to save model checkpoints.\n",
        "    resume=None,                 # Path to a checkpoint file to resume training, if any.\n",
        "    learning_rate=0.06,          # Initial learning rate for the optimizer.\n",
        "    weight_decay=5e-4,           # Weight decay for regularization.\n",
        "    momentum=0.9                 # Momentum for the SGD optimizer.\n",
        ")\n",
        "\n",
        "# Print the parsed arguments for verification and debugging\n",
        "print(\"Parsed Arguments:\", args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-MCEId8wNFm",
        "outputId": "c0b20978-ffdc-4a07-81c1-1720a3c91c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data_root': './data', 'exp_dir': './experiments', 'trial': '1', 'img_dim': 32, 'arch': 'resnet18', 'feat_dim': 2048, 'num_proj_layers': 2, 'batch_size': 512, 'num_workers': 4, 'epochs': 800, 'gpu': 0, 'loss_version': 'simplified', 'print_freq': 10, 'eval_freq': 5, 'save_freq': 50, 'resume': None, 'learning_rate': 0.06, 'weight_decay': 0.0005, 'momentum': 0.9}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training...\n",
            "Epoch: [1][ 0/97]\tTime  2.139 ( 2.139)\tLoss 3.6341e-03 (3.6341e-03)\n",
            "Epoch: [1][10/97]\tTime  0.187 ( 0.366)\tLoss -8.1604e-02 (-2.7029e-02)\n",
            "Epoch: [1][20/97]\tTime  0.187 ( 0.281)\tLoss -3.6523e-01 (-1.1276e-01)\n",
            "Epoch: [1][30/97]\tTime  0.189 ( 0.251)\tLoss -7.4587e-01 (-2.7691e-01)\n",
            "Epoch: [1][40/97]\tTime  0.188 ( 0.236)\tLoss -8.0495e-01 (-3.9837e-01)\n",
            "Epoch: [1][50/97]\tTime  0.187 ( 0.227)\tLoss -8.3025e-01 (-4.7971e-01)\n",
            "Epoch: [1][60/97]\tTime  0.188 ( 0.220)\tLoss -8.5005e-01 (-5.3893e-01)\n",
            "Epoch: [1][70/97]\tTime  0.190 ( 0.216)\tLoss -8.6042e-01 (-5.8236e-01)\n",
            "Epoch: [1][80/97]\tTime  0.190 ( 0.213)\tLoss -8.5555e-01 (-6.1644e-01)\n",
            "Epoch: [1][90/97]\tTime  0.187 ( 0.210)\tLoss -8.5923e-01 (-6.4291e-01)\n",
            "Training...\n",
            "Epoch: [2][ 0/97]\tTime  0.461 ( 0.461)\tLoss -8.6063e-01 (-8.6063e-01)\n",
            "Epoch: [2][10/97]\tTime  0.188 ( 0.214)\tLoss -8.4011e-01 (-8.4034e-01)\n",
            "Epoch: [2][20/97]\tTime  0.188 ( 0.203)\tLoss -8.3643e-01 (-8.4265e-01)\n",
            "Epoch: [2][30/97]\tTime  0.189 ( 0.198)\tLoss -8.2001e-01 (-8.4233e-01)\n",
            "Epoch: [2][40/97]\tTime  0.189 ( 0.196)\tLoss -7.8455e-01 (-8.3375e-01)\n",
            "Epoch: [2][50/97]\tTime  0.189 ( 0.195)\tLoss -7.5874e-01 (-8.1892e-01)\n",
            "Epoch: [2][60/97]\tTime  0.189 ( 0.194)\tLoss -6.8300e-01 (-8.0206e-01)\n",
            "Epoch: [2][70/97]\tTime  0.189 ( 0.193)\tLoss -6.7856e-01 (-7.8732e-01)\n",
            "Epoch: [2][80/97]\tTime  0.189 ( 0.193)\tLoss -6.7736e-01 (-7.7295e-01)\n",
            "Epoch: [2][90/97]\tTime  0.191 ( 0.193)\tLoss -7.0240e-01 (-7.6065e-01)\n",
            "Training...\n",
            "Epoch: [3][ 0/97]\tTime  0.463 ( 0.463)\tLoss -6.4482e-01 (-6.4482e-01)\n",
            "Epoch: [3][10/97]\tTime  0.189 ( 0.214)\tLoss -6.5844e-01 (-6.7189e-01)\n",
            "Epoch: [3][20/97]\tTime  0.190 ( 0.202)\tLoss -6.9445e-01 (-6.7147e-01)\n",
            "Epoch: [3][30/97]\tTime  0.188 ( 0.198)\tLoss -7.0380e-01 (-6.7780e-01)\n",
            "Epoch: [3][40/97]\tTime  0.188 ( 0.196)\tLoss -7.3018e-01 (-6.8458e-01)\n",
            "Epoch: [3][50/97]\tTime  0.189 ( 0.195)\tLoss -7.5075e-01 (-6.9198e-01)\n",
            "Epoch: [3][60/97]\tTime  0.190 ( 0.194)\tLoss -7.1959e-01 (-6.9614e-01)\n",
            "Epoch: [3][70/97]\tTime  0.179 ( 0.193)\tLoss -7.0514e-01 (-6.9938e-01)\n",
            "Epoch: [3][80/97]\tTime  0.177 ( 0.191)\tLoss -7.2591e-01 (-7.0165e-01)\n",
            "Epoch: [3][90/97]\tTime  0.176 ( 0.190)\tLoss -7.3230e-01 (-7.0347e-01)\n",
            "Training...\n",
            "Epoch: [4][ 0/97]\tTime  0.441 ( 0.441)\tLoss -6.7068e-01 (-6.7068e-01)\n",
            "Epoch: [4][10/97]\tTime  0.176 ( 0.200)\tLoss -7.0826e-01 (-7.2630e-01)\n",
            "Epoch: [4][20/97]\tTime  0.175 ( 0.188)\tLoss -7.5168e-01 (-7.2894e-01)\n",
            "Epoch: [4][30/97]\tTime  0.176 ( 0.184)\tLoss -7.4637e-01 (-7.3219e-01)\n",
            "Epoch: [4][40/97]\tTime  0.175 ( 0.183)\tLoss -7.2207e-01 (-7.3216e-01)\n",
            "Epoch: [4][50/97]\tTime  0.177 ( 0.181)\tLoss -7.3781e-01 (-7.3244e-01)\n",
            "Epoch: [4][60/97]\tTime  0.175 ( 0.180)\tLoss -7.2582e-01 (-7.3239e-01)\n",
            "Epoch: [4][70/97]\tTime  0.175 ( 0.180)\tLoss -7.2655e-01 (-7.3404e-01)\n",
            "Epoch: [4][80/97]\tTime  0.176 ( 0.179)\tLoss -7.2915e-01 (-7.3488e-01)\n",
            "Epoch: [4][90/97]\tTime  0.176 ( 0.179)\tLoss -7.5023e-01 (-7.3494e-01)\n",
            "Training...\n",
            "Epoch: [5][ 0/97]\tTime  0.450 ( 0.450)\tLoss -7.1957e-01 (-7.1957e-01)\n",
            "Epoch: [5][10/97]\tTime  0.176 ( 0.201)\tLoss -7.6297e-01 (-7.3096e-01)\n",
            "Epoch: [5][20/97]\tTime  0.176 ( 0.189)\tLoss -7.1878e-01 (-7.3648e-01)\n",
            "Epoch: [5][30/97]\tTime  0.176 ( 0.185)\tLoss -7.6829e-01 (-7.3627e-01)\n",
            "Epoch: [5][40/97]\tTime  0.176 ( 0.183)\tLoss -7.4497e-01 (-7.3533e-01)\n",
            "Epoch: [5][50/97]\tTime  0.176 ( 0.182)\tLoss -7.1435e-01 (-7.3179e-01)\n",
            "Epoch: [5][60/97]\tTime  0.176 ( 0.181)\tLoss -7.3963e-01 (-7.3178e-01)\n",
            "Epoch: [5][70/97]\tTime  0.177 ( 0.180)\tLoss -7.3605e-01 (-7.3331e-01)\n",
            "Epoch: [5][80/97]\tTime  0.176 ( 0.180)\tLoss -7.3641e-01 (-7.3552e-01)\n",
            "Epoch: [5][90/97]\tTime  0.176 ( 0.179)\tLoss -7.0457e-01 (-7.3450e-01)\n",
            "Validating...\n",
            "Top1: 0.31219161184210525\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [6][ 0/97]\tTime  0.454 ( 0.454)\tLoss -7.3672e-01 (-7.3672e-01)\n",
            "Epoch: [6][10/97]\tTime  0.176 ( 0.202)\tLoss -7.2948e-01 (-7.3348e-01)\n",
            "Epoch: [6][20/97]\tTime  0.177 ( 0.190)\tLoss -7.9130e-01 (-7.5240e-01)\n",
            "Epoch: [6][30/97]\tTime  0.177 ( 0.185)\tLoss -7.9427e-01 (-7.6497e-01)\n",
            "Epoch: [6][40/97]\tTime  0.177 ( 0.183)\tLoss -7.8786e-01 (-7.7134e-01)\n",
            "Epoch: [6][50/97]\tTime  0.177 ( 0.182)\tLoss -7.3814e-01 (-7.6877e-01)\n",
            "Epoch: [6][60/97]\tTime  0.177 ( 0.181)\tLoss -7.3423e-01 (-7.6018e-01)\n",
            "Epoch: [6][70/97]\tTime  0.178 ( 0.181)\tLoss -7.4187e-01 (-7.5533e-01)\n",
            "Epoch: [6][80/97]\tTime  0.177 ( 0.180)\tLoss -7.1125e-01 (-7.4978e-01)\n",
            "Epoch: [6][90/97]\tTime  0.177 ( 0.180)\tLoss -7.2898e-01 (-7.4812e-01)\n",
            "Training...\n",
            "Epoch: [7][ 0/97]\tTime  0.454 ( 0.454)\tLoss -7.1825e-01 (-7.1825e-01)\n",
            "Epoch: [7][10/97]\tTime  0.177 ( 0.202)\tLoss -7.2486e-01 (-7.2939e-01)\n",
            "Epoch: [7][20/97]\tTime  0.177 ( 0.190)\tLoss -7.3248e-01 (-7.3156e-01)\n",
            "Epoch: [7][30/97]\tTime  0.177 ( 0.186)\tLoss -6.7950e-01 (-7.2042e-01)\n",
            "Epoch: [7][40/97]\tTime  0.177 ( 0.184)\tLoss -6.8230e-01 (-7.0835e-01)\n",
            "Epoch: [7][50/97]\tTime  0.178 ( 0.182)\tLoss -7.4270e-01 (-7.0724e-01)\n",
            "Epoch: [7][60/97]\tTime  0.177 ( 0.182)\tLoss -7.7369e-01 (-7.1529e-01)\n",
            "Epoch: [7][70/97]\tTime  0.177 ( 0.181)\tLoss -7.7057e-01 (-7.2438e-01)\n",
            "Epoch: [7][80/97]\tTime  0.177 ( 0.181)\tLoss -7.9863e-01 (-7.3104e-01)\n",
            "Epoch: [7][90/97]\tTime  0.178 ( 0.180)\tLoss -7.8726e-01 (-7.3764e-01)\n",
            "Training...\n",
            "Epoch: [8][ 0/97]\tTime  0.449 ( 0.449)\tLoss -7.9947e-01 (-7.9947e-01)\n",
            "Epoch: [8][10/97]\tTime  0.177 ( 0.202)\tLoss -7.5145e-01 (-7.7790e-01)\n",
            "Epoch: [8][20/97]\tTime  0.178 ( 0.190)\tLoss -7.5614e-01 (-7.6958e-01)\n",
            "Epoch: [8][30/97]\tTime  0.177 ( 0.186)\tLoss -7.4688e-01 (-7.6582e-01)\n",
            "Epoch: [8][40/97]\tTime  0.177 ( 0.184)\tLoss -7.5171e-01 (-7.6224e-01)\n",
            "Epoch: [8][50/97]\tTime  0.177 ( 0.182)\tLoss -7.5274e-01 (-7.6061e-01)\n",
            "Epoch: [8][60/97]\tTime  0.177 ( 0.181)\tLoss -7.4539e-01 (-7.5828e-01)\n",
            "Epoch: [8][70/97]\tTime  0.178 ( 0.181)\tLoss -7.3281e-01 (-7.5656e-01)\n",
            "Epoch: [8][80/97]\tTime  0.177 ( 0.180)\tLoss -7.2131e-01 (-7.5375e-01)\n",
            "Epoch: [8][90/97]\tTime  0.177 ( 0.180)\tLoss -7.0202e-01 (-7.4912e-01)\n",
            "Training...\n",
            "Epoch: [9][ 0/97]\tTime  0.446 ( 0.446)\tLoss -6.9797e-01 (-6.9797e-01)\n",
            "Epoch: [9][10/97]\tTime  0.177 ( 0.202)\tLoss -6.7527e-01 (-6.7791e-01)\n",
            "Epoch: [9][20/97]\tTime  0.177 ( 0.190)\tLoss -6.7547e-01 (-6.8342e-01)\n",
            "Epoch: [9][30/97]\tTime  0.177 ( 0.186)\tLoss -7.1592e-01 (-6.9307e-01)\n",
            "Epoch: [9][40/97]\tTime  0.178 ( 0.184)\tLoss -7.8038e-01 (-7.0586e-01)\n",
            "Epoch: [9][50/97]\tTime  0.177 ( 0.182)\tLoss -7.5358e-01 (-7.1696e-01)\n",
            "Epoch: [9][60/97]\tTime  0.177 ( 0.182)\tLoss -7.5128e-01 (-7.2433e-01)\n",
            "Epoch: [9][70/97]\tTime  0.178 ( 0.181)\tLoss -7.8239e-01 (-7.3092e-01)\n",
            "Epoch: [9][80/97]\tTime  0.178 ( 0.181)\tLoss -7.8144e-01 (-7.3661e-01)\n",
            "Epoch: [9][90/97]\tTime  0.178 ( 0.180)\tLoss -7.5429e-01 (-7.4065e-01)\n",
            "Training...\n",
            "Epoch: [10][ 0/97]\tTime  0.447 ( 0.447)\tLoss -7.7163e-01 (-7.7163e-01)\n",
            "Epoch: [10][10/97]\tTime  0.178 ( 0.202)\tLoss -7.9056e-01 (-7.7300e-01)\n",
            "Epoch: [10][20/97]\tTime  0.179 ( 0.191)\tLoss -7.6387e-01 (-7.7776e-01)\n",
            "Epoch: [10][30/97]\tTime  0.177 ( 0.186)\tLoss -7.8335e-01 (-7.7900e-01)\n",
            "Epoch: [10][40/97]\tTime  0.177 ( 0.184)\tLoss -7.9021e-01 (-7.7803e-01)\n",
            "Epoch: [10][50/97]\tTime  0.178 ( 0.183)\tLoss -8.0183e-01 (-7.7761e-01)\n",
            "Epoch: [10][60/97]\tTime  0.178 ( 0.182)\tLoss -7.7536e-01 (-7.7764e-01)\n",
            "Epoch: [10][70/97]\tTime  0.177 ( 0.181)\tLoss -7.8115e-01 (-7.7763e-01)\n",
            "Epoch: [10][80/97]\tTime  0.177 ( 0.181)\tLoss -7.5276e-01 (-7.7651e-01)\n",
            "Epoch: [10][90/97]\tTime  0.178 ( 0.181)\tLoss -7.7422e-01 (-7.7609e-01)\n",
            "Validating...\n",
            "Top1: 0.3452919407894737\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [11][ 0/97]\tTime  0.452 ( 0.452)\tLoss -7.4698e-01 (-7.4698e-01)\n",
            "Epoch: [11][10/97]\tTime  0.178 ( 0.204)\tLoss -7.3118e-01 (-7.5655e-01)\n",
            "Epoch: [11][20/97]\tTime  0.178 ( 0.192)\tLoss -7.3948e-01 (-7.5625e-01)\n",
            "Epoch: [11][30/97]\tTime  0.180 ( 0.188)\tLoss -7.4308e-01 (-7.5324e-01)\n",
            "Epoch: [11][40/97]\tTime  0.179 ( 0.185)\tLoss -7.0665e-01 (-7.4621e-01)\n",
            "Epoch: [11][50/97]\tTime  0.178 ( 0.184)\tLoss -7.3430e-01 (-7.4296e-01)\n",
            "Epoch: [11][60/97]\tTime  0.178 ( 0.183)\tLoss -7.4617e-01 (-7.4080e-01)\n",
            "Epoch: [11][70/97]\tTime  0.179 ( 0.183)\tLoss -7.4401e-01 (-7.4071e-01)\n",
            "Epoch: [11][80/97]\tTime  0.179 ( 0.182)\tLoss -7.3280e-01 (-7.4116e-01)\n",
            "Epoch: [11][90/97]\tTime  0.179 ( 0.182)\tLoss -7.3005e-01 (-7.4137e-01)\n",
            "Training...\n",
            "Epoch: [12][ 0/97]\tTime  0.455 ( 0.455)\tLoss -7.3243e-01 (-7.3243e-01)\n",
            "Epoch: [12][10/97]\tTime  0.179 ( 0.203)\tLoss -7.5894e-01 (-7.4906e-01)\n",
            "Epoch: [12][20/97]\tTime  0.178 ( 0.191)\tLoss -7.4857e-01 (-7.4888e-01)\n",
            "Epoch: [12][30/97]\tTime  0.178 ( 0.187)\tLoss -7.4397e-01 (-7.4680e-01)\n",
            "Epoch: [12][40/97]\tTime  0.178 ( 0.185)\tLoss -7.3648e-01 (-7.4712e-01)\n",
            "Epoch: [12][50/97]\tTime  0.178 ( 0.183)\tLoss -7.4184e-01 (-7.4675e-01)\n",
            "Epoch: [12][60/97]\tTime  0.178 ( 0.182)\tLoss -7.3175e-01 (-7.4648e-01)\n",
            "Epoch: [12][70/97]\tTime  0.178 ( 0.182)\tLoss -7.3648e-01 (-7.4528e-01)\n",
            "Epoch: [12][80/97]\tTime  0.178 ( 0.181)\tLoss -7.6642e-01 (-7.4567e-01)\n",
            "Epoch: [12][90/97]\tTime  0.178 ( 0.181)\tLoss -7.4052e-01 (-7.4708e-01)\n",
            "Training...\n",
            "Epoch: [13][ 0/97]\tTime  0.446 ( 0.446)\tLoss -7.6087e-01 (-7.6087e-01)\n",
            "Epoch: [13][10/97]\tTime  0.177 ( 0.202)\tLoss -7.5608e-01 (-7.5872e-01)\n",
            "Epoch: [13][20/97]\tTime  0.178 ( 0.191)\tLoss -7.8510e-01 (-7.6171e-01)\n",
            "Epoch: [13][30/97]\tTime  0.178 ( 0.186)\tLoss -7.4560e-01 (-7.6108e-01)\n",
            "Epoch: [13][40/97]\tTime  0.178 ( 0.184)\tLoss -7.7718e-01 (-7.6580e-01)\n",
            "Epoch: [13][50/97]\tTime  0.178 ( 0.183)\tLoss -7.7440e-01 (-7.6766e-01)\n",
            "Epoch: [13][60/97]\tTime  0.177 ( 0.182)\tLoss -7.4929e-01 (-7.6829e-01)\n",
            "Epoch: [13][70/97]\tTime  0.177 ( 0.182)\tLoss -7.3533e-01 (-7.6866e-01)\n",
            "Epoch: [13][80/97]\tTime  0.179 ( 0.181)\tLoss -7.9095e-01 (-7.6976e-01)\n",
            "Epoch: [13][90/97]\tTime  0.178 ( 0.181)\tLoss -7.6178e-01 (-7.6976e-01)\n",
            "Training...\n",
            "Epoch: [14][ 0/97]\tTime  0.451 ( 0.451)\tLoss -7.8131e-01 (-7.8131e-01)\n",
            "Epoch: [14][10/97]\tTime  0.179 ( 0.203)\tLoss -7.7033e-01 (-7.6769e-01)\n",
            "Epoch: [14][20/97]\tTime  0.178 ( 0.191)\tLoss -7.4916e-01 (-7.6588e-01)\n",
            "Epoch: [14][30/97]\tTime  0.177 ( 0.187)\tLoss -7.3910e-01 (-7.6163e-01)\n",
            "Epoch: [14][40/97]\tTime  0.177 ( 0.184)\tLoss -7.4214e-01 (-7.5806e-01)\n",
            "Epoch: [14][50/97]\tTime  0.178 ( 0.183)\tLoss -7.5808e-01 (-7.5698e-01)\n",
            "Epoch: [14][60/97]\tTime  0.179 ( 0.182)\tLoss -7.5563e-01 (-7.5699e-01)\n",
            "Epoch: [14][70/97]\tTime  0.178 ( 0.182)\tLoss -7.4566e-01 (-7.5675e-01)\n",
            "Epoch: [14][80/97]\tTime  0.177 ( 0.181)\tLoss -7.4830e-01 (-7.5411e-01)\n",
            "Epoch: [14][90/97]\tTime  0.177 ( 0.181)\tLoss -7.6690e-01 (-7.5394e-01)\n",
            "Training...\n",
            "Epoch: [15][ 0/97]\tTime  0.461 ( 0.461)\tLoss -7.7413e-01 (-7.7413e-01)\n",
            "Epoch: [15][10/97]\tTime  0.177 ( 0.204)\tLoss -7.5766e-01 (-7.5095e-01)\n",
            "Epoch: [15][20/97]\tTime  0.178 ( 0.191)\tLoss -7.3838e-01 (-7.4626e-01)\n",
            "Epoch: [15][30/97]\tTime  0.178 ( 0.187)\tLoss -7.5780e-01 (-7.4214e-01)\n",
            "Epoch: [15][40/97]\tTime  0.178 ( 0.185)\tLoss -7.6600e-01 (-7.4620e-01)\n",
            "Epoch: [15][50/97]\tTime  0.177 ( 0.183)\tLoss -7.7658e-01 (-7.5245e-01)\n",
            "Epoch: [15][60/97]\tTime  0.178 ( 0.183)\tLoss -7.9777e-01 (-7.6079e-01)\n",
            "Epoch: [15][70/97]\tTime  0.177 ( 0.182)\tLoss -8.0126e-01 (-7.6522e-01)\n",
            "Epoch: [15][80/97]\tTime  0.178 ( 0.181)\tLoss -7.9258e-01 (-7.6832e-01)\n",
            "Epoch: [15][90/97]\tTime  0.178 ( 0.181)\tLoss -7.9766e-01 (-7.7050e-01)\n",
            "Validating...\n",
            "Top1: 0.3550575657894737\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [16][ 0/97]\tTime  0.448 ( 0.448)\tLoss -7.9637e-01 (-7.9637e-01)\n",
            "Epoch: [16][10/97]\tTime  0.179 ( 0.203)\tLoss -7.6628e-01 (-7.7958e-01)\n",
            "Epoch: [16][20/97]\tTime  0.178 ( 0.191)\tLoss -7.9491e-01 (-7.8027e-01)\n",
            "Epoch: [16][30/97]\tTime  0.179 ( 0.187)\tLoss -8.0460e-01 (-7.8198e-01)\n",
            "Epoch: [16][40/97]\tTime  0.178 ( 0.185)\tLoss -7.8869e-01 (-7.8321e-01)\n",
            "Epoch: [16][50/97]\tTime  0.179 ( 0.184)\tLoss -7.8109e-01 (-7.8443e-01)\n",
            "Epoch: [16][60/97]\tTime  0.179 ( 0.183)\tLoss -8.0230e-01 (-7.8641e-01)\n",
            "Epoch: [16][70/97]\tTime  0.178 ( 0.182)\tLoss -7.9272e-01 (-7.8694e-01)\n",
            "Epoch: [16][80/97]\tTime  0.179 ( 0.182)\tLoss -8.0797e-01 (-7.8618e-01)\n",
            "Epoch: [16][90/97]\tTime  0.180 ( 0.182)\tLoss -8.0026e-01 (-7.8581e-01)\n",
            "Training...\n",
            "Epoch: [17][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.0799e-01 (-8.0799e-01)\n",
            "Epoch: [17][10/97]\tTime  0.178 ( 0.202)\tLoss -8.0528e-01 (-7.9354e-01)\n",
            "Epoch: [17][20/97]\tTime  0.177 ( 0.190)\tLoss -7.8412e-01 (-7.9255e-01)\n",
            "Epoch: [17][30/97]\tTime  0.178 ( 0.186)\tLoss -7.8532e-01 (-7.9106e-01)\n",
            "Epoch: [17][40/97]\tTime  0.177 ( 0.184)\tLoss -7.5505e-01 (-7.8535e-01)\n",
            "Epoch: [17][50/97]\tTime  0.177 ( 0.183)\tLoss -7.3145e-01 (-7.7896e-01)\n",
            "Epoch: [17][60/97]\tTime  0.177 ( 0.182)\tLoss -7.6344e-01 (-7.7366e-01)\n",
            "Epoch: [17][70/97]\tTime  0.179 ( 0.181)\tLoss -7.7135e-01 (-7.7196e-01)\n",
            "Epoch: [17][80/97]\tTime  0.177 ( 0.181)\tLoss -7.4385e-01 (-7.6875e-01)\n",
            "Epoch: [17][90/97]\tTime  0.177 ( 0.181)\tLoss -7.6940e-01 (-7.6698e-01)\n",
            "Training...\n",
            "Epoch: [18][ 0/97]\tTime  0.444 ( 0.444)\tLoss -7.9284e-01 (-7.9284e-01)\n",
            "Epoch: [18][10/97]\tTime  0.177 ( 0.202)\tLoss -8.0031e-01 (-8.0069e-01)\n",
            "Epoch: [18][20/97]\tTime  0.177 ( 0.190)\tLoss -8.0399e-01 (-7.9905e-01)\n",
            "Epoch: [18][30/97]\tTime  0.177 ( 0.186)\tLoss -7.9702e-01 (-7.9712e-01)\n",
            "Epoch: [18][40/97]\tTime  0.178 ( 0.184)\tLoss -8.1290e-01 (-7.9779e-01)\n",
            "Epoch: [18][50/97]\tTime  0.179 ( 0.183)\tLoss -7.9965e-01 (-7.9674e-01)\n",
            "Epoch: [18][60/97]\tTime  0.178 ( 0.182)\tLoss -7.9340e-01 (-7.9554e-01)\n",
            "Epoch: [18][70/97]\tTime  0.177 ( 0.182)\tLoss -8.0173e-01 (-7.9535e-01)\n",
            "Epoch: [18][80/97]\tTime  0.177 ( 0.181)\tLoss -8.0113e-01 (-7.9585e-01)\n",
            "Epoch: [18][90/97]\tTime  0.178 ( 0.181)\tLoss -7.7135e-01 (-7.9424e-01)\n",
            "Training...\n",
            "Epoch: [19][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.0317e-01 (-8.0317e-01)\n",
            "Epoch: [19][10/97]\tTime  0.177 ( 0.202)\tLoss -8.0029e-01 (-8.0407e-01)\n",
            "Epoch: [19][20/97]\tTime  0.177 ( 0.190)\tLoss -7.9298e-01 (-8.0180e-01)\n",
            "Epoch: [19][30/97]\tTime  0.179 ( 0.186)\tLoss -7.9830e-01 (-8.0065e-01)\n",
            "Epoch: [19][40/97]\tTime  0.177 ( 0.184)\tLoss -7.9654e-01 (-7.9837e-01)\n",
            "Epoch: [19][50/97]\tTime  0.177 ( 0.183)\tLoss -7.8564e-01 (-7.9654e-01)\n",
            "Epoch: [19][60/97]\tTime  0.177 ( 0.182)\tLoss -7.9176e-01 (-7.9516e-01)\n",
            "Epoch: [19][70/97]\tTime  0.179 ( 0.181)\tLoss -7.5834e-01 (-7.9292e-01)\n",
            "Epoch: [19][80/97]\tTime  0.177 ( 0.181)\tLoss -7.6766e-01 (-7.8998e-01)\n",
            "Epoch: [19][90/97]\tTime  0.177 ( 0.181)\tLoss -7.7519e-01 (-7.8793e-01)\n",
            "Training...\n",
            "Epoch: [20][ 0/97]\tTime  0.451 ( 0.451)\tLoss -7.7599e-01 (-7.7599e-01)\n",
            "Epoch: [20][10/97]\tTime  0.178 ( 0.202)\tLoss -7.9226e-01 (-7.7978e-01)\n",
            "Epoch: [20][20/97]\tTime  0.177 ( 0.191)\tLoss -8.0465e-01 (-7.8794e-01)\n",
            "Epoch: [20][30/97]\tTime  0.178 ( 0.186)\tLoss -8.0429e-01 (-7.9365e-01)\n",
            "Epoch: [20][40/97]\tTime  0.178 ( 0.184)\tLoss -8.2170e-01 (-7.9716e-01)\n",
            "Epoch: [20][50/97]\tTime  0.178 ( 0.183)\tLoss -7.9563e-01 (-7.9793e-01)\n",
            "Epoch: [20][60/97]\tTime  0.177 ( 0.182)\tLoss -7.9302e-01 (-7.9789e-01)\n",
            "Epoch: [20][70/97]\tTime  0.177 ( 0.181)\tLoss -7.9077e-01 (-7.9820e-01)\n",
            "Epoch: [20][80/97]\tTime  0.177 ( 0.181)\tLoss -8.1611e-01 (-7.9927e-01)\n",
            "Epoch: [20][90/97]\tTime  0.177 ( 0.181)\tLoss -8.0947e-01 (-7.9925e-01)\n",
            "Validating...\n",
            "Top1: 0.40594161184210525\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [21][ 0/97]\tTime  0.453 ( 0.453)\tLoss -7.9441e-01 (-7.9441e-01)\n",
            "Epoch: [21][10/97]\tTime  0.179 ( 0.203)\tLoss -7.8845e-01 (-7.9946e-01)\n",
            "Epoch: [21][20/97]\tTime  0.179 ( 0.192)\tLoss -7.9783e-01 (-7.9900e-01)\n",
            "Epoch: [21][30/97]\tTime  0.178 ( 0.187)\tLoss -8.1535e-01 (-7.9932e-01)\n",
            "Epoch: [21][40/97]\tTime  0.178 ( 0.185)\tLoss -8.0776e-01 (-7.9921e-01)\n",
            "Epoch: [21][50/97]\tTime  0.180 ( 0.184)\tLoss -7.7909e-01 (-7.9823e-01)\n",
            "Epoch: [21][60/97]\tTime  0.180 ( 0.183)\tLoss -7.9871e-01 (-7.9738e-01)\n",
            "Epoch: [21][70/97]\tTime  0.178 ( 0.183)\tLoss -8.0783e-01 (-7.9612e-01)\n",
            "Epoch: [21][80/97]\tTime  0.178 ( 0.182)\tLoss -8.0561e-01 (-7.9504e-01)\n",
            "Epoch: [21][90/97]\tTime  0.179 ( 0.182)\tLoss -7.8142e-01 (-7.9366e-01)\n",
            "Training...\n",
            "Epoch: [22][ 0/97]\tTime  0.445 ( 0.445)\tLoss -7.8395e-01 (-7.8395e-01)\n",
            "Epoch: [22][10/97]\tTime  0.177 ( 0.202)\tLoss -7.9379e-01 (-7.8641e-01)\n",
            "Epoch: [22][20/97]\tTime  0.178 ( 0.190)\tLoss -7.9490e-01 (-7.8933e-01)\n",
            "Epoch: [22][30/97]\tTime  0.178 ( 0.186)\tLoss -8.0098e-01 (-7.9021e-01)\n",
            "Epoch: [22][40/97]\tTime  0.178 ( 0.184)\tLoss -7.8799e-01 (-7.8967e-01)\n",
            "Epoch: [22][50/97]\tTime  0.177 ( 0.183)\tLoss -8.1111e-01 (-7.8980e-01)\n",
            "Epoch: [22][60/97]\tTime  0.177 ( 0.182)\tLoss -7.9987e-01 (-7.9131e-01)\n",
            "Epoch: [22][70/97]\tTime  0.177 ( 0.181)\tLoss -8.0138e-01 (-7.9241e-01)\n",
            "Epoch: [22][80/97]\tTime  0.178 ( 0.181)\tLoss -7.9806e-01 (-7.9165e-01)\n",
            "Epoch: [22][90/97]\tTime  0.178 ( 0.181)\tLoss -7.9904e-01 (-7.9236e-01)\n",
            "Training...\n",
            "Epoch: [23][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.1697e-01 (-8.1697e-01)\n",
            "Epoch: [23][10/97]\tTime  0.178 ( 0.202)\tLoss -8.0379e-01 (-8.1816e-01)\n",
            "Epoch: [23][20/97]\tTime  0.178 ( 0.190)\tLoss -8.0482e-01 (-8.1144e-01)\n",
            "Epoch: [23][30/97]\tTime  0.177 ( 0.186)\tLoss -8.2155e-01 (-8.1165e-01)\n",
            "Epoch: [23][40/97]\tTime  0.178 ( 0.184)\tLoss -7.8720e-01 (-8.1050e-01)\n",
            "Epoch: [23][50/97]\tTime  0.177 ( 0.183)\tLoss -8.0616e-01 (-8.0957e-01)\n",
            "Epoch: [23][60/97]\tTime  0.178 ( 0.182)\tLoss -7.9242e-01 (-8.0944e-01)\n",
            "Epoch: [23][70/97]\tTime  0.178 ( 0.181)\tLoss -8.1924e-01 (-8.0981e-01)\n",
            "Epoch: [23][80/97]\tTime  0.177 ( 0.181)\tLoss -8.0880e-01 (-8.0982e-01)\n",
            "Epoch: [23][90/97]\tTime  0.177 ( 0.181)\tLoss -8.0836e-01 (-8.0961e-01)\n",
            "Training...\n",
            "Epoch: [24][ 0/97]\tTime  0.452 ( 0.452)\tLoss -7.9949e-01 (-7.9949e-01)\n",
            "Epoch: [24][10/97]\tTime  0.178 ( 0.203)\tLoss -8.0193e-01 (-8.0170e-01)\n",
            "Epoch: [24][20/97]\tTime  0.177 ( 0.191)\tLoss -7.9901e-01 (-8.0020e-01)\n",
            "Epoch: [24][30/97]\tTime  0.177 ( 0.187)\tLoss -8.1544e-01 (-8.0010e-01)\n",
            "Epoch: [24][40/97]\tTime  0.179 ( 0.184)\tLoss -7.8532e-01 (-7.9854e-01)\n",
            "Epoch: [24][50/97]\tTime  0.178 ( 0.183)\tLoss -7.8821e-01 (-7.9792e-01)\n",
            "Epoch: [24][60/97]\tTime  0.178 ( 0.182)\tLoss -7.7570e-01 (-7.9765e-01)\n",
            "Epoch: [24][70/97]\tTime  0.178 ( 0.182)\tLoss -7.9777e-01 (-7.9767e-01)\n",
            "Epoch: [24][80/97]\tTime  0.178 ( 0.181)\tLoss -8.0041e-01 (-7.9768e-01)\n",
            "Epoch: [24][90/97]\tTime  0.179 ( 0.181)\tLoss -7.9618e-01 (-7.9684e-01)\n",
            "Training...\n",
            "Epoch: [25][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.0821e-01 (-8.0821e-01)\n",
            "Epoch: [25][10/97]\tTime  0.177 ( 0.203)\tLoss -8.1964e-01 (-8.0967e-01)\n",
            "Epoch: [25][20/97]\tTime  0.179 ( 0.191)\tLoss -8.0651e-01 (-8.0939e-01)\n",
            "Epoch: [25][30/97]\tTime  0.178 ( 0.187)\tLoss -7.9264e-01 (-8.0874e-01)\n",
            "Epoch: [25][40/97]\tTime  0.178 ( 0.184)\tLoss -8.0728e-01 (-8.0787e-01)\n",
            "Epoch: [25][50/97]\tTime  0.177 ( 0.183)\tLoss -8.1727e-01 (-8.0795e-01)\n",
            "Epoch: [25][60/97]\tTime  0.178 ( 0.182)\tLoss -8.0671e-01 (-8.0840e-01)\n",
            "Epoch: [25][70/97]\tTime  0.178 ( 0.182)\tLoss -8.1104e-01 (-8.0849e-01)\n",
            "Epoch: [25][80/97]\tTime  0.177 ( 0.181)\tLoss -8.2336e-01 (-8.0890e-01)\n",
            "Epoch: [25][90/97]\tTime  0.177 ( 0.181)\tLoss -8.0905e-01 (-8.0956e-01)\n",
            "Validating...\n",
            "Top1: 0.46587171052631576\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [26][ 0/97]\tTime  0.442 ( 0.442)\tLoss -8.0529e-01 (-8.0529e-01)\n",
            "Epoch: [26][10/97]\tTime  0.178 ( 0.202)\tLoss -8.0441e-01 (-8.0507e-01)\n",
            "Epoch: [26][20/97]\tTime  0.179 ( 0.191)\tLoss -8.0950e-01 (-8.0740e-01)\n",
            "Epoch: [26][30/97]\tTime  0.180 ( 0.187)\tLoss -8.1389e-01 (-8.0864e-01)\n",
            "Epoch: [26][40/97]\tTime  0.178 ( 0.185)\tLoss -8.0290e-01 (-8.0898e-01)\n",
            "Epoch: [26][50/97]\tTime  0.179 ( 0.184)\tLoss -8.0092e-01 (-8.0884e-01)\n",
            "Epoch: [26][60/97]\tTime  0.179 ( 0.183)\tLoss -8.1934e-01 (-8.0909e-01)\n",
            "Epoch: [26][70/97]\tTime  0.179 ( 0.182)\tLoss -8.0757e-01 (-8.0887e-01)\n",
            "Epoch: [26][80/97]\tTime  0.179 ( 0.182)\tLoss -8.0438e-01 (-8.0888e-01)\n",
            "Epoch: [26][90/97]\tTime  0.179 ( 0.182)\tLoss -8.2360e-01 (-8.0953e-01)\n",
            "Training...\n",
            "Epoch: [27][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.1071e-01 (-8.1071e-01)\n",
            "Epoch: [27][10/97]\tTime  0.178 ( 0.202)\tLoss -8.1024e-01 (-8.1389e-01)\n",
            "Epoch: [27][20/97]\tTime  0.177 ( 0.190)\tLoss -8.1064e-01 (-8.1046e-01)\n",
            "Epoch: [27][30/97]\tTime  0.178 ( 0.186)\tLoss -7.9998e-01 (-8.0750e-01)\n",
            "Epoch: [27][40/97]\tTime  0.177 ( 0.184)\tLoss -8.0789e-01 (-8.0714e-01)\n",
            "Epoch: [27][50/97]\tTime  0.178 ( 0.183)\tLoss -7.8315e-01 (-8.0672e-01)\n",
            "Epoch: [27][60/97]\tTime  0.177 ( 0.182)\tLoss -7.8633e-01 (-8.0623e-01)\n",
            "Epoch: [27][70/97]\tTime  0.177 ( 0.181)\tLoss -8.0748e-01 (-8.0594e-01)\n",
            "Epoch: [27][80/97]\tTime  0.178 ( 0.181)\tLoss -8.1781e-01 (-8.0655e-01)\n",
            "Epoch: [27][90/97]\tTime  0.178 ( 0.181)\tLoss -8.1988e-01 (-8.0777e-01)\n",
            "Training...\n",
            "Epoch: [28][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.1350e-01 (-8.1350e-01)\n",
            "Epoch: [28][10/97]\tTime  0.178 ( 0.202)\tLoss -8.1262e-01 (-8.1719e-01)\n",
            "Epoch: [28][20/97]\tTime  0.178 ( 0.191)\tLoss -8.2259e-01 (-8.1809e-01)\n",
            "Epoch: [28][30/97]\tTime  0.179 ( 0.187)\tLoss -8.1618e-01 (-8.1538e-01)\n",
            "Epoch: [28][40/97]\tTime  0.178 ( 0.184)\tLoss -8.1074e-01 (-8.1277e-01)\n",
            "Epoch: [28][50/97]\tTime  0.178 ( 0.183)\tLoss -8.0135e-01 (-8.1165e-01)\n",
            "Epoch: [28][60/97]\tTime  0.178 ( 0.182)\tLoss -8.0818e-01 (-8.1077e-01)\n",
            "Epoch: [28][70/97]\tTime  0.177 ( 0.182)\tLoss -8.2853e-01 (-8.1040e-01)\n",
            "Epoch: [28][80/97]\tTime  0.178 ( 0.181)\tLoss -8.3123e-01 (-8.1247e-01)\n",
            "Epoch: [28][90/97]\tTime  0.178 ( 0.181)\tLoss -8.1763e-01 (-8.1333e-01)\n",
            "Training...\n",
            "Epoch: [29][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.1864e-01 (-8.1864e-01)\n",
            "Epoch: [29][10/97]\tTime  0.179 ( 0.203)\tLoss -8.3581e-01 (-8.1583e-01)\n",
            "Epoch: [29][20/97]\tTime  0.178 ( 0.191)\tLoss -8.1631e-01 (-8.2033e-01)\n",
            "Epoch: [29][30/97]\tTime  0.177 ( 0.186)\tLoss -8.2570e-01 (-8.2135e-01)\n",
            "Epoch: [29][40/97]\tTime  0.178 ( 0.184)\tLoss -8.3182e-01 (-8.2254e-01)\n",
            "Epoch: [29][50/97]\tTime  0.178 ( 0.183)\tLoss -8.1948e-01 (-8.2345e-01)\n",
            "Epoch: [29][60/97]\tTime  0.178 ( 0.182)\tLoss -8.3329e-01 (-8.2419e-01)\n",
            "Epoch: [29][70/97]\tTime  0.177 ( 0.181)\tLoss -8.2871e-01 (-8.2476e-01)\n",
            "Epoch: [29][80/97]\tTime  0.178 ( 0.181)\tLoss -8.3404e-01 (-8.2482e-01)\n",
            "Epoch: [29][90/97]\tTime  0.178 ( 0.181)\tLoss -8.2491e-01 (-8.2493e-01)\n",
            "Training...\n",
            "Epoch: [30][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.0165e-01 (-8.0165e-01)\n",
            "Epoch: [30][10/97]\tTime  0.178 ( 0.203)\tLoss -8.3369e-01 (-8.2295e-01)\n",
            "Epoch: [30][20/97]\tTime  0.178 ( 0.191)\tLoss -8.1391e-01 (-8.2242e-01)\n",
            "Epoch: [30][30/97]\tTime  0.178 ( 0.187)\tLoss -8.1579e-01 (-8.2078e-01)\n",
            "Epoch: [30][40/97]\tTime  0.177 ( 0.185)\tLoss -8.0330e-01 (-8.1845e-01)\n",
            "Epoch: [30][50/97]\tTime  0.178 ( 0.183)\tLoss -8.1610e-01 (-8.1953e-01)\n",
            "Epoch: [30][60/97]\tTime  0.178 ( 0.182)\tLoss -8.1090e-01 (-8.1851e-01)\n",
            "Epoch: [30][70/97]\tTime  0.178 ( 0.182)\tLoss -8.0797e-01 (-8.1729e-01)\n",
            "Epoch: [30][80/97]\tTime  0.179 ( 0.181)\tLoss -8.1152e-01 (-8.1681e-01)\n",
            "Epoch: [30][90/97]\tTime  0.178 ( 0.181)\tLoss -8.2103e-01 (-8.1677e-01)\n",
            "Validating...\n",
            "Top1: 0.5144942434210527\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [31][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.2145e-01 (-8.2145e-01)\n",
            "Epoch: [31][10/97]\tTime  0.179 ( 0.203)\tLoss -8.3382e-01 (-8.2519e-01)\n",
            "Epoch: [31][20/97]\tTime  0.179 ( 0.192)\tLoss -8.2297e-01 (-8.2292e-01)\n",
            "Epoch: [31][30/97]\tTime  0.178 ( 0.187)\tLoss -8.2321e-01 (-8.2131e-01)\n",
            "Epoch: [31][40/97]\tTime  0.179 ( 0.185)\tLoss -8.2980e-01 (-8.2096e-01)\n",
            "Epoch: [31][50/97]\tTime  0.178 ( 0.184)\tLoss -8.3053e-01 (-8.2178e-01)\n",
            "Epoch: [31][60/97]\tTime  0.179 ( 0.183)\tLoss -8.2675e-01 (-8.2170e-01)\n",
            "Epoch: [31][70/97]\tTime  0.178 ( 0.182)\tLoss -8.2492e-01 (-8.2158e-01)\n",
            "Epoch: [31][80/97]\tTime  0.178 ( 0.182)\tLoss -8.2355e-01 (-8.2166e-01)\n",
            "Epoch: [31][90/97]\tTime  0.180 ( 0.182)\tLoss -8.2610e-01 (-8.2120e-01)\n",
            "Training...\n",
            "Epoch: [32][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.2729e-01 (-8.2729e-01)\n",
            "Epoch: [32][10/97]\tTime  0.177 ( 0.202)\tLoss -8.1906e-01 (-8.2300e-01)\n",
            "Epoch: [32][20/97]\tTime  0.178 ( 0.190)\tLoss -8.1600e-01 (-8.2137e-01)\n",
            "Epoch: [32][30/97]\tTime  0.178 ( 0.186)\tLoss -8.1746e-01 (-8.2116e-01)\n",
            "Epoch: [32][40/97]\tTime  0.177 ( 0.184)\tLoss -8.1437e-01 (-8.1920e-01)\n",
            "Epoch: [32][50/97]\tTime  0.178 ( 0.183)\tLoss -8.1658e-01 (-8.1840e-01)\n",
            "Epoch: [32][60/97]\tTime  0.178 ( 0.182)\tLoss -8.2151e-01 (-8.1694e-01)\n",
            "Epoch: [32][70/97]\tTime  0.178 ( 0.181)\tLoss -8.2238e-01 (-8.1712e-01)\n",
            "Epoch: [32][80/97]\tTime  0.177 ( 0.181)\tLoss -8.2739e-01 (-8.1853e-01)\n",
            "Epoch: [32][90/97]\tTime  0.177 ( 0.181)\tLoss -8.1696e-01 (-8.1858e-01)\n",
            "Training...\n",
            "Epoch: [33][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.0943e-01 (-8.0943e-01)\n",
            "Epoch: [33][10/97]\tTime  0.177 ( 0.202)\tLoss -8.1949e-01 (-8.1796e-01)\n",
            "Epoch: [33][20/97]\tTime  0.177 ( 0.190)\tLoss -8.1201e-01 (-8.2004e-01)\n",
            "Epoch: [33][30/97]\tTime  0.178 ( 0.186)\tLoss -8.0963e-01 (-8.1977e-01)\n",
            "Epoch: [33][40/97]\tTime  0.178 ( 0.184)\tLoss -8.1600e-01 (-8.1863e-01)\n",
            "Epoch: [33][50/97]\tTime  0.178 ( 0.183)\tLoss -8.1758e-01 (-8.1837e-01)\n",
            "Epoch: [33][60/97]\tTime  0.177 ( 0.182)\tLoss -8.1394e-01 (-8.1788e-01)\n",
            "Epoch: [33][70/97]\tTime  0.177 ( 0.181)\tLoss -8.2289e-01 (-8.1756e-01)\n",
            "Epoch: [33][80/97]\tTime  0.177 ( 0.181)\tLoss -8.0267e-01 (-8.1758e-01)\n",
            "Epoch: [33][90/97]\tTime  0.179 ( 0.181)\tLoss -8.0726e-01 (-8.1717e-01)\n",
            "Training...\n",
            "Epoch: [34][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.2431e-01 (-8.2431e-01)\n",
            "Epoch: [34][10/97]\tTime  0.177 ( 0.202)\tLoss -8.3029e-01 (-8.1830e-01)\n",
            "Epoch: [34][20/97]\tTime  0.178 ( 0.190)\tLoss -8.0580e-01 (-8.1902e-01)\n",
            "Epoch: [34][30/97]\tTime  0.177 ( 0.186)\tLoss -8.3661e-01 (-8.2026e-01)\n",
            "Epoch: [34][40/97]\tTime  0.177 ( 0.184)\tLoss -8.3479e-01 (-8.2136e-01)\n",
            "Epoch: [34][50/97]\tTime  0.177 ( 0.183)\tLoss -8.1109e-01 (-8.2199e-01)\n",
            "Epoch: [34][60/97]\tTime  0.177 ( 0.182)\tLoss -8.3825e-01 (-8.2171e-01)\n",
            "Epoch: [34][70/97]\tTime  0.179 ( 0.181)\tLoss -8.3918e-01 (-8.2203e-01)\n",
            "Epoch: [34][80/97]\tTime  0.178 ( 0.181)\tLoss -8.4223e-01 (-8.2445e-01)\n",
            "Epoch: [34][90/97]\tTime  0.177 ( 0.181)\tLoss -8.3752e-01 (-8.2585e-01)\n",
            "Training...\n",
            "Epoch: [35][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.3862e-01 (-8.3862e-01)\n",
            "Epoch: [35][10/97]\tTime  0.178 ( 0.202)\tLoss -8.2245e-01 (-8.2892e-01)\n",
            "Epoch: [35][20/97]\tTime  0.177 ( 0.190)\tLoss -8.3875e-01 (-8.2959e-01)\n",
            "Epoch: [35][30/97]\tTime  0.177 ( 0.186)\tLoss -8.4169e-01 (-8.3155e-01)\n",
            "Epoch: [35][40/97]\tTime  0.177 ( 0.184)\tLoss -8.3430e-01 (-8.3255e-01)\n",
            "Epoch: [35][50/97]\tTime  0.179 ( 0.183)\tLoss -8.2881e-01 (-8.3306e-01)\n",
            "Epoch: [35][60/97]\tTime  0.177 ( 0.182)\tLoss -8.3474e-01 (-8.3387e-01)\n",
            "Epoch: [35][70/97]\tTime  0.178 ( 0.181)\tLoss -8.5020e-01 (-8.3453e-01)\n",
            "Epoch: [35][80/97]\tTime  0.178 ( 0.181)\tLoss -8.5366e-01 (-8.3517e-01)\n",
            "Epoch: [35][90/97]\tTime  0.178 ( 0.181)\tLoss -8.6017e-01 (-8.3700e-01)\n",
            "Validating...\n",
            "Top1: 0.5382401315789473\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [36][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.4695e-01 (-8.4695e-01)\n",
            "Epoch: [36][10/97]\tTime  0.180 ( 0.203)\tLoss -8.4574e-01 (-8.5242e-01)\n",
            "Epoch: [36][20/97]\tTime  0.178 ( 0.192)\tLoss -8.3992e-01 (-8.5005e-01)\n",
            "Epoch: [36][30/97]\tTime  0.179 ( 0.188)\tLoss -8.4612e-01 (-8.5000e-01)\n",
            "Epoch: [36][40/97]\tTime  0.178 ( 0.185)\tLoss -8.3952e-01 (-8.4942e-01)\n",
            "Epoch: [36][50/97]\tTime  0.178 ( 0.184)\tLoss -8.4005e-01 (-8.4939e-01)\n",
            "Epoch: [36][60/97]\tTime  0.179 ( 0.183)\tLoss -8.2343e-01 (-8.4763e-01)\n",
            "Epoch: [36][70/97]\tTime  0.178 ( 0.182)\tLoss -8.4519e-01 (-8.4590e-01)\n",
            "Epoch: [36][80/97]\tTime  0.179 ( 0.182)\tLoss -8.2412e-01 (-8.4495e-01)\n",
            "Epoch: [36][90/97]\tTime  0.179 ( 0.182)\tLoss -8.4879e-01 (-8.4437e-01)\n",
            "Training...\n",
            "Epoch: [37][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.3869e-01 (-8.3869e-01)\n",
            "Epoch: [37][10/97]\tTime  0.177 ( 0.202)\tLoss -8.4432e-01 (-8.3501e-01)\n",
            "Epoch: [37][20/97]\tTime  0.177 ( 0.191)\tLoss -8.3730e-01 (-8.3674e-01)\n",
            "Epoch: [37][30/97]\tTime  0.177 ( 0.186)\tLoss -8.5118e-01 (-8.3571e-01)\n",
            "Epoch: [37][40/97]\tTime  0.178 ( 0.184)\tLoss -8.4312e-01 (-8.3638e-01)\n",
            "Epoch: [37][50/97]\tTime  0.177 ( 0.183)\tLoss -8.5480e-01 (-8.3934e-01)\n",
            "Epoch: [37][60/97]\tTime  0.178 ( 0.182)\tLoss -8.5572e-01 (-8.4080e-01)\n",
            "Epoch: [37][70/97]\tTime  0.178 ( 0.181)\tLoss -8.6276e-01 (-8.4261e-01)\n",
            "Epoch: [37][80/97]\tTime  0.178 ( 0.181)\tLoss -8.4474e-01 (-8.4341e-01)\n",
            "Epoch: [37][90/97]\tTime  0.178 ( 0.181)\tLoss -8.2493e-01 (-8.4416e-01)\n",
            "Training...\n",
            "Epoch: [38][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.3021e-01 (-8.3021e-01)\n",
            "Epoch: [38][10/97]\tTime  0.178 ( 0.202)\tLoss -8.4195e-01 (-8.5063e-01)\n",
            "Epoch: [38][20/97]\tTime  0.177 ( 0.191)\tLoss -8.5582e-01 (-8.4986e-01)\n",
            "Epoch: [38][30/97]\tTime  0.177 ( 0.186)\tLoss -8.4120e-01 (-8.5139e-01)\n",
            "Epoch: [38][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6208e-01 (-8.5241e-01)\n",
            "Epoch: [38][50/97]\tTime  0.177 ( 0.183)\tLoss -8.3899e-01 (-8.5300e-01)\n",
            "Epoch: [38][60/97]\tTime  0.179 ( 0.182)\tLoss -8.4534e-01 (-8.5296e-01)\n",
            "Epoch: [38][70/97]\tTime  0.178 ( 0.181)\tLoss -8.4867e-01 (-8.5194e-01)\n",
            "Epoch: [38][80/97]\tTime  0.177 ( 0.181)\tLoss -8.4628e-01 (-8.5151e-01)\n",
            "Epoch: [38][90/97]\tTime  0.177 ( 0.181)\tLoss -8.3112e-01 (-8.5101e-01)\n",
            "Training...\n",
            "Epoch: [39][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.3323e-01 (-8.3323e-01)\n",
            "Epoch: [39][10/97]\tTime  0.178 ( 0.203)\tLoss -8.3330e-01 (-8.3357e-01)\n",
            "Epoch: [39][20/97]\tTime  0.177 ( 0.191)\tLoss -8.4945e-01 (-8.3543e-01)\n",
            "Epoch: [39][30/97]\tTime  0.177 ( 0.186)\tLoss -8.5831e-01 (-8.4116e-01)\n",
            "Epoch: [39][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6379e-01 (-8.4456e-01)\n",
            "Epoch: [39][50/97]\tTime  0.177 ( 0.183)\tLoss -8.4758e-01 (-8.4772e-01)\n",
            "Epoch: [39][60/97]\tTime  0.177 ( 0.182)\tLoss -8.5145e-01 (-8.4930e-01)\n",
            "Epoch: [39][70/97]\tTime  0.178 ( 0.181)\tLoss -8.4316e-01 (-8.5079e-01)\n",
            "Epoch: [39][80/97]\tTime  0.178 ( 0.181)\tLoss -8.3935e-01 (-8.5241e-01)\n",
            "Epoch: [39][90/97]\tTime  0.177 ( 0.181)\tLoss -8.6134e-01 (-8.5315e-01)\n",
            "Training...\n",
            "Epoch: [40][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.4754e-01 (-8.4754e-01)\n",
            "Epoch: [40][10/97]\tTime  0.177 ( 0.203)\tLoss -8.4822e-01 (-8.5405e-01)\n",
            "Epoch: [40][20/97]\tTime  0.178 ( 0.191)\tLoss -8.5897e-01 (-8.5676e-01)\n",
            "Epoch: [40][30/97]\tTime  0.177 ( 0.187)\tLoss -8.4869e-01 (-8.5539e-01)\n",
            "Epoch: [40][40/97]\tTime  0.177 ( 0.184)\tLoss -8.3780e-01 (-8.5364e-01)\n",
            "Epoch: [40][50/97]\tTime  0.178 ( 0.183)\tLoss -8.5908e-01 (-8.5413e-01)\n",
            "Epoch: [40][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6538e-01 (-8.5370e-01)\n",
            "Epoch: [40][70/97]\tTime  0.177 ( 0.182)\tLoss -8.5382e-01 (-8.5330e-01)\n",
            "Epoch: [40][80/97]\tTime  0.178 ( 0.181)\tLoss -8.4772e-01 (-8.5197e-01)\n",
            "Epoch: [40][90/97]\tTime  0.178 ( 0.181)\tLoss -8.6551e-01 (-8.5235e-01)\n",
            "Validating...\n",
            "Top1: 0.5226151315789473\n",
            "Training...\n",
            "Epoch: [41][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7087e-01 (-8.7087e-01)\n",
            "Epoch: [41][10/97]\tTime  0.177 ( 0.203)\tLoss -8.4869e-01 (-8.5291e-01)\n",
            "Epoch: [41][20/97]\tTime  0.179 ( 0.191)\tLoss -8.4503e-01 (-8.5526e-01)\n",
            "Epoch: [41][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6485e-01 (-8.5455e-01)\n",
            "Epoch: [41][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7726e-01 (-8.5709e-01)\n",
            "Epoch: [41][50/97]\tTime  0.178 ( 0.183)\tLoss -8.5529e-01 (-8.5609e-01)\n",
            "Epoch: [41][60/97]\tTime  0.178 ( 0.182)\tLoss -8.5228e-01 (-8.5553e-01)\n",
            "Epoch: [41][70/97]\tTime  0.177 ( 0.182)\tLoss -8.4979e-01 (-8.5539e-01)\n",
            "Epoch: [41][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8106e-01 (-8.5759e-01)\n",
            "Epoch: [41][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7360e-01 (-8.5870e-01)\n",
            "Training...\n",
            "Epoch: [42][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.5111e-01 (-8.5111e-01)\n",
            "Epoch: [42][10/97]\tTime  0.177 ( 0.203)\tLoss -8.5544e-01 (-8.5684e-01)\n",
            "Epoch: [42][20/97]\tTime  0.177 ( 0.191)\tLoss -8.5379e-01 (-8.5351e-01)\n",
            "Epoch: [42][30/97]\tTime  0.177 ( 0.187)\tLoss -8.6335e-01 (-8.5756e-01)\n",
            "Epoch: [42][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6731e-01 (-8.6017e-01)\n",
            "Epoch: [42][50/97]\tTime  0.177 ( 0.183)\tLoss -8.5605e-01 (-8.5904e-01)\n",
            "Epoch: [42][60/97]\tTime  0.178 ( 0.182)\tLoss -8.4750e-01 (-8.5809e-01)\n",
            "Epoch: [42][70/97]\tTime  0.178 ( 0.182)\tLoss -8.6899e-01 (-8.5896e-01)\n",
            "Epoch: [42][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7657e-01 (-8.6071e-01)\n",
            "Epoch: [42][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7741e-01 (-8.6253e-01)\n",
            "Training...\n",
            "Epoch: [43][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.6987e-01 (-8.6987e-01)\n",
            "Epoch: [43][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7957e-01 (-8.6746e-01)\n",
            "Epoch: [43][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7516e-01 (-8.7201e-01)\n",
            "Epoch: [43][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6673e-01 (-8.7121e-01)\n",
            "Epoch: [43][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6769e-01 (-8.6977e-01)\n",
            "Epoch: [43][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6427e-01 (-8.7052e-01)\n",
            "Epoch: [43][60/97]\tTime  0.179 ( 0.182)\tLoss -8.6353e-01 (-8.7044e-01)\n",
            "Epoch: [43][70/97]\tTime  0.178 ( 0.181)\tLoss -8.6777e-01 (-8.7065e-01)\n",
            "Epoch: [43][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7047e-01 (-8.7056e-01)\n",
            "Epoch: [43][90/97]\tTime  0.177 ( 0.181)\tLoss -8.5819e-01 (-8.6939e-01)\n",
            "Training...\n",
            "Epoch: [44][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.7093e-01 (-8.7093e-01)\n",
            "Epoch: [44][10/97]\tTime  0.178 ( 0.202)\tLoss -8.6478e-01 (-8.6476e-01)\n",
            "Epoch: [44][20/97]\tTime  0.177 ( 0.190)\tLoss -8.4333e-01 (-8.6123e-01)\n",
            "Epoch: [44][30/97]\tTime  0.177 ( 0.186)\tLoss -8.5791e-01 (-8.6135e-01)\n",
            "Epoch: [44][40/97]\tTime  0.179 ( 0.184)\tLoss -8.6080e-01 (-8.6240e-01)\n",
            "Epoch: [44][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6433e-01 (-8.6226e-01)\n",
            "Epoch: [44][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6238e-01 (-8.6257e-01)\n",
            "Epoch: [44][70/97]\tTime  0.178 ( 0.181)\tLoss -8.5752e-01 (-8.6161e-01)\n",
            "Epoch: [44][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6865e-01 (-8.6285e-01)\n",
            "Epoch: [44][90/97]\tTime  0.178 ( 0.181)\tLoss -8.4265e-01 (-8.6284e-01)\n",
            "Training...\n",
            "Epoch: [45][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.6104e-01 (-8.6104e-01)\n",
            "Epoch: [45][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8811e-01 (-8.8087e-01)\n",
            "Epoch: [45][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8017e-01 (-8.7973e-01)\n",
            "Epoch: [45][30/97]\tTime  0.178 ( 0.187)\tLoss -8.6533e-01 (-8.7353e-01)\n",
            "Epoch: [45][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8324e-01 (-8.7272e-01)\n",
            "Epoch: [45][50/97]\tTime  0.178 ( 0.183)\tLoss -8.5008e-01 (-8.7183e-01)\n",
            "Epoch: [45][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6286e-01 (-8.7031e-01)\n",
            "Epoch: [45][70/97]\tTime  0.177 ( 0.182)\tLoss -8.3055e-01 (-8.6838e-01)\n",
            "Epoch: [45][80/97]\tTime  0.177 ( 0.181)\tLoss -8.5764e-01 (-8.6618e-01)\n",
            "Epoch: [45][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7797e-01 (-8.6610e-01)\n",
            "Validating...\n",
            "Top1: 0.5453330592105263\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [46][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.5421e-01 (-8.5421e-01)\n",
            "Epoch: [46][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7090e-01 (-8.6236e-01)\n",
            "Epoch: [46][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8296e-01 (-8.7028e-01)\n",
            "Epoch: [46][30/97]\tTime  0.180 ( 0.187)\tLoss -8.5882e-01 (-8.7085e-01)\n",
            "Epoch: [46][40/97]\tTime  0.178 ( 0.185)\tLoss -8.5840e-01 (-8.6860e-01)\n",
            "Epoch: [46][50/97]\tTime  0.178 ( 0.184)\tLoss -8.6097e-01 (-8.6858e-01)\n",
            "Epoch: [46][60/97]\tTime  0.179 ( 0.183)\tLoss -8.7255e-01 (-8.6968e-01)\n",
            "Epoch: [46][70/97]\tTime  0.179 ( 0.182)\tLoss -8.7071e-01 (-8.7003e-01)\n",
            "Epoch: [46][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7284e-01 (-8.7117e-01)\n",
            "Epoch: [46][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7295e-01 (-8.7147e-01)\n",
            "Training...\n",
            "Epoch: [47][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.8461e-01 (-8.8461e-01)\n",
            "Epoch: [47][10/97]\tTime  0.179 ( 0.202)\tLoss -8.7371e-01 (-8.7950e-01)\n",
            "Epoch: [47][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7883e-01 (-8.7774e-01)\n",
            "Epoch: [47][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6916e-01 (-8.7492e-01)\n",
            "Epoch: [47][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6015e-01 (-8.7293e-01)\n",
            "Epoch: [47][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7157e-01 (-8.7179e-01)\n",
            "Epoch: [47][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7801e-01 (-8.7101e-01)\n",
            "Epoch: [47][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6740e-01 (-8.7176e-01)\n",
            "Epoch: [47][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8449e-01 (-8.7146e-01)\n",
            "Epoch: [47][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8152e-01 (-8.7219e-01)\n",
            "Training...\n",
            "Epoch: [48][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.7845e-01 (-8.7845e-01)\n",
            "Epoch: [48][10/97]\tTime  0.177 ( 0.203)\tLoss -8.7900e-01 (-8.8089e-01)\n",
            "Epoch: [48][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7283e-01 (-8.7682e-01)\n",
            "Epoch: [48][30/97]\tTime  0.178 ( 0.186)\tLoss -8.6594e-01 (-8.7562e-01)\n",
            "Epoch: [48][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7914e-01 (-8.7408e-01)\n",
            "Epoch: [48][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8131e-01 (-8.7433e-01)\n",
            "Epoch: [48][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8176e-01 (-8.7612e-01)\n",
            "Epoch: [48][70/97]\tTime  0.177 ( 0.181)\tLoss -8.4739e-01 (-8.7626e-01)\n",
            "Epoch: [48][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8215e-01 (-8.7623e-01)\n",
            "Epoch: [48][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8551e-01 (-8.7639e-01)\n",
            "Training...\n",
            "Epoch: [49][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.7884e-01 (-8.7884e-01)\n",
            "Epoch: [49][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7748e-01 (-8.7712e-01)\n",
            "Epoch: [49][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9508e-01 (-8.8233e-01)\n",
            "Epoch: [49][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8266e-01 (-8.8642e-01)\n",
            "Epoch: [49][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8500e-01 (-8.8610e-01)\n",
            "Epoch: [49][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8859e-01 (-8.8550e-01)\n",
            "Epoch: [49][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7817e-01 (-8.8601e-01)\n",
            "Epoch: [49][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7855e-01 (-8.8441e-01)\n",
            "Epoch: [49][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8850e-01 (-8.8359e-01)\n",
            "Epoch: [49][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7918e-01 (-8.8362e-01)\n",
            "Training...\n",
            "Epoch: [50][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8293e-01 (-8.8293e-01)\n",
            "Epoch: [50][10/97]\tTime  0.178 ( 0.203)\tLoss -8.9111e-01 (-8.8811e-01)\n",
            "Epoch: [50][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7653e-01 (-8.8550e-01)\n",
            "Epoch: [50][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8384e-01 (-8.8478e-01)\n",
            "Epoch: [50][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9271e-01 (-8.8496e-01)\n",
            "Epoch: [50][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9802e-01 (-8.8589e-01)\n",
            "Epoch: [50][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9013e-01 (-8.8506e-01)\n",
            "Epoch: [50][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7793e-01 (-8.8324e-01)\n",
            "Epoch: [50][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9174e-01 (-8.8274e-01)\n",
            "Epoch: [50][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9378e-01 (-8.8329e-01)\n",
            "Validating...\n",
            "Top1: 0.5401932565789473\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [51][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8106e-01 (-8.8106e-01)\n",
            "Epoch: [51][10/97]\tTime  0.178 ( 0.203)\tLoss -9.0366e-01 (-8.9571e-01)\n",
            "Epoch: [51][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8692e-01 (-8.9151e-01)\n",
            "Epoch: [51][30/97]\tTime  0.179 ( 0.187)\tLoss -8.9139e-01 (-8.9251e-01)\n",
            "Epoch: [51][40/97]\tTime  0.179 ( 0.185)\tLoss -8.8923e-01 (-8.9181e-01)\n",
            "Epoch: [51][50/97]\tTime  0.179 ( 0.184)\tLoss -8.8862e-01 (-8.9156e-01)\n",
            "Epoch: [51][60/97]\tTime  0.178 ( 0.183)\tLoss -8.9216e-01 (-8.9038e-01)\n",
            "Epoch: [51][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8199e-01 (-8.8926e-01)\n",
            "Epoch: [51][80/97]\tTime  0.179 ( 0.182)\tLoss -8.9108e-01 (-8.8893e-01)\n",
            "Epoch: [51][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8667e-01 (-8.8868e-01)\n",
            "Training...\n",
            "Epoch: [52][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9045e-01 (-8.9045e-01)\n",
            "Epoch: [52][10/97]\tTime  0.178 ( 0.203)\tLoss -8.9810e-01 (-8.9211e-01)\n",
            "Epoch: [52][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8266e-01 (-8.8876e-01)\n",
            "Epoch: [52][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8465e-01 (-8.8713e-01)\n",
            "Epoch: [52][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8759e-01 (-8.8710e-01)\n",
            "Epoch: [52][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9645e-01 (-8.8804e-01)\n",
            "Epoch: [52][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7639e-01 (-8.8883e-01)\n",
            "Epoch: [52][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7747e-01 (-8.8782e-01)\n",
            "Epoch: [52][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7375e-01 (-8.8730e-01)\n",
            "Epoch: [52][90/97]\tTime  0.177 ( 0.181)\tLoss -8.6044e-01 (-8.8559e-01)\n",
            "Training...\n",
            "Epoch: [53][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.7529e-01 (-8.7529e-01)\n",
            "Epoch: [53][10/97]\tTime  0.177 ( 0.202)\tLoss -8.6677e-01 (-8.7207e-01)\n",
            "Epoch: [53][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8523e-01 (-8.7492e-01)\n",
            "Epoch: [53][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7652e-01 (-8.7491e-01)\n",
            "Epoch: [53][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7449e-01 (-8.7622e-01)\n",
            "Epoch: [53][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9253e-01 (-8.7807e-01)\n",
            "Epoch: [53][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8912e-01 (-8.8017e-01)\n",
            "Epoch: [53][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7956e-01 (-8.8037e-01)\n",
            "Epoch: [53][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8044e-01 (-8.7988e-01)\n",
            "Epoch: [53][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7153e-01 (-8.7948e-01)\n",
            "Training...\n",
            "Epoch: [54][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.6753e-01 (-8.6753e-01)\n",
            "Epoch: [54][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9006e-01 (-8.7958e-01)\n",
            "Epoch: [54][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8251e-01 (-8.8050e-01)\n",
            "Epoch: [54][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8828e-01 (-8.8056e-01)\n",
            "Epoch: [54][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8980e-01 (-8.8193e-01)\n",
            "Epoch: [54][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7527e-01 (-8.8322e-01)\n",
            "Epoch: [54][60/97]\tTime  0.178 ( 0.182)\tLoss -9.0455e-01 (-8.8333e-01)\n",
            "Epoch: [54][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7589e-01 (-8.8262e-01)\n",
            "Epoch: [54][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7031e-01 (-8.8213e-01)\n",
            "Epoch: [54][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8391e-01 (-8.8235e-01)\n",
            "Training...\n",
            "Epoch: [55][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.7114e-01 (-8.7114e-01)\n",
            "Epoch: [55][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8938e-01 (-8.8479e-01)\n",
            "Epoch: [55][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8802e-01 (-8.8526e-01)\n",
            "Epoch: [55][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8688e-01 (-8.8204e-01)\n",
            "Epoch: [55][40/97]\tTime  0.179 ( 0.184)\tLoss -8.7731e-01 (-8.7942e-01)\n",
            "Epoch: [55][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9631e-01 (-8.7929e-01)\n",
            "Epoch: [55][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8633e-01 (-8.7961e-01)\n",
            "Epoch: [55][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7993e-01 (-8.8062e-01)\n",
            "Epoch: [55][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7804e-01 (-8.8046e-01)\n",
            "Epoch: [55][90/97]\tTime  0.178 ( 0.181)\tLoss -8.9934e-01 (-8.8128e-01)\n",
            "Validating...\n",
            "Top1: 0.5365953947368421\n",
            "Training...\n",
            "Epoch: [56][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8995e-01 (-8.8995e-01)\n",
            "Epoch: [56][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8607e-01 (-8.9568e-01)\n",
            "Epoch: [56][20/97]\tTime  0.177 ( 0.191)\tLoss -9.0491e-01 (-8.9221e-01)\n",
            "Epoch: [56][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9360e-01 (-8.9256e-01)\n",
            "Epoch: [56][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9256e-01 (-8.9335e-01)\n",
            "Epoch: [56][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1054e-01 (-8.9454e-01)\n",
            "Epoch: [56][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0700e-01 (-8.9536e-01)\n",
            "Epoch: [56][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1257e-01 (-8.9591e-01)\n",
            "Epoch: [56][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1560e-01 (-8.9650e-01)\n",
            "Epoch: [56][90/97]\tTime  0.179 ( 0.181)\tLoss -8.9870e-01 (-8.9661e-01)\n",
            "Training...\n",
            "Epoch: [57][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9260e-01 (-8.9260e-01)\n",
            "Epoch: [57][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8869e-01 (-8.9618e-01)\n",
            "Epoch: [57][20/97]\tTime  0.179 ( 0.191)\tLoss -8.9835e-01 (-8.9798e-01)\n",
            "Epoch: [57][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8349e-01 (-8.9521e-01)\n",
            "Epoch: [57][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0113e-01 (-8.9539e-01)\n",
            "Epoch: [57][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9931e-01 (-8.9486e-01)\n",
            "Epoch: [57][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6950e-01 (-8.9360e-01)\n",
            "Epoch: [57][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8858e-01 (-8.9270e-01)\n",
            "Epoch: [57][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9883e-01 (-8.9301e-01)\n",
            "Epoch: [57][90/97]\tTime  0.177 ( 0.181)\tLoss -9.0353e-01 (-8.9356e-01)\n",
            "Training...\n",
            "Epoch: [58][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8039e-01 (-8.8039e-01)\n",
            "Epoch: [58][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0623e-01 (-9.0092e-01)\n",
            "Epoch: [58][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0508e-01 (-8.9864e-01)\n",
            "Epoch: [58][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0035e-01 (-8.9862e-01)\n",
            "Epoch: [58][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6490e-01 (-8.9658e-01)\n",
            "Epoch: [58][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8598e-01 (-8.9551e-01)\n",
            "Epoch: [58][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9425e-01 (-8.9487e-01)\n",
            "Epoch: [58][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9200e-01 (-8.9353e-01)\n",
            "Epoch: [58][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8802e-01 (-8.9321e-01)\n",
            "Epoch: [58][90/97]\tTime  0.179 ( 0.181)\tLoss -8.8539e-01 (-8.9248e-01)\n",
            "Training...\n",
            "Epoch: [59][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.9854e-01 (-8.9854e-01)\n",
            "Epoch: [59][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8541e-01 (-8.9275e-01)\n",
            "Epoch: [59][20/97]\tTime  0.179 ( 0.190)\tLoss -8.9407e-01 (-8.8975e-01)\n",
            "Epoch: [59][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9971e-01 (-8.8985e-01)\n",
            "Epoch: [59][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9007e-01 (-8.8858e-01)\n",
            "Epoch: [59][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7565e-01 (-8.8843e-01)\n",
            "Epoch: [59][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9342e-01 (-8.8809e-01)\n",
            "Epoch: [59][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9369e-01 (-8.8810e-01)\n",
            "Epoch: [59][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9851e-01 (-8.8805e-01)\n",
            "Epoch: [59][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7997e-01 (-8.8768e-01)\n",
            "Training...\n",
            "Epoch: [60][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.7644e-01 (-8.7644e-01)\n",
            "Epoch: [60][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8523e-01 (-8.8610e-01)\n",
            "Epoch: [60][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7201e-01 (-8.8494e-01)\n",
            "Epoch: [60][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8216e-01 (-8.8524e-01)\n",
            "Epoch: [60][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8376e-01 (-8.8565e-01)\n",
            "Epoch: [60][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6934e-01 (-8.8527e-01)\n",
            "Epoch: [60][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8376e-01 (-8.8559e-01)\n",
            "Epoch: [60][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8890e-01 (-8.8646e-01)\n",
            "Epoch: [60][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9266e-01 (-8.8759e-01)\n",
            "Epoch: [60][90/97]\tTime  0.179 ( 0.180)\tLoss -8.9473e-01 (-8.8834e-01)\n",
            "Validating...\n",
            "Top1: 0.5519120065789473\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [61][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.8105e-01 (-8.8105e-01)\n",
            "Epoch: [61][10/97]\tTime  0.179 ( 0.203)\tLoss -8.8045e-01 (-8.8920e-01)\n",
            "Epoch: [61][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9637e-01 (-8.9389e-01)\n",
            "Epoch: [61][30/97]\tTime  0.179 ( 0.187)\tLoss -8.8844e-01 (-8.9368e-01)\n",
            "Epoch: [61][40/97]\tTime  0.178 ( 0.185)\tLoss -9.0502e-01 (-8.9210e-01)\n",
            "Epoch: [61][50/97]\tTime  0.179 ( 0.184)\tLoss -8.8570e-01 (-8.9192e-01)\n",
            "Epoch: [61][60/97]\tTime  0.178 ( 0.183)\tLoss -8.9008e-01 (-8.9241e-01)\n",
            "Epoch: [61][70/97]\tTime  0.179 ( 0.182)\tLoss -9.0683e-01 (-8.9209e-01)\n",
            "Epoch: [61][80/97]\tTime  0.178 ( 0.182)\tLoss -8.9335e-01 (-8.9204e-01)\n",
            "Epoch: [61][90/97]\tTime  0.178 ( 0.181)\tLoss -9.0185e-01 (-8.9313e-01)\n",
            "Training...\n",
            "Epoch: [62][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1338e-01 (-9.1338e-01)\n",
            "Epoch: [62][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9192e-01 (-9.0264e-01)\n",
            "Epoch: [62][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9918e-01 (-8.9980e-01)\n",
            "Epoch: [62][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0347e-01 (-8.9868e-01)\n",
            "Epoch: [62][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9553e-01 (-8.9719e-01)\n",
            "Epoch: [62][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9901e-01 (-8.9713e-01)\n",
            "Epoch: [62][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1162e-01 (-8.9726e-01)\n",
            "Epoch: [62][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9676e-01 (-8.9772e-01)\n",
            "Epoch: [62][80/97]\tTime  0.178 ( 0.181)\tLoss -9.0451e-01 (-8.9749e-01)\n",
            "Epoch: [62][90/97]\tTime  0.177 ( 0.181)\tLoss -8.6956e-01 (-8.9661e-01)\n",
            "Training...\n",
            "Epoch: [63][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8575e-01 (-8.8575e-01)\n",
            "Epoch: [63][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8977e-01 (-8.9792e-01)\n",
            "Epoch: [63][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8037e-01 (-8.9759e-01)\n",
            "Epoch: [63][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9875e-01 (-8.9674e-01)\n",
            "Epoch: [63][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8839e-01 (-8.9689e-01)\n",
            "Epoch: [63][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9808e-01 (-8.9588e-01)\n",
            "Epoch: [63][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9885e-01 (-8.9656e-01)\n",
            "Epoch: [63][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8899e-01 (-8.9673e-01)\n",
            "Epoch: [63][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9826e-01 (-8.9703e-01)\n",
            "Epoch: [63][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7967e-01 (-8.9708e-01)\n",
            "Training...\n",
            "Epoch: [64][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0526e-01 (-9.0526e-01)\n",
            "Epoch: [64][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7968e-01 (-8.9601e-01)\n",
            "Epoch: [64][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9942e-01 (-8.9643e-01)\n",
            "Epoch: [64][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9838e-01 (-8.9444e-01)\n",
            "Epoch: [64][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9298e-01 (-8.9295e-01)\n",
            "Epoch: [64][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8893e-01 (-8.9198e-01)\n",
            "Epoch: [64][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0387e-01 (-8.9300e-01)\n",
            "Epoch: [64][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9630e-01 (-8.9382e-01)\n",
            "Epoch: [64][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7642e-01 (-8.9378e-01)\n",
            "Epoch: [64][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9701e-01 (-8.9357e-01)\n",
            "Training...\n",
            "Epoch: [65][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8948e-01 (-8.8948e-01)\n",
            "Epoch: [65][10/97]\tTime  0.179 ( 0.203)\tLoss -9.0044e-01 (-8.9579e-01)\n",
            "Epoch: [65][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8164e-01 (-8.9490e-01)\n",
            "Epoch: [65][30/97]\tTime  0.178 ( 0.187)\tLoss -8.9411e-01 (-8.9275e-01)\n",
            "Epoch: [65][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8258e-01 (-8.9267e-01)\n",
            "Epoch: [65][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9160e-01 (-8.9336e-01)\n",
            "Epoch: [65][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1786e-01 (-8.9446e-01)\n",
            "Epoch: [65][70/97]\tTime  0.178 ( 0.182)\tLoss -8.9497e-01 (-8.9494e-01)\n",
            "Epoch: [65][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7701e-01 (-8.9508e-01)\n",
            "Epoch: [65][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9538e-01 (-8.9493e-01)\n",
            "Validating...\n",
            "Top1: 0.5714432565789473\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [66][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9030e-01 (-8.9030e-01)\n",
            "Epoch: [66][10/97]\tTime  0.178 ( 0.203)\tLoss -8.9589e-01 (-8.9264e-01)\n",
            "Epoch: [66][20/97]\tTime  0.179 ( 0.191)\tLoss -8.9022e-01 (-8.9563e-01)\n",
            "Epoch: [66][30/97]\tTime  0.178 ( 0.187)\tLoss -8.9537e-01 (-8.9433e-01)\n",
            "Epoch: [66][40/97]\tTime  0.178 ( 0.185)\tLoss -9.0326e-01 (-8.9463e-01)\n",
            "Epoch: [66][50/97]\tTime  0.178 ( 0.184)\tLoss -8.9262e-01 (-8.9365e-01)\n",
            "Epoch: [66][60/97]\tTime  0.179 ( 0.183)\tLoss -8.9784e-01 (-8.9441e-01)\n",
            "Epoch: [66][70/97]\tTime  0.178 ( 0.182)\tLoss -9.0173e-01 (-8.9481e-01)\n",
            "Epoch: [66][80/97]\tTime  0.178 ( 0.182)\tLoss -8.9563e-01 (-8.9563e-01)\n",
            "Epoch: [66][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8472e-01 (-8.9554e-01)\n",
            "Training...\n",
            "Epoch: [67][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7643e-01 (-8.7643e-01)\n",
            "Epoch: [67][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0135e-01 (-8.9902e-01)\n",
            "Epoch: [67][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7372e-01 (-8.9862e-01)\n",
            "Epoch: [67][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9127e-01 (-8.9609e-01)\n",
            "Epoch: [67][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8951e-01 (-8.9418e-01)\n",
            "Epoch: [67][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9634e-01 (-8.9376e-01)\n",
            "Epoch: [67][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9040e-01 (-8.9332e-01)\n",
            "Epoch: [67][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9787e-01 (-8.9276e-01)\n",
            "Epoch: [67][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9052e-01 (-8.9194e-01)\n",
            "Epoch: [67][90/97]\tTime  0.178 ( 0.181)\tLoss -9.0355e-01 (-8.9227e-01)\n",
            "Training...\n",
            "Epoch: [68][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9546e-01 (-8.9546e-01)\n",
            "Epoch: [68][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9726e-01 (-8.9884e-01)\n",
            "Epoch: [68][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9971e-01 (-8.9710e-01)\n",
            "Epoch: [68][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9947e-01 (-8.9670e-01)\n",
            "Epoch: [68][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9724e-01 (-8.9726e-01)\n",
            "Epoch: [68][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9915e-01 (-8.9577e-01)\n",
            "Epoch: [68][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6991e-01 (-8.9358e-01)\n",
            "Epoch: [68][70/97]\tTime  0.179 ( 0.181)\tLoss -8.9879e-01 (-8.9278e-01)\n",
            "Epoch: [68][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0009e-01 (-8.9329e-01)\n",
            "Epoch: [68][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9847e-01 (-8.9329e-01)\n",
            "Training...\n",
            "Epoch: [69][ 0/97]\tTime  0.460 ( 0.460)\tLoss -8.9355e-01 (-8.9355e-01)\n",
            "Epoch: [69][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0207e-01 (-8.9522e-01)\n",
            "Epoch: [69][20/97]\tTime  0.177 ( 0.191)\tLoss -9.0257e-01 (-8.9744e-01)\n",
            "Epoch: [69][30/97]\tTime  0.178 ( 0.187)\tLoss -8.9770e-01 (-8.9751e-01)\n",
            "Epoch: [69][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8233e-01 (-8.9720e-01)\n",
            "Epoch: [69][50/97]\tTime  0.178 ( 0.183)\tLoss -9.0117e-01 (-8.9612e-01)\n",
            "Epoch: [69][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9645e-01 (-8.9566e-01)\n",
            "Epoch: [69][70/97]\tTime  0.177 ( 0.182)\tLoss -8.8601e-01 (-8.9594e-01)\n",
            "Epoch: [69][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9032e-01 (-8.9669e-01)\n",
            "Epoch: [69][90/97]\tTime  0.178 ( 0.181)\tLoss -9.0484e-01 (-8.9672e-01)\n",
            "Training...\n",
            "Epoch: [70][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9309e-01 (-8.9309e-01)\n",
            "Epoch: [70][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0619e-01 (-8.9936e-01)\n",
            "Epoch: [70][20/97]\tTime  0.178 ( 0.191)\tLoss -9.0709e-01 (-9.0009e-01)\n",
            "Epoch: [70][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0289e-01 (-8.9997e-01)\n",
            "Epoch: [70][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0414e-01 (-9.0041e-01)\n",
            "Epoch: [70][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0325e-01 (-9.0046e-01)\n",
            "Epoch: [70][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8588e-01 (-9.0011e-01)\n",
            "Epoch: [70][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9541e-01 (-8.9896e-01)\n",
            "Epoch: [70][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9906e-01 (-8.9915e-01)\n",
            "Epoch: [70][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9060e-01 (-8.9903e-01)\n",
            "Validating...\n",
            "Top1: 0.580078125\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [71][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8608e-01 (-8.8608e-01)\n",
            "Epoch: [71][10/97]\tTime  0.178 ( 0.203)\tLoss -9.0340e-01 (-8.9750e-01)\n",
            "Epoch: [71][20/97]\tTime  0.178 ( 0.192)\tLoss -9.0490e-01 (-8.9807e-01)\n",
            "Epoch: [71][30/97]\tTime  0.179 ( 0.187)\tLoss -8.9898e-01 (-8.9841e-01)\n",
            "Epoch: [71][40/97]\tTime  0.178 ( 0.185)\tLoss -9.1312e-01 (-9.0004e-01)\n",
            "Epoch: [71][50/97]\tTime  0.178 ( 0.184)\tLoss -9.0080e-01 (-9.0122e-01)\n",
            "Epoch: [71][60/97]\tTime  0.179 ( 0.183)\tLoss -8.9348e-01 (-9.0138e-01)\n",
            "Epoch: [71][70/97]\tTime  0.178 ( 0.182)\tLoss -8.9736e-01 (-9.0131e-01)\n",
            "Epoch: [71][80/97]\tTime  0.179 ( 0.182)\tLoss -9.0488e-01 (-9.0127e-01)\n",
            "Epoch: [71][90/97]\tTime  0.178 ( 0.181)\tLoss -9.2776e-01 (-9.0281e-01)\n",
            "Training...\n",
            "Epoch: [72][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0189e-01 (-9.0189e-01)\n",
            "Epoch: [72][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8783e-01 (-9.0086e-01)\n",
            "Epoch: [72][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9836e-01 (-9.0095e-01)\n",
            "Epoch: [72][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0589e-01 (-9.0241e-01)\n",
            "Epoch: [72][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0200e-01 (-9.0271e-01)\n",
            "Epoch: [72][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8048e-01 (-9.0134e-01)\n",
            "Epoch: [72][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1482e-01 (-9.0170e-01)\n",
            "Epoch: [72][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9282e-01 (-9.0198e-01)\n",
            "Epoch: [72][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9467e-01 (-9.0189e-01)\n",
            "Epoch: [72][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9149e-01 (-9.0126e-01)\n",
            "Training...\n",
            "Epoch: [73][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.9656e-01 (-8.9656e-01)\n",
            "Epoch: [73][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9386e-01 (-8.9892e-01)\n",
            "Epoch: [73][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9848e-01 (-8.9666e-01)\n",
            "Epoch: [73][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9756e-01 (-8.9692e-01)\n",
            "Epoch: [73][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9757e-01 (-8.9635e-01)\n",
            "Epoch: [73][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0399e-01 (-8.9651e-01)\n",
            "Epoch: [73][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9907e-01 (-8.9699e-01)\n",
            "Epoch: [73][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9472e-01 (-8.9705e-01)\n",
            "Epoch: [73][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9300e-01 (-8.9657e-01)\n",
            "Epoch: [73][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9376e-01 (-8.9664e-01)\n",
            "Training...\n",
            "Epoch: [74][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8251e-01 (-8.8251e-01)\n",
            "Epoch: [74][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0877e-01 (-8.9910e-01)\n",
            "Epoch: [74][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9990e-01 (-8.9863e-01)\n",
            "Epoch: [74][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8859e-01 (-8.9697e-01)\n",
            "Epoch: [74][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8297e-01 (-8.9622e-01)\n",
            "Epoch: [74][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0166e-01 (-8.9562e-01)\n",
            "Epoch: [74][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9929e-01 (-8.9495e-01)\n",
            "Epoch: [74][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0956e-01 (-8.9532e-01)\n",
            "Epoch: [74][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0401e-01 (-8.9656e-01)\n",
            "Epoch: [74][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0403e-01 (-8.9688e-01)\n",
            "Training...\n",
            "Epoch: [75][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9609e-01 (-8.9609e-01)\n",
            "Epoch: [75][10/97]\tTime  0.178 ( 0.203)\tLoss -9.0394e-01 (-9.0213e-01)\n",
            "Epoch: [75][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9797e-01 (-9.0433e-01)\n",
            "Epoch: [75][30/97]\tTime  0.177 ( 0.187)\tLoss -8.8837e-01 (-9.0235e-01)\n",
            "Epoch: [75][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8894e-01 (-9.0079e-01)\n",
            "Epoch: [75][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9305e-01 (-8.9976e-01)\n",
            "Epoch: [75][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7867e-01 (-8.9848e-01)\n",
            "Epoch: [75][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7965e-01 (-8.9748e-01)\n",
            "Epoch: [75][80/97]\tTime  0.178 ( 0.181)\tLoss -9.0834e-01 (-8.9811e-01)\n",
            "Epoch: [75][90/97]\tTime  0.177 ( 0.181)\tLoss -9.0699e-01 (-8.9821e-01)\n",
            "Validating...\n",
            "Top1: 0.5872738486842105\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [76][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9960e-01 (-8.9960e-01)\n",
            "Epoch: [76][10/97]\tTime  0.178 ( 0.203)\tLoss -9.0788e-01 (-9.0318e-01)\n",
            "Epoch: [76][20/97]\tTime  0.178 ( 0.191)\tLoss -9.0293e-01 (-9.0452e-01)\n",
            "Epoch: [76][30/97]\tTime  0.178 ( 0.187)\tLoss -8.8971e-01 (-9.0291e-01)\n",
            "Epoch: [76][40/97]\tTime  0.179 ( 0.185)\tLoss -9.1497e-01 (-9.0282e-01)\n",
            "Epoch: [76][50/97]\tTime  0.178 ( 0.184)\tLoss -8.8925e-01 (-9.0095e-01)\n",
            "Epoch: [76][60/97]\tTime  0.178 ( 0.183)\tLoss -9.0090e-01 (-9.0035e-01)\n",
            "Epoch: [76][70/97]\tTime  0.178 ( 0.182)\tLoss -9.1222e-01 (-9.0040e-01)\n",
            "Epoch: [76][80/97]\tTime  0.178 ( 0.182)\tLoss -9.0316e-01 (-9.0043e-01)\n",
            "Epoch: [76][90/97]\tTime  0.179 ( 0.181)\tLoss -8.9094e-01 (-8.9964e-01)\n",
            "Training...\n",
            "Epoch: [77][ 0/97]\tTime  0.439 ( 0.439)\tLoss -8.8945e-01 (-8.8945e-01)\n",
            "Epoch: [77][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9686e-01 (-8.9306e-01)\n",
            "Epoch: [77][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9626e-01 (-8.9277e-01)\n",
            "Epoch: [77][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0114e-01 (-8.9272e-01)\n",
            "Epoch: [77][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8729e-01 (-8.9402e-01)\n",
            "Epoch: [77][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1152e-01 (-8.9651e-01)\n",
            "Epoch: [77][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1132e-01 (-8.9725e-01)\n",
            "Epoch: [77][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9533e-01 (-8.9665e-01)\n",
            "Epoch: [77][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0205e-01 (-8.9627e-01)\n",
            "Epoch: [77][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9359e-01 (-8.9636e-01)\n",
            "Training...\n",
            "Epoch: [78][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9037e-01 (-8.9037e-01)\n",
            "Epoch: [78][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0428e-01 (-8.9633e-01)\n",
            "Epoch: [78][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8703e-01 (-8.9434e-01)\n",
            "Epoch: [78][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0743e-01 (-8.9663e-01)\n",
            "Epoch: [78][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1652e-01 (-8.9929e-01)\n",
            "Epoch: [78][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1171e-01 (-9.0055e-01)\n",
            "Epoch: [78][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0365e-01 (-9.0072e-01)\n",
            "Epoch: [78][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0647e-01 (-9.0109e-01)\n",
            "Epoch: [78][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0399e-01 (-9.0127e-01)\n",
            "Epoch: [78][90/97]\tTime  0.178 ( 0.181)\tLoss -9.0059e-01 (-9.0126e-01)\n",
            "Training...\n",
            "Epoch: [79][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9120e-01 (-8.9120e-01)\n",
            "Epoch: [79][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0945e-01 (-8.9951e-01)\n",
            "Epoch: [79][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0197e-01 (-9.0025e-01)\n",
            "Epoch: [79][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9988e-01 (-9.0173e-01)\n",
            "Epoch: [79][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0457e-01 (-9.0278e-01)\n",
            "Epoch: [79][50/97]\tTime  0.178 ( 0.183)\tLoss -9.0264e-01 (-9.0248e-01)\n",
            "Epoch: [79][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9691e-01 (-9.0244e-01)\n",
            "Epoch: [79][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1444e-01 (-9.0303e-01)\n",
            "Epoch: [79][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0497e-01 (-9.0327e-01)\n",
            "Epoch: [79][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9599e-01 (-9.0330e-01)\n",
            "Training...\n",
            "Epoch: [80][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.1219e-01 (-9.1219e-01)\n",
            "Epoch: [80][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9915e-01 (-9.0196e-01)\n",
            "Epoch: [80][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9933e-01 (-9.0099e-01)\n",
            "Epoch: [80][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1356e-01 (-9.0127e-01)\n",
            "Epoch: [80][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0695e-01 (-9.0079e-01)\n",
            "Epoch: [80][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0068e-01 (-9.0138e-01)\n",
            "Epoch: [80][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8708e-01 (-9.0089e-01)\n",
            "Epoch: [80][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0731e-01 (-9.0036e-01)\n",
            "Epoch: [80][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0484e-01 (-9.0023e-01)\n",
            "Epoch: [80][90/97]\tTime  0.178 ( 0.181)\tLoss -8.9610e-01 (-9.0016e-01)\n",
            "Validating...\n",
            "Top1: 0.5801809210526315\n",
            "Training...\n",
            "Epoch: [81][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9777e-01 (-8.9777e-01)\n",
            "Epoch: [81][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8459e-01 (-8.9204e-01)\n",
            "Epoch: [81][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8678e-01 (-8.9523e-01)\n",
            "Epoch: [81][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9972e-01 (-8.9714e-01)\n",
            "Epoch: [81][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9939e-01 (-8.9762e-01)\n",
            "Epoch: [81][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9955e-01 (-8.9829e-01)\n",
            "Epoch: [81][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0592e-01 (-8.9909e-01)\n",
            "Epoch: [81][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9390e-01 (-8.9911e-01)\n",
            "Epoch: [81][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0839e-01 (-8.9981e-01)\n",
            "Epoch: [81][90/97]\tTime  0.178 ( 0.181)\tLoss -9.0511e-01 (-9.0034e-01)\n",
            "Training...\n",
            "Epoch: [82][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0343e-01 (-9.0343e-01)\n",
            "Epoch: [82][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0164e-01 (-9.0755e-01)\n",
            "Epoch: [82][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1615e-01 (-9.0708e-01)\n",
            "Epoch: [82][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9379e-01 (-9.0501e-01)\n",
            "Epoch: [82][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1832e-01 (-9.0488e-01)\n",
            "Epoch: [82][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0017e-01 (-9.0447e-01)\n",
            "Epoch: [82][60/97]\tTime  0.178 ( 0.182)\tLoss -9.0205e-01 (-9.0497e-01)\n",
            "Epoch: [82][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0049e-01 (-9.0408e-01)\n",
            "Epoch: [82][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1587e-01 (-9.0421e-01)\n",
            "Epoch: [82][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8917e-01 (-9.0397e-01)\n",
            "Training...\n",
            "Epoch: [83][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.1091e-01 (-9.1091e-01)\n",
            "Epoch: [83][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9700e-01 (-9.0119e-01)\n",
            "Epoch: [83][20/97]\tTime  0.177 ( 0.191)\tLoss -9.0189e-01 (-9.0207e-01)\n",
            "Epoch: [83][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0450e-01 (-9.0163e-01)\n",
            "Epoch: [83][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1037e-01 (-9.0176e-01)\n",
            "Epoch: [83][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0948e-01 (-9.0149e-01)\n",
            "Epoch: [83][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9765e-01 (-9.0157e-01)\n",
            "Epoch: [83][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1475e-01 (-9.0287e-01)\n",
            "Epoch: [83][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9443e-01 (-9.0368e-01)\n",
            "Epoch: [83][90/97]\tTime  0.178 ( 0.181)\tLoss -9.0623e-01 (-9.0373e-01)\n",
            "Training...\n",
            "Epoch: [84][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0309e-01 (-9.0309e-01)\n",
            "Epoch: [84][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0157e-01 (-9.0394e-01)\n",
            "Epoch: [84][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9913e-01 (-9.0413e-01)\n",
            "Epoch: [84][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9902e-01 (-9.0597e-01)\n",
            "Epoch: [84][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1142e-01 (-9.0774e-01)\n",
            "Epoch: [84][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2097e-01 (-9.0978e-01)\n",
            "Epoch: [84][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1230e-01 (-9.1029e-01)\n",
            "Epoch: [84][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1298e-01 (-9.1050e-01)\n",
            "Epoch: [84][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1778e-01 (-9.0990e-01)\n",
            "Epoch: [84][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1088e-01 (-9.0996e-01)\n",
            "Training...\n",
            "Epoch: [85][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.1884e-01 (-9.1884e-01)\n",
            "Epoch: [85][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0512e-01 (-9.0464e-01)\n",
            "Epoch: [85][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0691e-01 (-9.0516e-01)\n",
            "Epoch: [85][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0699e-01 (-9.0480e-01)\n",
            "Epoch: [85][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9869e-01 (-9.0319e-01)\n",
            "Epoch: [85][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9561e-01 (-9.0419e-01)\n",
            "Epoch: [85][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0165e-01 (-9.0421e-01)\n",
            "Epoch: [85][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0443e-01 (-9.0403e-01)\n",
            "Epoch: [85][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1555e-01 (-9.0420e-01)\n",
            "Epoch: [85][90/97]\tTime  0.179 ( 0.181)\tLoss -9.1464e-01 (-9.0438e-01)\n",
            "Validating...\n",
            "Top1: 0.5952919407894737\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [86][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.1794e-01 (-9.1794e-01)\n",
            "Epoch: [86][10/97]\tTime  0.178 ( 0.203)\tLoss -9.1297e-01 (-9.1061e-01)\n",
            "Epoch: [86][20/97]\tTime  0.178 ( 0.191)\tLoss -9.0151e-01 (-9.1232e-01)\n",
            "Epoch: [86][30/97]\tTime  0.178 ( 0.187)\tLoss -9.2275e-01 (-9.1227e-01)\n",
            "Epoch: [86][40/97]\tTime  0.178 ( 0.185)\tLoss -8.9955e-01 (-9.1131e-01)\n",
            "Epoch: [86][50/97]\tTime  0.179 ( 0.184)\tLoss -9.0675e-01 (-9.1097e-01)\n",
            "Epoch: [86][60/97]\tTime  0.178 ( 0.183)\tLoss -9.0150e-01 (-9.1032e-01)\n",
            "Epoch: [86][70/97]\tTime  0.178 ( 0.182)\tLoss -8.9143e-01 (-9.0877e-01)\n",
            "Epoch: [86][80/97]\tTime  0.179 ( 0.182)\tLoss -8.8993e-01 (-9.0760e-01)\n",
            "Epoch: [86][90/97]\tTime  0.178 ( 0.181)\tLoss -9.0398e-01 (-9.0720e-01)\n",
            "Training...\n",
            "Epoch: [87][ 0/97]\tTime  0.465 ( 0.465)\tLoss -9.1175e-01 (-9.1175e-01)\n",
            "Epoch: [87][10/97]\tTime  0.177 ( 0.203)\tLoss -9.2336e-01 (-9.1302e-01)\n",
            "Epoch: [87][20/97]\tTime  0.177 ( 0.191)\tLoss -9.0996e-01 (-9.1233e-01)\n",
            "Epoch: [87][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0813e-01 (-9.1236e-01)\n",
            "Epoch: [87][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2214e-01 (-9.1263e-01)\n",
            "Epoch: [87][50/97]\tTime  0.176 ( 0.183)\tLoss -9.0249e-01 (-9.1250e-01)\n",
            "Epoch: [87][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0671e-01 (-9.1162e-01)\n",
            "Epoch: [87][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9260e-01 (-9.1083e-01)\n",
            "Epoch: [87][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1144e-01 (-9.1024e-01)\n",
            "Epoch: [87][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0240e-01 (-9.1053e-01)\n",
            "Training...\n",
            "Epoch: [88][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.1481e-01 (-9.1481e-01)\n",
            "Epoch: [88][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1171e-01 (-9.0900e-01)\n",
            "Epoch: [88][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1128e-01 (-9.0913e-01)\n",
            "Epoch: [88][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1110e-01 (-9.1000e-01)\n",
            "Epoch: [88][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9028e-01 (-9.0973e-01)\n",
            "Epoch: [88][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0171e-01 (-9.0943e-01)\n",
            "Epoch: [88][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0727e-01 (-9.0905e-01)\n",
            "Epoch: [88][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8974e-01 (-9.0820e-01)\n",
            "Epoch: [88][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0281e-01 (-9.0736e-01)\n",
            "Epoch: [88][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9354e-01 (-9.0627e-01)\n",
            "Training...\n",
            "Epoch: [89][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9689e-01 (-8.9689e-01)\n",
            "Epoch: [89][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1161e-01 (-9.0519e-01)\n",
            "Epoch: [89][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1398e-01 (-9.0416e-01)\n",
            "Epoch: [89][30/97]\tTime  0.178 ( 0.186)\tLoss -9.1434e-01 (-9.0538e-01)\n",
            "Epoch: [89][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1447e-01 (-9.0603e-01)\n",
            "Epoch: [89][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1747e-01 (-9.0649e-01)\n",
            "Epoch: [89][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9953e-01 (-9.0643e-01)\n",
            "Epoch: [89][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1362e-01 (-9.0581e-01)\n",
            "Epoch: [89][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0490e-01 (-9.0578e-01)\n",
            "Epoch: [89][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8754e-01 (-9.0540e-01)\n",
            "Training...\n",
            "Epoch: [90][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0937e-01 (-9.0937e-01)\n",
            "Epoch: [90][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0244e-01 (-9.0512e-01)\n",
            "Epoch: [90][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0291e-01 (-9.0246e-01)\n",
            "Epoch: [90][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0976e-01 (-9.0350e-01)\n",
            "Epoch: [90][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9652e-01 (-9.0344e-01)\n",
            "Epoch: [90][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9580e-01 (-9.0400e-01)\n",
            "Epoch: [90][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1238e-01 (-9.0520e-01)\n",
            "Epoch: [90][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9872e-01 (-9.0586e-01)\n",
            "Epoch: [90][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9820e-01 (-9.0582e-01)\n",
            "Epoch: [90][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1692e-01 (-9.0563e-01)\n",
            "Validating...\n",
            "Top1: 0.5984786184210527\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [91][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.0936e-01 (-9.0936e-01)\n",
            "Epoch: [91][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0218e-01 (-9.0672e-01)\n",
            "Epoch: [91][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9393e-01 (-9.0468e-01)\n",
            "Epoch: [91][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1098e-01 (-9.0453e-01)\n",
            "Epoch: [91][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1063e-01 (-9.0525e-01)\n",
            "Epoch: [91][50/97]\tTime  0.178 ( 0.182)\tLoss -9.1146e-01 (-9.0583e-01)\n",
            "Epoch: [91][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1312e-01 (-9.0576e-01)\n",
            "Epoch: [91][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0877e-01 (-9.0549e-01)\n",
            "Epoch: [91][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1755e-01 (-9.0620e-01)\n",
            "Epoch: [91][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1061e-01 (-9.0630e-01)\n",
            "Training...\n",
            "Epoch: [92][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0908e-01 (-9.0908e-01)\n",
            "Epoch: [92][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9513e-01 (-9.0643e-01)\n",
            "Epoch: [92][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0819e-01 (-9.0390e-01)\n",
            "Epoch: [92][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0933e-01 (-9.0460e-01)\n",
            "Epoch: [92][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0735e-01 (-9.0596e-01)\n",
            "Epoch: [92][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0250e-01 (-9.0563e-01)\n",
            "Epoch: [92][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1005e-01 (-9.0586e-01)\n",
            "Epoch: [92][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0980e-01 (-9.0566e-01)\n",
            "Epoch: [92][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0385e-01 (-9.0557e-01)\n",
            "Epoch: [92][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1059e-01 (-9.0612e-01)\n",
            "Training...\n",
            "Epoch: [93][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.1252e-01 (-9.1252e-01)\n",
            "Epoch: [93][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9584e-01 (-9.0972e-01)\n",
            "Epoch: [93][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9810e-01 (-9.0874e-01)\n",
            "Epoch: [93][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0505e-01 (-9.0730e-01)\n",
            "Epoch: [93][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0354e-01 (-9.0585e-01)\n",
            "Epoch: [93][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9369e-01 (-9.0461e-01)\n",
            "Epoch: [93][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0732e-01 (-9.0409e-01)\n",
            "Epoch: [93][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0524e-01 (-9.0398e-01)\n",
            "Epoch: [93][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9220e-01 (-9.0349e-01)\n",
            "Epoch: [93][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1048e-01 (-9.0394e-01)\n",
            "Training...\n",
            "Epoch: [94][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.9355e-01 (-8.9355e-01)\n",
            "Epoch: [94][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0399e-01 (-9.0194e-01)\n",
            "Epoch: [94][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1244e-01 (-8.9962e-01)\n",
            "Epoch: [94][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9238e-01 (-9.0093e-01)\n",
            "Epoch: [94][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9917e-01 (-9.0028e-01)\n",
            "Epoch: [94][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1680e-01 (-9.0097e-01)\n",
            "Epoch: [94][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0092e-01 (-9.0129e-01)\n",
            "Epoch: [94][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0307e-01 (-9.0166e-01)\n",
            "Epoch: [94][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1266e-01 (-9.0197e-01)\n",
            "Epoch: [94][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1431e-01 (-9.0202e-01)\n",
            "Training...\n",
            "Epoch: [95][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.0871e-01 (-9.0871e-01)\n",
            "Epoch: [95][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1647e-01 (-8.9937e-01)\n",
            "Epoch: [95][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9478e-01 (-9.0030e-01)\n",
            "Epoch: [95][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1449e-01 (-9.0270e-01)\n",
            "Epoch: [95][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0866e-01 (-9.0368e-01)\n",
            "Epoch: [95][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0896e-01 (-9.0408e-01)\n",
            "Epoch: [95][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2222e-01 (-9.0560e-01)\n",
            "Epoch: [95][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1117e-01 (-9.0545e-01)\n",
            "Epoch: [95][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9924e-01 (-9.0475e-01)\n",
            "Epoch: [95][90/97]\tTime  0.178 ( 0.180)\tLoss -9.0867e-01 (-9.0457e-01)\n",
            "Validating...\n",
            "Top1: 0.6037212171052632\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [96][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.2519e-01 (-9.2519e-01)\n",
            "Epoch: [96][10/97]\tTime  0.179 ( 0.203)\tLoss -8.9462e-01 (-9.0203e-01)\n",
            "Epoch: [96][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9401e-01 (-9.0226e-01)\n",
            "Epoch: [96][30/97]\tTime  0.178 ( 0.187)\tLoss -9.0861e-01 (-9.0209e-01)\n",
            "Epoch: [96][40/97]\tTime  0.178 ( 0.185)\tLoss -8.8862e-01 (-9.0150e-01)\n",
            "Epoch: [96][50/97]\tTime  0.179 ( 0.184)\tLoss -9.0739e-01 (-9.0184e-01)\n",
            "Epoch: [96][60/97]\tTime  0.178 ( 0.183)\tLoss -9.0602e-01 (-9.0257e-01)\n",
            "Epoch: [96][70/97]\tTime  0.178 ( 0.182)\tLoss -9.0444e-01 (-9.0192e-01)\n",
            "Epoch: [96][80/97]\tTime  0.178 ( 0.182)\tLoss -9.1611e-01 (-9.0237e-01)\n",
            "Epoch: [96][90/97]\tTime  0.178 ( 0.181)\tLoss -9.1517e-01 (-9.0238e-01)\n",
            "Training...\n",
            "Epoch: [97][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.9443e-01 (-8.9443e-01)\n",
            "Epoch: [97][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0447e-01 (-9.0281e-01)\n",
            "Epoch: [97][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0660e-01 (-9.0210e-01)\n",
            "Epoch: [97][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9519e-01 (-9.0288e-01)\n",
            "Epoch: [97][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0663e-01 (-9.0192e-01)\n",
            "Epoch: [97][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9664e-01 (-9.0182e-01)\n",
            "Epoch: [97][60/97]\tTime  0.178 ( 0.182)\tLoss -9.0143e-01 (-9.0154e-01)\n",
            "Epoch: [97][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9912e-01 (-9.0167e-01)\n",
            "Epoch: [97][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9733e-01 (-9.0140e-01)\n",
            "Epoch: [97][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8453e-01 (-9.0139e-01)\n",
            "Training...\n",
            "Epoch: [98][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0655e-01 (-9.0655e-01)\n",
            "Epoch: [98][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9874e-01 (-9.0216e-01)\n",
            "Epoch: [98][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0520e-01 (-9.0266e-01)\n",
            "Epoch: [98][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9584e-01 (-9.0295e-01)\n",
            "Epoch: [98][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0344e-01 (-9.0160e-01)\n",
            "Epoch: [98][50/97]\tTime  0.178 ( 0.183)\tLoss -9.0254e-01 (-9.0091e-01)\n",
            "Epoch: [98][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9239e-01 (-9.0095e-01)\n",
            "Epoch: [98][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9606e-01 (-9.0012e-01)\n",
            "Epoch: [98][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9693e-01 (-8.9976e-01)\n",
            "Epoch: [98][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8134e-01 (-8.9894e-01)\n",
            "Training...\n",
            "Epoch: [99][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8533e-01 (-8.8533e-01)\n",
            "Epoch: [99][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0238e-01 (-8.9518e-01)\n",
            "Epoch: [99][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9752e-01 (-8.9765e-01)\n",
            "Epoch: [99][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0225e-01 (-8.9610e-01)\n",
            "Epoch: [99][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8860e-01 (-8.9410e-01)\n",
            "Epoch: [99][50/97]\tTime  0.178 ( 0.183)\tLoss -9.0080e-01 (-8.9400e-01)\n",
            "Epoch: [99][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9917e-01 (-8.9399e-01)\n",
            "Epoch: [99][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9174e-01 (-8.9503e-01)\n",
            "Epoch: [99][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9553e-01 (-8.9488e-01)\n",
            "Epoch: [99][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0129e-01 (-8.9408e-01)\n",
            "Training...\n",
            "Epoch: [100][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.6688e-01 (-8.6688e-01)\n",
            "Epoch: [100][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8985e-01 (-8.8648e-01)\n",
            "Epoch: [100][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9115e-01 (-8.8811e-01)\n",
            "Epoch: [100][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9503e-01 (-8.8970e-01)\n",
            "Epoch: [100][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9165e-01 (-8.9131e-01)\n",
            "Epoch: [100][50/97]\tTime  0.178 ( 0.183)\tLoss -9.0276e-01 (-8.9144e-01)\n",
            "Epoch: [100][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0799e-01 (-8.9337e-01)\n",
            "Epoch: [100][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0001e-01 (-8.9335e-01)\n",
            "Epoch: [100][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8966e-01 (-8.9383e-01)\n",
            "Epoch: [100][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8892e-01 (-8.9380e-01)\n",
            "Validating...\n",
            "Top1: 0.6326069078947368\n",
            "Saving the best model!\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [101][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8678e-01 (-8.8678e-01)\n",
            "Epoch: [101][10/97]\tTime  0.179 ( 0.204)\tLoss -8.9061e-01 (-8.8945e-01)\n",
            "Epoch: [101][20/97]\tTime  0.179 ( 0.192)\tLoss -8.9190e-01 (-8.9006e-01)\n",
            "Epoch: [101][30/97]\tTime  0.179 ( 0.187)\tLoss -8.8052e-01 (-8.9052e-01)\n",
            "Epoch: [101][40/97]\tTime  0.178 ( 0.185)\tLoss -8.8808e-01 (-8.9005e-01)\n",
            "Epoch: [101][50/97]\tTime  0.179 ( 0.184)\tLoss -9.0199e-01 (-8.8936e-01)\n",
            "Epoch: [101][60/97]\tTime  0.178 ( 0.183)\tLoss -8.9140e-01 (-8.8979e-01)\n",
            "Epoch: [101][70/97]\tTime  0.179 ( 0.182)\tLoss -8.8002e-01 (-8.8983e-01)\n",
            "Epoch: [101][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7629e-01 (-8.8941e-01)\n",
            "Epoch: [101][90/97]\tTime  0.179 ( 0.181)\tLoss -8.8458e-01 (-8.8820e-01)\n",
            "Training...\n",
            "Epoch: [102][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8487e-01 (-8.8487e-01)\n",
            "Epoch: [102][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8355e-01 (-8.8305e-01)\n",
            "Epoch: [102][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8111e-01 (-8.8570e-01)\n",
            "Epoch: [102][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8918e-01 (-8.8725e-01)\n",
            "Epoch: [102][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8522e-01 (-8.8791e-01)\n",
            "Epoch: [102][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7693e-01 (-8.8709e-01)\n",
            "Epoch: [102][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9261e-01 (-8.8738e-01)\n",
            "Epoch: [102][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9509e-01 (-8.8690e-01)\n",
            "Epoch: [102][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8227e-01 (-8.8668e-01)\n",
            "Epoch: [102][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9168e-01 (-8.8683e-01)\n",
            "Training...\n",
            "Epoch: [103][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8807e-01 (-8.8807e-01)\n",
            "Epoch: [103][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8824e-01 (-8.8888e-01)\n",
            "Epoch: [103][20/97]\tTime  0.178 ( 0.191)\tLoss -8.6980e-01 (-8.8676e-01)\n",
            "Epoch: [103][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8835e-01 (-8.8481e-01)\n",
            "Epoch: [103][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8685e-01 (-8.8466e-01)\n",
            "Epoch: [103][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8015e-01 (-8.8469e-01)\n",
            "Epoch: [103][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9470e-01 (-8.8500e-01)\n",
            "Epoch: [103][70/97]\tTime  0.179 ( 0.182)\tLoss -8.7919e-01 (-8.8513e-01)\n",
            "Epoch: [103][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9418e-01 (-8.8503e-01)\n",
            "Epoch: [103][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8360e-01 (-8.8524e-01)\n",
            "Training...\n",
            "Epoch: [104][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8921e-01 (-8.8921e-01)\n",
            "Epoch: [104][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7930e-01 (-8.8096e-01)\n",
            "Epoch: [104][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8266e-01 (-8.8146e-01)\n",
            "Epoch: [104][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8971e-01 (-8.8112e-01)\n",
            "Epoch: [104][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9197e-01 (-8.8129e-01)\n",
            "Epoch: [104][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7886e-01 (-8.8137e-01)\n",
            "Epoch: [104][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8159e-01 (-8.8146e-01)\n",
            "Epoch: [104][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8491e-01 (-8.8162e-01)\n",
            "Epoch: [104][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7912e-01 (-8.8162e-01)\n",
            "Epoch: [104][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7675e-01 (-8.8128e-01)\n",
            "Training...\n",
            "Epoch: [105][ 0/97]\tTime  0.442 ( 0.442)\tLoss -8.8604e-01 (-8.8604e-01)\n",
            "Epoch: [105][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7677e-01 (-8.7898e-01)\n",
            "Epoch: [105][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8846e-01 (-8.7670e-01)\n",
            "Epoch: [105][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7138e-01 (-8.7521e-01)\n",
            "Epoch: [105][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6907e-01 (-8.7457e-01)\n",
            "Epoch: [105][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8236e-01 (-8.7528e-01)\n",
            "Epoch: [105][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6731e-01 (-8.7459e-01)\n",
            "Epoch: [105][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7512e-01 (-8.7430e-01)\n",
            "Epoch: [105][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6145e-01 (-8.7336e-01)\n",
            "Epoch: [105][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7412e-01 (-8.7372e-01)\n",
            "Validating...\n",
            "Top1: 0.6546052631578947\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [106][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.7802e-01 (-8.7802e-01)\n",
            "Epoch: [106][10/97]\tTime  0.178 ( 0.204)\tLoss -8.7625e-01 (-8.7574e-01)\n",
            "Epoch: [106][20/97]\tTime  0.179 ( 0.192)\tLoss -8.8304e-01 (-8.7464e-01)\n",
            "Epoch: [106][30/97]\tTime  0.180 ( 0.187)\tLoss -8.7405e-01 (-8.7370e-01)\n",
            "Epoch: [106][40/97]\tTime  0.178 ( 0.185)\tLoss -8.7425e-01 (-8.7353e-01)\n",
            "Epoch: [106][50/97]\tTime  0.178 ( 0.184)\tLoss -8.8426e-01 (-8.7342e-01)\n",
            "Epoch: [106][60/97]\tTime  0.179 ( 0.183)\tLoss -8.5299e-01 (-8.7242e-01)\n",
            "Epoch: [106][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8086e-01 (-8.7231e-01)\n",
            "Epoch: [106][80/97]\tTime  0.179 ( 0.182)\tLoss -8.7264e-01 (-8.7246e-01)\n",
            "Epoch: [106][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7830e-01 (-8.7276e-01)\n",
            "Training...\n",
            "Epoch: [107][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.7042e-01 (-8.7042e-01)\n",
            "Epoch: [107][10/97]\tTime  0.182 ( 0.207)\tLoss -8.7972e-01 (-8.7378e-01)\n",
            "Epoch: [107][20/97]\tTime  0.179 ( 0.193)\tLoss -8.6960e-01 (-8.7316e-01)\n",
            "Epoch: [107][30/97]\tTime  0.178 ( 0.188)\tLoss -8.8089e-01 (-8.7442e-01)\n",
            "Epoch: [107][40/97]\tTime  0.177 ( 0.186)\tLoss -8.8444e-01 (-8.7496e-01)\n",
            "Epoch: [107][50/97]\tTime  0.208 ( 0.185)\tLoss -8.7711e-01 (-8.7608e-01)\n",
            "Epoch: [107][60/97]\tTime  0.177 ( 0.185)\tLoss -8.7985e-01 (-8.7690e-01)\n",
            "Epoch: [107][70/97]\tTime  0.193 ( 0.186)\tLoss -8.7668e-01 (-8.7641e-01)\n",
            "Epoch: [107][80/97]\tTime  0.193 ( 0.187)\tLoss -8.6341e-01 (-8.7596e-01)\n",
            "Epoch: [107][90/97]\tTime  0.191 ( 0.187)\tLoss -8.6754e-01 (-8.7574e-01)\n",
            "Training...\n",
            "Epoch: [108][ 0/97]\tTime  0.465 ( 0.465)\tLoss -8.7289e-01 (-8.7289e-01)\n",
            "Epoch: [108][10/97]\tTime  0.194 ( 0.218)\tLoss -8.6310e-01 (-8.7090e-01)\n",
            "Epoch: [108][20/97]\tTime  0.187 ( 0.205)\tLoss -8.8495e-01 (-8.7138e-01)\n",
            "Epoch: [108][30/97]\tTime  0.192 ( 0.200)\tLoss -8.7629e-01 (-8.7264e-01)\n",
            "Epoch: [108][40/97]\tTime  0.183 ( 0.197)\tLoss -8.8370e-01 (-8.7395e-01)\n",
            "Epoch: [108][50/97]\tTime  0.183 ( 0.194)\tLoss -8.8105e-01 (-8.7584e-01)\n",
            "Epoch: [108][60/97]\tTime  0.185 ( 0.192)\tLoss -8.7503e-01 (-8.7633e-01)\n",
            "Epoch: [108][70/97]\tTime  0.184 ( 0.191)\tLoss -8.8874e-01 (-8.7686e-01)\n",
            "Epoch: [108][80/97]\tTime  0.187 ( 0.190)\tLoss -8.7084e-01 (-8.7706e-01)\n",
            "Epoch: [108][90/97]\tTime  0.204 ( 0.190)\tLoss -8.7274e-01 (-8.7702e-01)\n",
            "Training...\n",
            "Epoch: [109][ 0/97]\tTime  0.464 ( 0.464)\tLoss -8.9780e-01 (-8.9780e-01)\n",
            "Epoch: [109][10/97]\tTime  0.192 ( 0.216)\tLoss -8.7737e-01 (-8.7808e-01)\n",
            "Epoch: [109][20/97]\tTime  0.191 ( 0.204)\tLoss -8.7446e-01 (-8.7581e-01)\n",
            "Epoch: [109][30/97]\tTime  0.189 ( 0.200)\tLoss -8.6664e-01 (-8.7601e-01)\n",
            "Epoch: [109][40/97]\tTime  0.193 ( 0.198)\tLoss -8.7376e-01 (-8.7674e-01)\n",
            "Epoch: [109][50/97]\tTime  0.190 ( 0.197)\tLoss -8.6822e-01 (-8.7675e-01)\n",
            "Epoch: [109][60/97]\tTime  0.193 ( 0.196)\tLoss -8.7221e-01 (-8.7672e-01)\n",
            "Epoch: [109][70/97]\tTime  0.185 ( 0.195)\tLoss -8.8240e-01 (-8.7656e-01)\n",
            "Epoch: [109][80/97]\tTime  0.178 ( 0.193)\tLoss -8.9017e-01 (-8.7681e-01)\n",
            "Epoch: [109][90/97]\tTime  0.178 ( 0.192)\tLoss -8.9217e-01 (-8.7743e-01)\n",
            "Training...\n",
            "Epoch: [110][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8245e-01 (-8.8245e-01)\n",
            "Epoch: [110][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9990e-01 (-8.8202e-01)\n",
            "Epoch: [110][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8693e-01 (-8.8312e-01)\n",
            "Epoch: [110][30/97]\tTime  0.220 ( 0.188)\tLoss -8.8384e-01 (-8.8290e-01)\n",
            "Epoch: [110][40/97]\tTime  0.178 ( 0.185)\tLoss -8.7885e-01 (-8.8153e-01)\n",
            "Epoch: [110][50/97]\tTime  0.178 ( 0.184)\tLoss -8.6818e-01 (-8.8098e-01)\n",
            "Epoch: [110][60/97]\tTime  0.177 ( 0.183)\tLoss -8.7048e-01 (-8.8012e-01)\n",
            "Epoch: [110][70/97]\tTime  0.177 ( 0.182)\tLoss -8.7344e-01 (-8.7905e-01)\n",
            "Epoch: [110][80/97]\tTime  0.178 ( 0.182)\tLoss -8.8753e-01 (-8.7909e-01)\n",
            "Epoch: [110][90/97]\tTime  0.178 ( 0.181)\tLoss -8.9166e-01 (-8.7881e-01)\n",
            "Validating...\n",
            "Top1: 0.6614925986842105\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [111][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.7360e-01 (-8.7360e-01)\n",
            "Epoch: [111][10/97]\tTime  0.178 ( 0.204)\tLoss -8.7743e-01 (-8.8228e-01)\n",
            "Epoch: [111][20/97]\tTime  0.178 ( 0.192)\tLoss -8.7928e-01 (-8.7857e-01)\n",
            "Epoch: [111][30/97]\tTime  0.178 ( 0.187)\tLoss -8.8229e-01 (-8.7861e-01)\n",
            "Epoch: [111][40/97]\tTime  0.179 ( 0.185)\tLoss -8.7944e-01 (-8.7801e-01)\n",
            "Epoch: [111][50/97]\tTime  0.178 ( 0.184)\tLoss -8.7790e-01 (-8.7777e-01)\n",
            "Epoch: [111][60/97]\tTime  0.178 ( 0.183)\tLoss -8.8430e-01 (-8.7869e-01)\n",
            "Epoch: [111][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8784e-01 (-8.7883e-01)\n",
            "Epoch: [111][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7734e-01 (-8.7858e-01)\n",
            "Epoch: [111][90/97]\tTime  0.179 ( 0.181)\tLoss -8.8467e-01 (-8.7827e-01)\n",
            "Training...\n",
            "Epoch: [112][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7149e-01 (-8.7149e-01)\n",
            "Epoch: [112][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7678e-01 (-8.7312e-01)\n",
            "Epoch: [112][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7581e-01 (-8.7424e-01)\n",
            "Epoch: [112][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9022e-01 (-8.7572e-01)\n",
            "Epoch: [112][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8166e-01 (-8.7625e-01)\n",
            "Epoch: [112][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7766e-01 (-8.7731e-01)\n",
            "Epoch: [112][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8180e-01 (-8.7724e-01)\n",
            "Epoch: [112][70/97]\tTime  0.178 ( 0.181)\tLoss -8.6723e-01 (-8.7685e-01)\n",
            "Epoch: [112][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7402e-01 (-8.7602e-01)\n",
            "Epoch: [112][90/97]\tTime  0.177 ( 0.180)\tLoss -8.5620e-01 (-8.7487e-01)\n",
            "Training...\n",
            "Epoch: [113][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.6921e-01 (-8.6921e-01)\n",
            "Epoch: [113][10/97]\tTime  0.177 ( 0.201)\tLoss -8.7104e-01 (-8.7391e-01)\n",
            "Epoch: [113][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7050e-01 (-8.7426e-01)\n",
            "Epoch: [113][30/97]\tTime  0.178 ( 0.186)\tLoss -8.6968e-01 (-8.7389e-01)\n",
            "Epoch: [113][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8388e-01 (-8.7384e-01)\n",
            "Epoch: [113][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7549e-01 (-8.7223e-01)\n",
            "Epoch: [113][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6613e-01 (-8.7142e-01)\n",
            "Epoch: [113][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8118e-01 (-8.7134e-01)\n",
            "Epoch: [113][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6627e-01 (-8.7122e-01)\n",
            "Epoch: [113][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8143e-01 (-8.7149e-01)\n",
            "Training...\n",
            "Epoch: [114][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.6510e-01 (-8.6510e-01)\n",
            "Epoch: [114][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7099e-01 (-8.6818e-01)\n",
            "Epoch: [114][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9279e-01 (-8.7175e-01)\n",
            "Epoch: [114][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8415e-01 (-8.7234e-01)\n",
            "Epoch: [114][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7648e-01 (-8.7199e-01)\n",
            "Epoch: [114][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8347e-01 (-8.7327e-01)\n",
            "Epoch: [114][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7529e-01 (-8.7431e-01)\n",
            "Epoch: [114][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7405e-01 (-8.7441e-01)\n",
            "Epoch: [114][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8851e-01 (-8.7443e-01)\n",
            "Epoch: [114][90/97]\tTime  0.177 ( 0.181)\tLoss -8.6683e-01 (-8.7454e-01)\n",
            "Training...\n",
            "Epoch: [115][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7259e-01 (-8.7259e-01)\n",
            "Epoch: [115][10/97]\tTime  0.177 ( 0.202)\tLoss -8.6419e-01 (-8.7277e-01)\n",
            "Epoch: [115][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7953e-01 (-8.7309e-01)\n",
            "Epoch: [115][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7512e-01 (-8.7345e-01)\n",
            "Epoch: [115][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7302e-01 (-8.7379e-01)\n",
            "Epoch: [115][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6991e-01 (-8.7377e-01)\n",
            "Epoch: [115][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7601e-01 (-8.7343e-01)\n",
            "Epoch: [115][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7399e-01 (-8.7299e-01)\n",
            "Epoch: [115][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7707e-01 (-8.7347e-01)\n",
            "Epoch: [115][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7224e-01 (-8.7429e-01)\n",
            "Validating...\n",
            "Top1: 0.6723889802631579\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [116][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.6869e-01 (-8.6869e-01)\n",
            "Epoch: [116][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7096e-01 (-8.7866e-01)\n",
            "Epoch: [116][20/97]\tTime  0.179 ( 0.191)\tLoss -8.7759e-01 (-8.8003e-01)\n",
            "Epoch: [116][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7543e-01 (-8.7920e-01)\n",
            "Epoch: [116][40/97]\tTime  0.178 ( 0.185)\tLoss -8.7141e-01 (-8.7769e-01)\n",
            "Epoch: [116][50/97]\tTime  0.179 ( 0.184)\tLoss -8.7732e-01 (-8.7767e-01)\n",
            "Epoch: [116][60/97]\tTime  0.178 ( 0.183)\tLoss -8.7260e-01 (-8.7711e-01)\n",
            "Epoch: [116][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8107e-01 (-8.7653e-01)\n",
            "Epoch: [116][80/97]\tTime  0.179 ( 0.182)\tLoss -8.5854e-01 (-8.7562e-01)\n",
            "Epoch: [116][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7236e-01 (-8.7551e-01)\n",
            "Training...\n",
            "Epoch: [117][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7877e-01 (-8.7877e-01)\n",
            "Epoch: [117][10/97]\tTime  0.177 ( 0.202)\tLoss -8.6379e-01 (-8.7562e-01)\n",
            "Epoch: [117][20/97]\tTime  0.177 ( 0.190)\tLoss -8.6554e-01 (-8.7499e-01)\n",
            "Epoch: [117][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7640e-01 (-8.7499e-01)\n",
            "Epoch: [117][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7368e-01 (-8.7366e-01)\n",
            "Epoch: [117][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8215e-01 (-8.7290e-01)\n",
            "Epoch: [117][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7535e-01 (-8.7337e-01)\n",
            "Epoch: [117][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8288e-01 (-8.7362e-01)\n",
            "Epoch: [117][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6250e-01 (-8.7325e-01)\n",
            "Epoch: [117][90/97]\tTime  0.178 ( 0.180)\tLoss -8.6879e-01 (-8.7258e-01)\n",
            "Training...\n",
            "Epoch: [118][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.5931e-01 (-8.5931e-01)\n",
            "Epoch: [118][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8236e-01 (-8.7445e-01)\n",
            "Epoch: [118][20/97]\tTime  0.177 ( 0.191)\tLoss -8.5877e-01 (-8.7317e-01)\n",
            "Epoch: [118][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6799e-01 (-8.7293e-01)\n",
            "Epoch: [118][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7512e-01 (-8.7165e-01)\n",
            "Epoch: [118][50/97]\tTime  0.178 ( 0.183)\tLoss -8.5919e-01 (-8.7087e-01)\n",
            "Epoch: [118][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6593e-01 (-8.7044e-01)\n",
            "Epoch: [118][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6654e-01 (-8.6990e-01)\n",
            "Epoch: [118][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7012e-01 (-8.6953e-01)\n",
            "Epoch: [118][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6999e-01 (-8.6925e-01)\n",
            "Training...\n",
            "Epoch: [119][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.9410e-01 (-8.9410e-01)\n",
            "Epoch: [119][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8699e-01 (-8.8345e-01)\n",
            "Epoch: [119][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7756e-01 (-8.8068e-01)\n",
            "Epoch: [119][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8323e-01 (-8.8146e-01)\n",
            "Epoch: [119][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8335e-01 (-8.8260e-01)\n",
            "Epoch: [119][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8470e-01 (-8.8289e-01)\n",
            "Epoch: [119][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9099e-01 (-8.8343e-01)\n",
            "Epoch: [119][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8396e-01 (-8.8371e-01)\n",
            "Epoch: [119][80/97]\tTime  0.178 ( 0.181)\tLoss -9.0460e-01 (-8.8474e-01)\n",
            "Epoch: [119][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9692e-01 (-8.8541e-01)\n",
            "Training...\n",
            "Epoch: [120][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7899e-01 (-8.7899e-01)\n",
            "Epoch: [120][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9002e-01 (-8.8917e-01)\n",
            "Epoch: [120][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7530e-01 (-8.8733e-01)\n",
            "Epoch: [120][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9080e-01 (-8.8632e-01)\n",
            "Epoch: [120][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9243e-01 (-8.8415e-01)\n",
            "Epoch: [120][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8905e-01 (-8.8410e-01)\n",
            "Epoch: [120][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8516e-01 (-8.8331e-01)\n",
            "Epoch: [120][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8471e-01 (-8.8260e-01)\n",
            "Epoch: [120][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8537e-01 (-8.8270e-01)\n",
            "Epoch: [120][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8138e-01 (-8.8276e-01)\n",
            "Validating...\n",
            "Top1: 0.690686677631579\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [121][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.8758e-01 (-8.8758e-01)\n",
            "Epoch: [121][10/97]\tTime  0.179 ( 0.203)\tLoss -8.9004e-01 (-8.8272e-01)\n",
            "Epoch: [121][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7848e-01 (-8.8060e-01)\n",
            "Epoch: [121][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7955e-01 (-8.7971e-01)\n",
            "Epoch: [121][40/97]\tTime  0.178 ( 0.185)\tLoss -8.7503e-01 (-8.7927e-01)\n",
            "Epoch: [121][50/97]\tTime  0.179 ( 0.184)\tLoss -8.8005e-01 (-8.7815e-01)\n",
            "Epoch: [121][60/97]\tTime  0.179 ( 0.183)\tLoss -8.7004e-01 (-8.7787e-01)\n",
            "Epoch: [121][70/97]\tTime  0.179 ( 0.182)\tLoss -8.7741e-01 (-8.7820e-01)\n",
            "Epoch: [121][80/97]\tTime  0.178 ( 0.182)\tLoss -8.9463e-01 (-8.7844e-01)\n",
            "Epoch: [121][90/97]\tTime  0.179 ( 0.181)\tLoss -8.7663e-01 (-8.7804e-01)\n",
            "Training...\n",
            "Epoch: [122][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.7989e-01 (-8.7989e-01)\n",
            "Epoch: [122][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8338e-01 (-8.8060e-01)\n",
            "Epoch: [122][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7935e-01 (-8.8062e-01)\n",
            "Epoch: [122][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9048e-01 (-8.8081e-01)\n",
            "Epoch: [122][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9143e-01 (-8.8085e-01)\n",
            "Epoch: [122][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9319e-01 (-8.8170e-01)\n",
            "Epoch: [122][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7939e-01 (-8.8161e-01)\n",
            "Epoch: [122][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8112e-01 (-8.8143e-01)\n",
            "Epoch: [122][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8720e-01 (-8.8236e-01)\n",
            "Epoch: [122][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6673e-01 (-8.8186e-01)\n",
            "Training...\n",
            "Epoch: [123][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.7697e-01 (-8.7697e-01)\n",
            "Epoch: [123][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8549e-01 (-8.7926e-01)\n",
            "Epoch: [123][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9038e-01 (-8.8259e-01)\n",
            "Epoch: [123][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7943e-01 (-8.8242e-01)\n",
            "Epoch: [123][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8317e-01 (-8.8252e-01)\n",
            "Epoch: [123][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8782e-01 (-8.8240e-01)\n",
            "Epoch: [123][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8298e-01 (-8.8210e-01)\n",
            "Epoch: [123][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7336e-01 (-8.8148e-01)\n",
            "Epoch: [123][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6540e-01 (-8.8033e-01)\n",
            "Epoch: [123][90/97]\tTime  0.178 ( 0.181)\tLoss -8.6559e-01 (-8.7961e-01)\n",
            "Training...\n",
            "Epoch: [124][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.6586e-01 (-8.6586e-01)\n",
            "Epoch: [124][10/97]\tTime  0.177 ( 0.203)\tLoss -8.6977e-01 (-8.7445e-01)\n",
            "Epoch: [124][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7306e-01 (-8.7473e-01)\n",
            "Epoch: [124][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7402e-01 (-8.7565e-01)\n",
            "Epoch: [124][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8289e-01 (-8.7401e-01)\n",
            "Epoch: [124][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7124e-01 (-8.7341e-01)\n",
            "Epoch: [124][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7535e-01 (-8.7388e-01)\n",
            "Epoch: [124][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8670e-01 (-8.7483e-01)\n",
            "Epoch: [124][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8241e-01 (-8.7549e-01)\n",
            "Epoch: [124][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7839e-01 (-8.7603e-01)\n",
            "Training...\n",
            "Epoch: [125][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.8146e-01 (-8.8146e-01)\n",
            "Epoch: [125][10/97]\tTime  0.177 ( 0.203)\tLoss -8.7762e-01 (-8.7374e-01)\n",
            "Epoch: [125][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8309e-01 (-8.7451e-01)\n",
            "Epoch: [125][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8528e-01 (-8.7598e-01)\n",
            "Epoch: [125][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8269e-01 (-8.7572e-01)\n",
            "Epoch: [125][50/97]\tTime  0.177 ( 0.183)\tLoss -8.5737e-01 (-8.7401e-01)\n",
            "Epoch: [125][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7451e-01 (-8.7334e-01)\n",
            "Epoch: [125][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8362e-01 (-8.7411e-01)\n",
            "Epoch: [125][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8413e-01 (-8.7485e-01)\n",
            "Epoch: [125][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7639e-01 (-8.7519e-01)\n",
            "Validating...\n",
            "Top1: 0.684313322368421\n",
            "Training...\n",
            "Epoch: [126][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.6227e-01 (-8.6227e-01)\n",
            "Epoch: [126][10/97]\tTime  0.177 ( 0.202)\tLoss -8.5488e-01 (-8.7052e-01)\n",
            "Epoch: [126][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9033e-01 (-8.7057e-01)\n",
            "Epoch: [126][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7726e-01 (-8.7335e-01)\n",
            "Epoch: [126][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8991e-01 (-8.7695e-01)\n",
            "Epoch: [126][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0376e-01 (-8.7981e-01)\n",
            "Epoch: [126][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9766e-01 (-8.8169e-01)\n",
            "Epoch: [126][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9417e-01 (-8.8253e-01)\n",
            "Epoch: [126][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6681e-01 (-8.8253e-01)\n",
            "Epoch: [126][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6641e-01 (-8.8265e-01)\n",
            "Training...\n",
            "Epoch: [127][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8348e-01 (-8.8348e-01)\n",
            "Epoch: [127][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8887e-01 (-8.7941e-01)\n",
            "Epoch: [127][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7271e-01 (-8.7859e-01)\n",
            "Epoch: [127][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8028e-01 (-8.7834e-01)\n",
            "Epoch: [127][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8129e-01 (-8.7648e-01)\n",
            "Epoch: [127][50/97]\tTime  0.178 ( 0.183)\tLoss -8.6891e-01 (-8.7531e-01)\n",
            "Epoch: [127][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7447e-01 (-8.7491e-01)\n",
            "Epoch: [127][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7887e-01 (-8.7459e-01)\n",
            "Epoch: [127][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8419e-01 (-8.7417e-01)\n",
            "Epoch: [127][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8015e-01 (-8.7436e-01)\n",
            "Training...\n",
            "Epoch: [128][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.8820e-01 (-8.8820e-01)\n",
            "Epoch: [128][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7829e-01 (-8.7578e-01)\n",
            "Epoch: [128][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7969e-01 (-8.7402e-01)\n",
            "Epoch: [128][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7203e-01 (-8.7513e-01)\n",
            "Epoch: [128][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7625e-01 (-8.7384e-01)\n",
            "Epoch: [128][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7219e-01 (-8.7200e-01)\n",
            "Epoch: [128][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7298e-01 (-8.7164e-01)\n",
            "Epoch: [128][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8107e-01 (-8.7206e-01)\n",
            "Epoch: [128][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7292e-01 (-8.7262e-01)\n",
            "Epoch: [128][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7879e-01 (-8.7311e-01)\n",
            "Training...\n",
            "Epoch: [129][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.8797e-01 (-8.8797e-01)\n",
            "Epoch: [129][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8839e-01 (-8.7923e-01)\n",
            "Epoch: [129][20/97]\tTime  0.177 ( 0.190)\tLoss -8.6937e-01 (-8.7708e-01)\n",
            "Epoch: [129][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6986e-01 (-8.7635e-01)\n",
            "Epoch: [129][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8113e-01 (-8.7531e-01)\n",
            "Epoch: [129][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6752e-01 (-8.7483e-01)\n",
            "Epoch: [129][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6425e-01 (-8.7385e-01)\n",
            "Epoch: [129][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6954e-01 (-8.7343e-01)\n",
            "Epoch: [129][80/97]\tTime  0.178 ( 0.181)\tLoss -8.5710e-01 (-8.7262e-01)\n",
            "Epoch: [129][90/97]\tTime  0.179 ( 0.181)\tLoss -8.7966e-01 (-8.7294e-01)\n",
            "Training...\n",
            "Epoch: [130][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.7711e-01 (-8.7711e-01)\n",
            "Epoch: [130][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9042e-01 (-8.8620e-01)\n",
            "Epoch: [130][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8721e-01 (-8.8544e-01)\n",
            "Epoch: [130][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8415e-01 (-8.8358e-01)\n",
            "Epoch: [130][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7260e-01 (-8.8105e-01)\n",
            "Epoch: [130][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7874e-01 (-8.8024e-01)\n",
            "Epoch: [130][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7124e-01 (-8.7895e-01)\n",
            "Epoch: [130][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8826e-01 (-8.7925e-01)\n",
            "Epoch: [130][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8167e-01 (-8.7933e-01)\n",
            "Epoch: [130][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7685e-01 (-8.7910e-01)\n",
            "Validating...\n",
            "Top1: 0.7025082236842105\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [131][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.8986e-01 (-8.8986e-01)\n",
            "Epoch: [131][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9232e-01 (-8.8443e-01)\n",
            "Epoch: [131][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7024e-01 (-8.8406e-01)\n",
            "Epoch: [131][30/97]\tTime  0.179 ( 0.187)\tLoss -8.7296e-01 (-8.8188e-01)\n",
            "Epoch: [131][40/97]\tTime  0.178 ( 0.185)\tLoss -8.7161e-01 (-8.8054e-01)\n",
            "Epoch: [131][50/97]\tTime  0.178 ( 0.184)\tLoss -8.8023e-01 (-8.7973e-01)\n",
            "Epoch: [131][60/97]\tTime  0.178 ( 0.183)\tLoss -8.8149e-01 (-8.7924e-01)\n",
            "Epoch: [131][70/97]\tTime  0.179 ( 0.182)\tLoss -8.7255e-01 (-8.7887e-01)\n",
            "Epoch: [131][80/97]\tTime  0.178 ( 0.182)\tLoss -8.8236e-01 (-8.7773e-01)\n",
            "Epoch: [131][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7886e-01 (-8.7817e-01)\n",
            "Training...\n",
            "Epoch: [132][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9436e-01 (-8.9436e-01)\n",
            "Epoch: [132][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7124e-01 (-8.7818e-01)\n",
            "Epoch: [132][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7656e-01 (-8.7495e-01)\n",
            "Epoch: [132][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6815e-01 (-8.7575e-01)\n",
            "Epoch: [132][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7114e-01 (-8.7516e-01)\n",
            "Epoch: [132][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8003e-01 (-8.7543e-01)\n",
            "Epoch: [132][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8104e-01 (-8.7473e-01)\n",
            "Epoch: [132][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6806e-01 (-8.7386e-01)\n",
            "Epoch: [132][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7095e-01 (-8.7363e-01)\n",
            "Epoch: [132][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7692e-01 (-8.7301e-01)\n",
            "Training...\n",
            "Epoch: [133][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8388e-01 (-8.8388e-01)\n",
            "Epoch: [133][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7257e-01 (-8.7370e-01)\n",
            "Epoch: [133][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8104e-01 (-8.7536e-01)\n",
            "Epoch: [133][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7848e-01 (-8.7661e-01)\n",
            "Epoch: [133][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8913e-01 (-8.7744e-01)\n",
            "Epoch: [133][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8137e-01 (-8.7781e-01)\n",
            "Epoch: [133][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6997e-01 (-8.7765e-01)\n",
            "Epoch: [133][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7935e-01 (-8.7806e-01)\n",
            "Epoch: [133][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7528e-01 (-8.7766e-01)\n",
            "Epoch: [133][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7490e-01 (-8.7654e-01)\n",
            "Training...\n",
            "Epoch: [134][ 0/97]\tTime  0.442 ( 0.442)\tLoss -8.7486e-01 (-8.7486e-01)\n",
            "Epoch: [134][10/97]\tTime  0.177 ( 0.201)\tLoss -8.7103e-01 (-8.7263e-01)\n",
            "Epoch: [134][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8268e-01 (-8.7516e-01)\n",
            "Epoch: [134][30/97]\tTime  0.178 ( 0.186)\tLoss -8.6007e-01 (-8.7522e-01)\n",
            "Epoch: [134][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7952e-01 (-8.7349e-01)\n",
            "Epoch: [134][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7302e-01 (-8.7275e-01)\n",
            "Epoch: [134][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6569e-01 (-8.7312e-01)\n",
            "Epoch: [134][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8637e-01 (-8.7419e-01)\n",
            "Epoch: [134][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8259e-01 (-8.7437e-01)\n",
            "Epoch: [134][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8135e-01 (-8.7464e-01)\n",
            "Training...\n",
            "Epoch: [135][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7511e-01 (-8.7511e-01)\n",
            "Epoch: [135][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7498e-01 (-8.7246e-01)\n",
            "Epoch: [135][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7443e-01 (-8.7291e-01)\n",
            "Epoch: [135][30/97]\tTime  0.179 ( 0.186)\tLoss -8.8017e-01 (-8.7362e-01)\n",
            "Epoch: [135][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7106e-01 (-8.7281e-01)\n",
            "Epoch: [135][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6569e-01 (-8.7252e-01)\n",
            "Epoch: [135][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6654e-01 (-8.7230e-01)\n",
            "Epoch: [135][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7998e-01 (-8.7213e-01)\n",
            "Epoch: [135][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7212e-01 (-8.7183e-01)\n",
            "Epoch: [135][90/97]\tTime  0.177 ( 0.181)\tLoss -8.5742e-01 (-8.7205e-01)\n",
            "Validating...\n",
            "Top1: 0.7106291118421053\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [136][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.7779e-01 (-8.7779e-01)\n",
            "Epoch: [136][10/97]\tTime  0.179 ( 0.203)\tLoss -8.6296e-01 (-8.6978e-01)\n",
            "Epoch: [136][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7387e-01 (-8.7231e-01)\n",
            "Epoch: [136][30/97]\tTime  0.178 ( 0.187)\tLoss -8.8879e-01 (-8.7234e-01)\n",
            "Epoch: [136][40/97]\tTime  0.178 ( 0.185)\tLoss -8.6566e-01 (-8.7213e-01)\n",
            "Epoch: [136][50/97]\tTime  0.178 ( 0.184)\tLoss -8.6672e-01 (-8.7235e-01)\n",
            "Epoch: [136][60/97]\tTime  0.178 ( 0.183)\tLoss -8.8329e-01 (-8.7272e-01)\n",
            "Epoch: [136][70/97]\tTime  0.178 ( 0.182)\tLoss -8.7766e-01 (-8.7226e-01)\n",
            "Epoch: [136][80/97]\tTime  0.179 ( 0.182)\tLoss -8.6296e-01 (-8.7233e-01)\n",
            "Epoch: [136][90/97]\tTime  0.178 ( 0.182)\tLoss -8.7038e-01 (-8.7217e-01)\n",
            "Training...\n",
            "Epoch: [137][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8383e-01 (-8.8383e-01)\n",
            "Epoch: [137][10/97]\tTime  0.177 ( 0.202)\tLoss -8.6935e-01 (-8.7878e-01)\n",
            "Epoch: [137][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7832e-01 (-8.7542e-01)\n",
            "Epoch: [137][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7509e-01 (-8.7400e-01)\n",
            "Epoch: [137][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7218e-01 (-8.7405e-01)\n",
            "Epoch: [137][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7653e-01 (-8.7345e-01)\n",
            "Epoch: [137][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6800e-01 (-8.7245e-01)\n",
            "Epoch: [137][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6816e-01 (-8.7217e-01)\n",
            "Epoch: [137][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6065e-01 (-8.7212e-01)\n",
            "Epoch: [137][90/97]\tTime  0.178 ( 0.180)\tLoss -8.6536e-01 (-8.7185e-01)\n",
            "Training...\n",
            "Epoch: [138][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.7168e-01 (-8.7168e-01)\n",
            "Epoch: [138][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8894e-01 (-8.7836e-01)\n",
            "Epoch: [138][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7515e-01 (-8.7682e-01)\n",
            "Epoch: [138][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7853e-01 (-8.7799e-01)\n",
            "Epoch: [138][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8511e-01 (-8.7859e-01)\n",
            "Epoch: [138][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8747e-01 (-8.7972e-01)\n",
            "Epoch: [138][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8544e-01 (-8.8007e-01)\n",
            "Epoch: [138][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7933e-01 (-8.7987e-01)\n",
            "Epoch: [138][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8622e-01 (-8.7973e-01)\n",
            "Epoch: [138][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8571e-01 (-8.7974e-01)\n",
            "Training...\n",
            "Epoch: [139][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.8949e-01 (-8.8949e-01)\n",
            "Epoch: [139][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7197e-01 (-8.7960e-01)\n",
            "Epoch: [139][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7550e-01 (-8.7733e-01)\n",
            "Epoch: [139][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8921e-01 (-8.7800e-01)\n",
            "Epoch: [139][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8061e-01 (-8.7786e-01)\n",
            "Epoch: [139][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6517e-01 (-8.7636e-01)\n",
            "Epoch: [139][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7514e-01 (-8.7599e-01)\n",
            "Epoch: [139][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7803e-01 (-8.7671e-01)\n",
            "Epoch: [139][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7195e-01 (-8.7580e-01)\n",
            "Epoch: [139][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6365e-01 (-8.7494e-01)\n",
            "Training...\n",
            "Epoch: [140][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.6914e-01 (-8.6914e-01)\n",
            "Epoch: [140][10/97]\tTime  0.177 ( 0.203)\tLoss -8.7811e-01 (-8.7061e-01)\n",
            "Epoch: [140][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7266e-01 (-8.6814e-01)\n",
            "Epoch: [140][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8389e-01 (-8.7052e-01)\n",
            "Epoch: [140][40/97]\tTime  0.178 ( 0.184)\tLoss -8.5792e-01 (-8.7112e-01)\n",
            "Epoch: [140][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7715e-01 (-8.7179e-01)\n",
            "Epoch: [140][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6597e-01 (-8.7188e-01)\n",
            "Epoch: [140][70/97]\tTime  0.177 ( 0.182)\tLoss -8.6105e-01 (-8.7211e-01)\n",
            "Epoch: [140][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6803e-01 (-8.7218e-01)\n",
            "Epoch: [140][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7139e-01 (-8.7209e-01)\n",
            "Validating...\n",
            "Top1: 0.7150493421052632\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [141][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.7566e-01 (-8.7566e-01)\n",
            "Epoch: [141][10/97]\tTime  0.178 ( 0.204)\tLoss -8.6393e-01 (-8.6705e-01)\n",
            "Epoch: [141][20/97]\tTime  0.178 ( 0.192)\tLoss -8.8434e-01 (-8.7261e-01)\n",
            "Epoch: [141][30/97]\tTime  0.179 ( 0.187)\tLoss -8.8844e-01 (-8.7468e-01)\n",
            "Epoch: [141][40/97]\tTime  0.178 ( 0.185)\tLoss -8.8331e-01 (-8.7466e-01)\n",
            "Epoch: [141][50/97]\tTime  0.179 ( 0.184)\tLoss -8.8045e-01 (-8.7530e-01)\n",
            "Epoch: [141][60/97]\tTime  0.179 ( 0.183)\tLoss -8.6735e-01 (-8.7537e-01)\n",
            "Epoch: [141][70/97]\tTime  0.178 ( 0.182)\tLoss -8.6914e-01 (-8.7510e-01)\n",
            "Epoch: [141][80/97]\tTime  0.179 ( 0.182)\tLoss -8.6181e-01 (-8.7451e-01)\n",
            "Epoch: [141][90/97]\tTime  0.179 ( 0.182)\tLoss -8.6783e-01 (-8.7377e-01)\n",
            "Training...\n",
            "Epoch: [142][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.7423e-01 (-8.7423e-01)\n",
            "Epoch: [142][10/97]\tTime  0.177 ( 0.202)\tLoss -8.5627e-01 (-8.6541e-01)\n",
            "Epoch: [142][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7696e-01 (-8.6902e-01)\n",
            "Epoch: [142][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8456e-01 (-8.7192e-01)\n",
            "Epoch: [142][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8410e-01 (-8.7254e-01)\n",
            "Epoch: [142][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7725e-01 (-8.7283e-01)\n",
            "Epoch: [142][60/97]\tTime  0.177 ( 0.182)\tLoss -8.5900e-01 (-8.7207e-01)\n",
            "Epoch: [142][70/97]\tTime  0.178 ( 0.181)\tLoss -8.5978e-01 (-8.7216e-01)\n",
            "Epoch: [142][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6816e-01 (-8.7177e-01)\n",
            "Epoch: [142][90/97]\tTime  0.178 ( 0.180)\tLoss -8.6541e-01 (-8.7125e-01)\n",
            "Training...\n",
            "Epoch: [143][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9141e-01 (-8.9141e-01)\n",
            "Epoch: [143][10/97]\tTime  0.178 ( 0.203)\tLoss -8.6140e-01 (-8.6877e-01)\n",
            "Epoch: [143][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8190e-01 (-8.6929e-01)\n",
            "Epoch: [143][30/97]\tTime  0.177 ( 0.187)\tLoss -8.9044e-01 (-8.7398e-01)\n",
            "Epoch: [143][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8716e-01 (-8.7697e-01)\n",
            "Epoch: [143][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8402e-01 (-8.7862e-01)\n",
            "Epoch: [143][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8108e-01 (-8.7949e-01)\n",
            "Epoch: [143][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8144e-01 (-8.8001e-01)\n",
            "Epoch: [143][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7316e-01 (-8.7952e-01)\n",
            "Epoch: [143][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7185e-01 (-8.7932e-01)\n",
            "Training...\n",
            "Epoch: [144][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9009e-01 (-8.9009e-01)\n",
            "Epoch: [144][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7161e-01 (-8.7653e-01)\n",
            "Epoch: [144][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8205e-01 (-8.7743e-01)\n",
            "Epoch: [144][30/97]\tTime  0.179 ( 0.186)\tLoss -8.8471e-01 (-8.7804e-01)\n",
            "Epoch: [144][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8463e-01 (-8.7741e-01)\n",
            "Epoch: [144][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8498e-01 (-8.7657e-01)\n",
            "Epoch: [144][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7586e-01 (-8.7627e-01)\n",
            "Epoch: [144][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8706e-01 (-8.7661e-01)\n",
            "Epoch: [144][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9008e-01 (-8.7701e-01)\n",
            "Epoch: [144][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7607e-01 (-8.7735e-01)\n",
            "Training...\n",
            "Epoch: [145][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.8882e-01 (-8.8882e-01)\n",
            "Epoch: [145][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8248e-01 (-8.7965e-01)\n",
            "Epoch: [145][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8283e-01 (-8.7964e-01)\n",
            "Epoch: [145][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7448e-01 (-8.7964e-01)\n",
            "Epoch: [145][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8577e-01 (-8.7874e-01)\n",
            "Epoch: [145][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7964e-01 (-8.7730e-01)\n",
            "Epoch: [145][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6659e-01 (-8.7632e-01)\n",
            "Epoch: [145][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8403e-01 (-8.7620e-01)\n",
            "Epoch: [145][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6060e-01 (-8.7497e-01)\n",
            "Epoch: [145][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8239e-01 (-8.7451e-01)\n",
            "Validating...\n",
            "Top1: 0.7150493421052632\n",
            "Training...\n",
            "Epoch: [146][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.7451e-01 (-8.7451e-01)\n",
            "Epoch: [146][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7506e-01 (-8.7224e-01)\n",
            "Epoch: [146][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7801e-01 (-8.7247e-01)\n",
            "Epoch: [146][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6570e-01 (-8.7271e-01)\n",
            "Epoch: [146][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6388e-01 (-8.7230e-01)\n",
            "Epoch: [146][50/97]\tTime  0.179 ( 0.183)\tLoss -8.6790e-01 (-8.7132e-01)\n",
            "Epoch: [146][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6878e-01 (-8.7058e-01)\n",
            "Epoch: [146][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8178e-01 (-8.7056e-01)\n",
            "Epoch: [146][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8964e-01 (-8.7142e-01)\n",
            "Epoch: [146][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8238e-01 (-8.7231e-01)\n",
            "Training...\n",
            "Epoch: [147][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7865e-01 (-8.7865e-01)\n",
            "Epoch: [147][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7259e-01 (-8.7926e-01)\n",
            "Epoch: [147][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8262e-01 (-8.7541e-01)\n",
            "Epoch: [147][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7264e-01 (-8.7378e-01)\n",
            "Epoch: [147][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6428e-01 (-8.7266e-01)\n",
            "Epoch: [147][50/97]\tTime  0.178 ( 0.183)\tLoss -8.5661e-01 (-8.7131e-01)\n",
            "Epoch: [147][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7848e-01 (-8.7111e-01)\n",
            "Epoch: [147][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7972e-01 (-8.7094e-01)\n",
            "Epoch: [147][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6789e-01 (-8.7019e-01)\n",
            "Epoch: [147][90/97]\tTime  0.178 ( 0.181)\tLoss -8.5843e-01 (-8.6969e-01)\n",
            "Training...\n",
            "Epoch: [148][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8122e-01 (-8.8122e-01)\n",
            "Epoch: [148][10/97]\tTime  0.179 ( 0.203)\tLoss -8.7717e-01 (-8.7481e-01)\n",
            "Epoch: [148][20/97]\tTime  0.177 ( 0.191)\tLoss -8.5573e-01 (-8.7460e-01)\n",
            "Epoch: [148][30/97]\tTime  0.177 ( 0.187)\tLoss -8.7099e-01 (-8.7071e-01)\n",
            "Epoch: [148][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7310e-01 (-8.7146e-01)\n",
            "Epoch: [148][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7175e-01 (-8.7217e-01)\n",
            "Epoch: [148][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7031e-01 (-8.7105e-01)\n",
            "Epoch: [148][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7425e-01 (-8.7095e-01)\n",
            "Epoch: [148][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6993e-01 (-8.7172e-01)\n",
            "Epoch: [148][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7585e-01 (-8.7200e-01)\n",
            "Training...\n",
            "Epoch: [149][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.8330e-01 (-8.8330e-01)\n",
            "Epoch: [149][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7864e-01 (-8.7936e-01)\n",
            "Epoch: [149][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7714e-01 (-8.7822e-01)\n",
            "Epoch: [149][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7585e-01 (-8.7592e-01)\n",
            "Epoch: [149][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7473e-01 (-8.7419e-01)\n",
            "Epoch: [149][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6866e-01 (-8.7257e-01)\n",
            "Epoch: [149][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7954e-01 (-8.7156e-01)\n",
            "Epoch: [149][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8295e-01 (-8.7195e-01)\n",
            "Epoch: [149][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8658e-01 (-8.7271e-01)\n",
            "Epoch: [149][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6810e-01 (-8.7289e-01)\n",
            "Training...\n",
            "Epoch: [150][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.7999e-01 (-8.7999e-01)\n",
            "Epoch: [150][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8332e-01 (-8.8042e-01)\n",
            "Epoch: [150][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8168e-01 (-8.7669e-01)\n",
            "Epoch: [150][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6199e-01 (-8.7344e-01)\n",
            "Epoch: [150][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6664e-01 (-8.7244e-01)\n",
            "Epoch: [150][50/97]\tTime  0.178 ( 0.183)\tLoss -8.6967e-01 (-8.7142e-01)\n",
            "Epoch: [150][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7269e-01 (-8.7137e-01)\n",
            "Epoch: [150][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7253e-01 (-8.7252e-01)\n",
            "Epoch: [150][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7288e-01 (-8.7220e-01)\n",
            "Epoch: [150][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7780e-01 (-8.7181e-01)\n",
            "Validating...\n",
            "Top1: 0.7125822368421053\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [151][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.6316e-01 (-8.6316e-01)\n",
            "Epoch: [151][10/97]\tTime  0.179 ( 0.203)\tLoss -8.6309e-01 (-8.7281e-01)\n",
            "Epoch: [151][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7751e-01 (-8.7367e-01)\n",
            "Epoch: [151][30/97]\tTime  0.179 ( 0.187)\tLoss -8.5332e-01 (-8.7107e-01)\n",
            "Epoch: [151][40/97]\tTime  0.178 ( 0.185)\tLoss -8.7528e-01 (-8.7025e-01)\n",
            "Epoch: [151][50/97]\tTime  0.178 ( 0.184)\tLoss -8.7498e-01 (-8.7001e-01)\n",
            "Epoch: [151][60/97]\tTime  0.179 ( 0.183)\tLoss -8.6679e-01 (-8.7026e-01)\n",
            "Epoch: [151][70/97]\tTime  0.179 ( 0.182)\tLoss -8.7147e-01 (-8.7049e-01)\n",
            "Epoch: [151][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7476e-01 (-8.7093e-01)\n",
            "Epoch: [151][90/97]\tTime  0.179 ( 0.181)\tLoss -8.6676e-01 (-8.7117e-01)\n",
            "Training...\n",
            "Epoch: [152][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.6971e-01 (-8.6971e-01)\n",
            "Epoch: [152][10/97]\tTime  0.178 ( 0.202)\tLoss -8.5720e-01 (-8.6946e-01)\n",
            "Epoch: [152][20/97]\tTime  0.177 ( 0.191)\tLoss -8.6817e-01 (-8.7090e-01)\n",
            "Epoch: [152][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6279e-01 (-8.7038e-01)\n",
            "Epoch: [152][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7403e-01 (-8.6999e-01)\n",
            "Epoch: [152][50/97]\tTime  0.177 ( 0.183)\tLoss -8.5924e-01 (-8.6898e-01)\n",
            "Epoch: [152][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7640e-01 (-8.6958e-01)\n",
            "Epoch: [152][70/97]\tTime  0.177 ( 0.181)\tLoss -8.5024e-01 (-8.6894e-01)\n",
            "Epoch: [152][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7068e-01 (-8.6853e-01)\n",
            "Epoch: [152][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7272e-01 (-8.6827e-01)\n",
            "Training...\n",
            "Epoch: [153][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.7513e-01 (-8.7513e-01)\n",
            "Epoch: [153][10/97]\tTime  0.177 ( 0.202)\tLoss -8.6854e-01 (-8.6959e-01)\n",
            "Epoch: [153][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7155e-01 (-8.6868e-01)\n",
            "Epoch: [153][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7885e-01 (-8.7188e-01)\n",
            "Epoch: [153][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6790e-01 (-8.7277e-01)\n",
            "Epoch: [153][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7407e-01 (-8.7285e-01)\n",
            "Epoch: [153][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7259e-01 (-8.7289e-01)\n",
            "Epoch: [153][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7056e-01 (-8.7259e-01)\n",
            "Epoch: [153][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8572e-01 (-8.7309e-01)\n",
            "Epoch: [153][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7215e-01 (-8.7322e-01)\n",
            "Training...\n",
            "Epoch: [154][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.6243e-01 (-8.6243e-01)\n",
            "Epoch: [154][10/97]\tTime  0.178 ( 0.207)\tLoss -8.6806e-01 (-8.6840e-01)\n",
            "Epoch: [154][20/97]\tTime  0.177 ( 0.193)\tLoss -8.7712e-01 (-8.7208e-01)\n",
            "Epoch: [154][30/97]\tTime  0.178 ( 0.189)\tLoss -8.8851e-01 (-8.7291e-01)\n",
            "Epoch: [154][40/97]\tTime  0.179 ( 0.186)\tLoss -8.7852e-01 (-8.7352e-01)\n",
            "Epoch: [154][50/97]\tTime  0.200 ( 0.187)\tLoss -8.8562e-01 (-8.7304e-01)\n",
            "Epoch: [154][60/97]\tTime  0.178 ( 0.186)\tLoss -8.8295e-01 (-8.7286e-01)\n",
            "Epoch: [154][70/97]\tTime  0.190 ( 0.187)\tLoss -8.6641e-01 (-8.7284e-01)\n",
            "Epoch: [154][80/97]\tTime  0.190 ( 0.187)\tLoss -8.5996e-01 (-8.7245e-01)\n",
            "Epoch: [154][90/97]\tTime  0.190 ( 0.187)\tLoss -8.7592e-01 (-8.7209e-01)\n",
            "Training...\n",
            "Epoch: [155][ 0/97]\tTime  0.464 ( 0.464)\tLoss -8.8365e-01 (-8.8365e-01)\n",
            "Epoch: [155][10/97]\tTime  0.190 ( 0.215)\tLoss -8.8052e-01 (-8.8254e-01)\n",
            "Epoch: [155][20/97]\tTime  0.191 ( 0.203)\tLoss -8.7509e-01 (-8.8018e-01)\n",
            "Epoch: [155][30/97]\tTime  0.189 ( 0.200)\tLoss -8.7077e-01 (-8.7679e-01)\n",
            "Epoch: [155][40/97]\tTime  0.183 ( 0.196)\tLoss -8.7415e-01 (-8.7511e-01)\n",
            "Epoch: [155][50/97]\tTime  0.178 ( 0.193)\tLoss -8.6615e-01 (-8.7250e-01)\n",
            "Epoch: [155][60/97]\tTime  0.178 ( 0.190)\tLoss -8.7566e-01 (-8.7294e-01)\n",
            "Epoch: [155][70/97]\tTime  0.178 ( 0.189)\tLoss -8.6890e-01 (-8.7275e-01)\n",
            "Epoch: [155][80/97]\tTime  0.178 ( 0.187)\tLoss -8.7634e-01 (-8.7225e-01)\n",
            "Epoch: [155][90/97]\tTime  0.178 ( 0.186)\tLoss -8.7735e-01 (-8.7265e-01)\n",
            "Validating...\n",
            "Top1: 0.7362253289473685\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [156][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7340e-01 (-8.7340e-01)\n",
            "Epoch: [156][10/97]\tTime  0.186 ( 0.206)\tLoss -8.6851e-01 (-8.6685e-01)\n",
            "Epoch: [156][20/97]\tTime  0.179 ( 0.193)\tLoss -8.6246e-01 (-8.6754e-01)\n",
            "Epoch: [156][30/97]\tTime  0.178 ( 0.188)\tLoss -8.7748e-01 (-8.6914e-01)\n",
            "Epoch: [156][40/97]\tTime  0.178 ( 0.186)\tLoss -8.8046e-01 (-8.7006e-01)\n",
            "Epoch: [156][50/97]\tTime  0.179 ( 0.185)\tLoss -8.6584e-01 (-8.7139e-01)\n",
            "Epoch: [156][60/97]\tTime  0.179 ( 0.184)\tLoss -8.6964e-01 (-8.7223e-01)\n",
            "Epoch: [156][70/97]\tTime  0.179 ( 0.183)\tLoss -8.8638e-01 (-8.7280e-01)\n",
            "Epoch: [156][80/97]\tTime  0.178 ( 0.182)\tLoss -8.9134e-01 (-8.7384e-01)\n",
            "Epoch: [156][90/97]\tTime  0.179 ( 0.182)\tLoss -8.7575e-01 (-8.7351e-01)\n",
            "Training...\n",
            "Epoch: [157][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.5892e-01 (-8.5892e-01)\n",
            "Epoch: [157][10/97]\tTime  0.177 ( 0.203)\tLoss -8.5134e-01 (-8.6535e-01)\n",
            "Epoch: [157][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7138e-01 (-8.6568e-01)\n",
            "Epoch: [157][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7447e-01 (-8.6562e-01)\n",
            "Epoch: [157][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7372e-01 (-8.6659e-01)\n",
            "Epoch: [157][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7813e-01 (-8.6827e-01)\n",
            "Epoch: [157][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6509e-01 (-8.6793e-01)\n",
            "Epoch: [157][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6521e-01 (-8.6828e-01)\n",
            "Epoch: [157][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6687e-01 (-8.6825e-01)\n",
            "Epoch: [157][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9225e-01 (-8.6914e-01)\n",
            "Training...\n",
            "Epoch: [158][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.6293e-01 (-8.6293e-01)\n",
            "Epoch: [158][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8192e-01 (-8.7461e-01)\n",
            "Epoch: [158][20/97]\tTime  0.178 ( 0.191)\tLoss -8.6712e-01 (-8.7353e-01)\n",
            "Epoch: [158][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7487e-01 (-8.7235e-01)\n",
            "Epoch: [158][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7434e-01 (-8.7172e-01)\n",
            "Epoch: [158][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7202e-01 (-8.7151e-01)\n",
            "Epoch: [158][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6729e-01 (-8.7187e-01)\n",
            "Epoch: [158][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7966e-01 (-8.7224e-01)\n",
            "Epoch: [158][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9087e-01 (-8.7264e-01)\n",
            "Epoch: [158][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7240e-01 (-8.7284e-01)\n",
            "Training...\n",
            "Epoch: [159][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7185e-01 (-8.7185e-01)\n",
            "Epoch: [159][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7426e-01 (-8.7427e-01)\n",
            "Epoch: [159][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7246e-01 (-8.7334e-01)\n",
            "Epoch: [159][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7672e-01 (-8.7198e-01)\n",
            "Epoch: [159][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7596e-01 (-8.7297e-01)\n",
            "Epoch: [159][50/97]\tTime  0.177 ( 0.183)\tLoss -8.5418e-01 (-8.7263e-01)\n",
            "Epoch: [159][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7274e-01 (-8.7197e-01)\n",
            "Epoch: [159][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9692e-01 (-8.7270e-01)\n",
            "Epoch: [159][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6731e-01 (-8.7260e-01)\n",
            "Epoch: [159][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7660e-01 (-8.7333e-01)\n",
            "Training...\n",
            "Epoch: [160][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8306e-01 (-8.8306e-01)\n",
            "Epoch: [160][10/97]\tTime  0.178 ( 0.203)\tLoss -8.6254e-01 (-8.7302e-01)\n",
            "Epoch: [160][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8642e-01 (-8.7166e-01)\n",
            "Epoch: [160][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7826e-01 (-8.7148e-01)\n",
            "Epoch: [160][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7547e-01 (-8.7361e-01)\n",
            "Epoch: [160][50/97]\tTime  0.178 ( 0.183)\tLoss -8.6623e-01 (-8.7439e-01)\n",
            "Epoch: [160][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8761e-01 (-8.7592e-01)\n",
            "Epoch: [160][70/97]\tTime  0.177 ( 0.182)\tLoss -8.8549e-01 (-8.7570e-01)\n",
            "Epoch: [160][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7428e-01 (-8.7629e-01)\n",
            "Epoch: [160][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8252e-01 (-8.7588e-01)\n",
            "Validating...\n",
            "Top1: 0.7305715460526315\n",
            "Training...\n",
            "Epoch: [161][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.6589e-01 (-8.6589e-01)\n",
            "Epoch: [161][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8006e-01 (-8.7293e-01)\n",
            "Epoch: [161][20/97]\tTime  0.178 ( 0.190)\tLoss -8.6313e-01 (-8.7036e-01)\n",
            "Epoch: [161][30/97]\tTime  0.178 ( 0.186)\tLoss -8.6985e-01 (-8.7057e-01)\n",
            "Epoch: [161][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7778e-01 (-8.7029e-01)\n",
            "Epoch: [161][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7701e-01 (-8.7075e-01)\n",
            "Epoch: [161][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7552e-01 (-8.7169e-01)\n",
            "Epoch: [161][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9193e-01 (-8.7323e-01)\n",
            "Epoch: [161][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7752e-01 (-8.7466e-01)\n",
            "Epoch: [161][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9113e-01 (-8.7603e-01)\n",
            "Training...\n",
            "Epoch: [162][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7893e-01 (-8.7893e-01)\n",
            "Epoch: [162][10/97]\tTime  0.178 ( 0.203)\tLoss -8.9089e-01 (-8.7934e-01)\n",
            "Epoch: [162][20/97]\tTime  0.178 ( 0.191)\tLoss -8.6495e-01 (-8.7679e-01)\n",
            "Epoch: [162][30/97]\tTime  0.177 ( 0.187)\tLoss -8.8337e-01 (-8.7622e-01)\n",
            "Epoch: [162][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7840e-01 (-8.7639e-01)\n",
            "Epoch: [162][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7946e-01 (-8.7703e-01)\n",
            "Epoch: [162][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7320e-01 (-8.7748e-01)\n",
            "Epoch: [162][70/97]\tTime  0.177 ( 0.182)\tLoss -8.7596e-01 (-8.7723e-01)\n",
            "Epoch: [162][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8830e-01 (-8.7670e-01)\n",
            "Epoch: [162][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7435e-01 (-8.7635e-01)\n",
            "Training...\n",
            "Epoch: [163][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.8841e-01 (-8.8841e-01)\n",
            "Epoch: [163][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8666e-01 (-8.7850e-01)\n",
            "Epoch: [163][20/97]\tTime  0.178 ( 0.191)\tLoss -8.5394e-01 (-8.7730e-01)\n",
            "Epoch: [163][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6410e-01 (-8.7620e-01)\n",
            "Epoch: [163][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6818e-01 (-8.7584e-01)\n",
            "Epoch: [163][50/97]\tTime  0.177 ( 0.183)\tLoss -8.5595e-01 (-8.7429e-01)\n",
            "Epoch: [163][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7452e-01 (-8.7397e-01)\n",
            "Epoch: [163][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7504e-01 (-8.7370e-01)\n",
            "Epoch: [163][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8304e-01 (-8.7451e-01)\n",
            "Epoch: [163][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8748e-01 (-8.7590e-01)\n",
            "Training...\n",
            "Epoch: [164][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.8089e-01 (-8.8089e-01)\n",
            "Epoch: [164][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8117e-01 (-8.8371e-01)\n",
            "Epoch: [164][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7040e-01 (-8.8254e-01)\n",
            "Epoch: [164][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8355e-01 (-8.8213e-01)\n",
            "Epoch: [164][40/97]\tTime  0.179 ( 0.184)\tLoss -8.7939e-01 (-8.8189e-01)\n",
            "Epoch: [164][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8828e-01 (-8.8176e-01)\n",
            "Epoch: [164][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7679e-01 (-8.8184e-01)\n",
            "Epoch: [164][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7922e-01 (-8.8103e-01)\n",
            "Epoch: [164][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7464e-01 (-8.8023e-01)\n",
            "Epoch: [164][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7073e-01 (-8.7928e-01)\n",
            "Training...\n",
            "Epoch: [165][ 0/97]\tTime  0.462 ( 0.462)\tLoss -8.7515e-01 (-8.7515e-01)\n",
            "Epoch: [165][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7744e-01 (-8.7635e-01)\n",
            "Epoch: [165][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9248e-01 (-8.7605e-01)\n",
            "Epoch: [165][30/97]\tTime  0.177 ( 0.187)\tLoss -8.8140e-01 (-8.7766e-01)\n",
            "Epoch: [165][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7886e-01 (-8.7851e-01)\n",
            "Epoch: [165][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8247e-01 (-8.7818e-01)\n",
            "Epoch: [165][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7806e-01 (-8.7791e-01)\n",
            "Epoch: [165][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7798e-01 (-8.7846e-01)\n",
            "Epoch: [165][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8367e-01 (-8.7901e-01)\n",
            "Epoch: [165][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7444e-01 (-8.7932e-01)\n",
            "Validating...\n",
            "Top1: 0.7329358552631579\n",
            "Training...\n",
            "Epoch: [166][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.7578e-01 (-8.7578e-01)\n",
            "Epoch: [166][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8045e-01 (-8.7670e-01)\n",
            "Epoch: [166][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9112e-01 (-8.7796e-01)\n",
            "Epoch: [166][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6976e-01 (-8.7910e-01)\n",
            "Epoch: [166][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7811e-01 (-8.7728e-01)\n",
            "Epoch: [166][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7175e-01 (-8.7598e-01)\n",
            "Epoch: [166][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7862e-01 (-8.7579e-01)\n",
            "Epoch: [166][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7612e-01 (-8.7570e-01)\n",
            "Epoch: [166][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7352e-01 (-8.7534e-01)\n",
            "Epoch: [166][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7817e-01 (-8.7543e-01)\n",
            "Training...\n",
            "Epoch: [167][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7288e-01 (-8.7288e-01)\n",
            "Epoch: [167][10/97]\tTime  0.178 ( 0.202)\tLoss -8.6857e-01 (-8.7265e-01)\n",
            "Epoch: [167][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8008e-01 (-8.7449e-01)\n",
            "Epoch: [167][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7813e-01 (-8.7558e-01)\n",
            "Epoch: [167][40/97]\tTime  0.179 ( 0.184)\tLoss -8.8486e-01 (-8.7484e-01)\n",
            "Epoch: [167][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8079e-01 (-8.7620e-01)\n",
            "Epoch: [167][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7643e-01 (-8.7644e-01)\n",
            "Epoch: [167][70/97]\tTime  0.178 ( 0.182)\tLoss -8.7625e-01 (-8.7704e-01)\n",
            "Epoch: [167][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6353e-01 (-8.7650e-01)\n",
            "Epoch: [167][90/97]\tTime  0.178 ( 0.181)\tLoss -8.6319e-01 (-8.7569e-01)\n",
            "Training...\n",
            "Epoch: [168][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8454e-01 (-8.8454e-01)\n",
            "Epoch: [168][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8304e-01 (-8.8018e-01)\n",
            "Epoch: [168][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7603e-01 (-8.8006e-01)\n",
            "Epoch: [168][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8404e-01 (-8.7929e-01)\n",
            "Epoch: [168][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7624e-01 (-8.7885e-01)\n",
            "Epoch: [168][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7939e-01 (-8.7977e-01)\n",
            "Epoch: [168][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8522e-01 (-8.8130e-01)\n",
            "Epoch: [168][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8902e-01 (-8.8196e-01)\n",
            "Epoch: [168][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8775e-01 (-8.8225e-01)\n",
            "Epoch: [168][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7702e-01 (-8.8138e-01)\n",
            "Training...\n",
            "Epoch: [169][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.6654e-01 (-8.6654e-01)\n",
            "Epoch: [169][10/97]\tTime  0.177 ( 0.203)\tLoss -8.7400e-01 (-8.7979e-01)\n",
            "Epoch: [169][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8305e-01 (-8.8202e-01)\n",
            "Epoch: [169][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9261e-01 (-8.8379e-01)\n",
            "Epoch: [169][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7197e-01 (-8.8260e-01)\n",
            "Epoch: [169][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7075e-01 (-8.8143e-01)\n",
            "Epoch: [169][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8319e-01 (-8.8032e-01)\n",
            "Epoch: [169][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8476e-01 (-8.8072e-01)\n",
            "Epoch: [169][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8254e-01 (-8.8084e-01)\n",
            "Epoch: [169][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8412e-01 (-8.8114e-01)\n",
            "Training...\n",
            "Epoch: [170][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.7128e-01 (-8.7128e-01)\n",
            "Epoch: [170][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8555e-01 (-8.7999e-01)\n",
            "Epoch: [170][20/97]\tTime  0.178 ( 0.191)\tLoss -8.6947e-01 (-8.7853e-01)\n",
            "Epoch: [170][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7803e-01 (-8.7953e-01)\n",
            "Epoch: [170][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7672e-01 (-8.8142e-01)\n",
            "Epoch: [170][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8684e-01 (-8.8144e-01)\n",
            "Epoch: [170][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8104e-01 (-8.8161e-01)\n",
            "Epoch: [170][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9097e-01 (-8.8179e-01)\n",
            "Epoch: [170][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8699e-01 (-8.8187e-01)\n",
            "Epoch: [170][90/97]\tTime  0.178 ( 0.181)\tLoss -8.6832e-01 (-8.8166e-01)\n",
            "Validating...\n",
            "Top1: 0.7365337171052632\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [171][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.8285e-01 (-8.8285e-01)\n",
            "Epoch: [171][10/97]\tTime  0.178 ( 0.203)\tLoss -8.9946e-01 (-8.8603e-01)\n",
            "Epoch: [171][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7873e-01 (-8.8277e-01)\n",
            "Epoch: [171][30/97]\tTime  0.179 ( 0.187)\tLoss -8.8623e-01 (-8.8049e-01)\n",
            "Epoch: [171][40/97]\tTime  0.178 ( 0.185)\tLoss -8.8582e-01 (-8.7973e-01)\n",
            "Epoch: [171][50/97]\tTime  0.178 ( 0.184)\tLoss -8.8535e-01 (-8.7997e-01)\n",
            "Epoch: [171][60/97]\tTime  0.179 ( 0.183)\tLoss -8.7900e-01 (-8.8065e-01)\n",
            "Epoch: [171][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8148e-01 (-8.8128e-01)\n",
            "Epoch: [171][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7597e-01 (-8.8092e-01)\n",
            "Epoch: [171][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7178e-01 (-8.7995e-01)\n",
            "Training...\n",
            "Epoch: [172][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9162e-01 (-8.9162e-01)\n",
            "Epoch: [172][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8406e-01 (-8.7983e-01)\n",
            "Epoch: [172][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8053e-01 (-8.8221e-01)\n",
            "Epoch: [172][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7616e-01 (-8.8371e-01)\n",
            "Epoch: [172][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8305e-01 (-8.8386e-01)\n",
            "Epoch: [172][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8494e-01 (-8.8301e-01)\n",
            "Epoch: [172][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7652e-01 (-8.8246e-01)\n",
            "Epoch: [172][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7305e-01 (-8.8190e-01)\n",
            "Epoch: [172][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8201e-01 (-8.8075e-01)\n",
            "Epoch: [172][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8149e-01 (-8.8035e-01)\n",
            "Training...\n",
            "Epoch: [173][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9552e-01 (-8.9552e-01)\n",
            "Epoch: [173][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8157e-01 (-8.7975e-01)\n",
            "Epoch: [173][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7906e-01 (-8.8188e-01)\n",
            "Epoch: [173][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7999e-01 (-8.8093e-01)\n",
            "Epoch: [173][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7599e-01 (-8.8015e-01)\n",
            "Epoch: [173][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8159e-01 (-8.8058e-01)\n",
            "Epoch: [173][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8105e-01 (-8.8031e-01)\n",
            "Epoch: [173][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7875e-01 (-8.7984e-01)\n",
            "Epoch: [173][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7634e-01 (-8.7957e-01)\n",
            "Epoch: [173][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8065e-01 (-8.7933e-01)\n",
            "Training...\n",
            "Epoch: [174][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8245e-01 (-8.8245e-01)\n",
            "Epoch: [174][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8844e-01 (-8.7863e-01)\n",
            "Epoch: [174][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8234e-01 (-8.7985e-01)\n",
            "Epoch: [174][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8343e-01 (-8.7982e-01)\n",
            "Epoch: [174][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8312e-01 (-8.8031e-01)\n",
            "Epoch: [174][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8200e-01 (-8.8079e-01)\n",
            "Epoch: [174][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6885e-01 (-8.8059e-01)\n",
            "Epoch: [174][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7529e-01 (-8.8081e-01)\n",
            "Epoch: [174][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8707e-01 (-8.8087e-01)\n",
            "Epoch: [174][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8175e-01 (-8.8049e-01)\n",
            "Training...\n",
            "Epoch: [175][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.6834e-01 (-8.6834e-01)\n",
            "Epoch: [175][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8002e-01 (-8.8089e-01)\n",
            "Epoch: [175][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8312e-01 (-8.8171e-01)\n",
            "Epoch: [175][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8170e-01 (-8.8091e-01)\n",
            "Epoch: [175][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7520e-01 (-8.8084e-01)\n",
            "Epoch: [175][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8972e-01 (-8.8124e-01)\n",
            "Epoch: [175][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7050e-01 (-8.8108e-01)\n",
            "Epoch: [175][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8365e-01 (-8.8089e-01)\n",
            "Epoch: [175][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7795e-01 (-8.8092e-01)\n",
            "Epoch: [175][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9384e-01 (-8.8165e-01)\n",
            "Validating...\n",
            "Top1: 0.7365337171052632\n",
            "Training...\n",
            "Epoch: [176][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.8513e-01 (-8.8513e-01)\n",
            "Epoch: [176][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8381e-01 (-8.8716e-01)\n",
            "Epoch: [176][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8891e-01 (-8.9035e-01)\n",
            "Epoch: [176][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9842e-01 (-8.9145e-01)\n",
            "Epoch: [176][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9746e-01 (-8.9158e-01)\n",
            "Epoch: [176][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8262e-01 (-8.9167e-01)\n",
            "Epoch: [176][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8671e-01 (-8.9104e-01)\n",
            "Epoch: [176][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9248e-01 (-8.9051e-01)\n",
            "Epoch: [176][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8670e-01 (-8.9018e-01)\n",
            "Epoch: [176][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8871e-01 (-8.8949e-01)\n",
            "Training...\n",
            "Epoch: [177][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9289e-01 (-8.9289e-01)\n",
            "Epoch: [177][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8670e-01 (-8.8383e-01)\n",
            "Epoch: [177][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8901e-01 (-8.8320e-01)\n",
            "Epoch: [177][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8394e-01 (-8.8276e-01)\n",
            "Epoch: [177][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7250e-01 (-8.8195e-01)\n",
            "Epoch: [177][50/97]\tTime  0.183 ( 0.183)\tLoss -8.8502e-01 (-8.8150e-01)\n",
            "Epoch: [177][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7617e-01 (-8.8067e-01)\n",
            "Epoch: [177][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8346e-01 (-8.8096e-01)\n",
            "Epoch: [177][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7732e-01 (-8.8139e-01)\n",
            "Epoch: [177][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8003e-01 (-8.8118e-01)\n",
            "Training...\n",
            "Epoch: [178][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7427e-01 (-8.7427e-01)\n",
            "Epoch: [178][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7607e-01 (-8.7567e-01)\n",
            "Epoch: [178][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9396e-01 (-8.8035e-01)\n",
            "Epoch: [178][30/97]\tTime  0.179 ( 0.186)\tLoss -8.7048e-01 (-8.8097e-01)\n",
            "Epoch: [178][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8510e-01 (-8.8014e-01)\n",
            "Epoch: [178][50/97]\tTime  0.178 ( 0.183)\tLoss -8.6412e-01 (-8.7962e-01)\n",
            "Epoch: [178][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8297e-01 (-8.7951e-01)\n",
            "Epoch: [178][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8163e-01 (-8.7943e-01)\n",
            "Epoch: [178][80/97]\tTime  0.179 ( 0.181)\tLoss -8.8438e-01 (-8.7929e-01)\n",
            "Epoch: [178][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7493e-01 (-8.7919e-01)\n",
            "Training...\n",
            "Epoch: [179][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8520e-01 (-8.8520e-01)\n",
            "Epoch: [179][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8524e-01 (-8.8143e-01)\n",
            "Epoch: [179][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7763e-01 (-8.8054e-01)\n",
            "Epoch: [179][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7752e-01 (-8.7935e-01)\n",
            "Epoch: [179][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7216e-01 (-8.7862e-01)\n",
            "Epoch: [179][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7754e-01 (-8.7839e-01)\n",
            "Epoch: [179][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8696e-01 (-8.7919e-01)\n",
            "Epoch: [179][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7820e-01 (-8.8005e-01)\n",
            "Epoch: [179][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8313e-01 (-8.8031e-01)\n",
            "Epoch: [179][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9096e-01 (-8.8054e-01)\n",
            "Training...\n",
            "Epoch: [180][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.7684e-01 (-8.7684e-01)\n",
            "Epoch: [180][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9314e-01 (-8.8146e-01)\n",
            "Epoch: [180][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7166e-01 (-8.8110e-01)\n",
            "Epoch: [180][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7092e-01 (-8.8047e-01)\n",
            "Epoch: [180][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0260e-01 (-8.8238e-01)\n",
            "Epoch: [180][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8757e-01 (-8.8311e-01)\n",
            "Epoch: [180][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8103e-01 (-8.8357e-01)\n",
            "Epoch: [180][70/97]\tTime  0.178 ( 0.181)\tLoss -8.6564e-01 (-8.8265e-01)\n",
            "Epoch: [180][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7275e-01 (-8.8236e-01)\n",
            "Epoch: [180][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8009e-01 (-8.8191e-01)\n",
            "Validating...\n",
            "Top1: 0.7548314144736842\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [181][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.7373e-01 (-8.7373e-01)\n",
            "Epoch: [181][10/97]\tTime  0.178 ( 0.203)\tLoss -8.6668e-01 (-8.7073e-01)\n",
            "Epoch: [181][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7108e-01 (-8.7544e-01)\n",
            "Epoch: [181][30/97]\tTime  0.178 ( 0.187)\tLoss -8.8151e-01 (-8.7847e-01)\n",
            "Epoch: [181][40/97]\tTime  0.179 ( 0.185)\tLoss -8.9186e-01 (-8.7914e-01)\n",
            "Epoch: [181][50/97]\tTime  0.179 ( 0.184)\tLoss -8.8138e-01 (-8.7922e-01)\n",
            "Epoch: [181][60/97]\tTime  0.178 ( 0.183)\tLoss -8.7831e-01 (-8.7869e-01)\n",
            "Epoch: [181][70/97]\tTime  0.179 ( 0.182)\tLoss -8.8531e-01 (-8.7914e-01)\n",
            "Epoch: [181][80/97]\tTime  0.178 ( 0.182)\tLoss -8.8350e-01 (-8.7896e-01)\n",
            "Epoch: [181][90/97]\tTime  0.179 ( 0.181)\tLoss -8.8963e-01 (-8.7854e-01)\n",
            "Training...\n",
            "Epoch: [182][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8157e-01 (-8.8157e-01)\n",
            "Epoch: [182][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8798e-01 (-8.8331e-01)\n",
            "Epoch: [182][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8980e-01 (-8.8398e-01)\n",
            "Epoch: [182][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8325e-01 (-8.8393e-01)\n",
            "Epoch: [182][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8958e-01 (-8.8384e-01)\n",
            "Epoch: [182][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8957e-01 (-8.8472e-01)\n",
            "Epoch: [182][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8708e-01 (-8.8401e-01)\n",
            "Epoch: [182][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0346e-01 (-8.8453e-01)\n",
            "Epoch: [182][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8949e-01 (-8.8392e-01)\n",
            "Epoch: [182][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8331e-01 (-8.8365e-01)\n",
            "Training...\n",
            "Epoch: [183][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8652e-01 (-8.8652e-01)\n",
            "Epoch: [183][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7305e-01 (-8.8179e-01)\n",
            "Epoch: [183][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8822e-01 (-8.8362e-01)\n",
            "Epoch: [183][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8802e-01 (-8.8327e-01)\n",
            "Epoch: [183][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8199e-01 (-8.8232e-01)\n",
            "Epoch: [183][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8200e-01 (-8.8155e-01)\n",
            "Epoch: [183][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7532e-01 (-8.8110e-01)\n",
            "Epoch: [183][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8272e-01 (-8.8098e-01)\n",
            "Epoch: [183][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6695e-01 (-8.8101e-01)\n",
            "Epoch: [183][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8228e-01 (-8.8050e-01)\n",
            "Training...\n",
            "Epoch: [184][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.8373e-01 (-8.8373e-01)\n",
            "Epoch: [184][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9470e-01 (-8.8066e-01)\n",
            "Epoch: [184][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7048e-01 (-8.7871e-01)\n",
            "Epoch: [184][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7647e-01 (-8.7868e-01)\n",
            "Epoch: [184][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7871e-01 (-8.7927e-01)\n",
            "Epoch: [184][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9326e-01 (-8.7997e-01)\n",
            "Epoch: [184][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8318e-01 (-8.8020e-01)\n",
            "Epoch: [184][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7308e-01 (-8.8022e-01)\n",
            "Epoch: [184][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6485e-01 (-8.7997e-01)\n",
            "Epoch: [184][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7654e-01 (-8.7963e-01)\n",
            "Training...\n",
            "Epoch: [185][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7206e-01 (-8.7206e-01)\n",
            "Epoch: [185][10/97]\tTime  0.178 ( 0.202)\tLoss -8.5970e-01 (-8.7464e-01)\n",
            "Epoch: [185][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7820e-01 (-8.7459e-01)\n",
            "Epoch: [185][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9317e-01 (-8.7713e-01)\n",
            "Epoch: [185][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8339e-01 (-8.7986e-01)\n",
            "Epoch: [185][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7929e-01 (-8.8004e-01)\n",
            "Epoch: [185][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7138e-01 (-8.7866e-01)\n",
            "Epoch: [185][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8276e-01 (-8.7903e-01)\n",
            "Epoch: [185][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7476e-01 (-8.7865e-01)\n",
            "Epoch: [185][90/97]\tTime  0.178 ( 0.181)\tLoss -8.6943e-01 (-8.7845e-01)\n",
            "Validating...\n",
            "Top1: 0.7486636513157895\n",
            "Training...\n",
            "Epoch: [186][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.6290e-01 (-8.6290e-01)\n",
            "Epoch: [186][10/97]\tTime  0.177 ( 0.203)\tLoss -8.7369e-01 (-8.7610e-01)\n",
            "Epoch: [186][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7433e-01 (-8.7499e-01)\n",
            "Epoch: [186][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7883e-01 (-8.7795e-01)\n",
            "Epoch: [186][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0388e-01 (-8.8065e-01)\n",
            "Epoch: [186][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7948e-01 (-8.8161e-01)\n",
            "Epoch: [186][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7864e-01 (-8.8107e-01)\n",
            "Epoch: [186][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7728e-01 (-8.8034e-01)\n",
            "Epoch: [186][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9170e-01 (-8.8024e-01)\n",
            "Epoch: [186][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7551e-01 (-8.8006e-01)\n",
            "Training...\n",
            "Epoch: [187][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8159e-01 (-8.8159e-01)\n",
            "Epoch: [187][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7317e-01 (-8.7391e-01)\n",
            "Epoch: [187][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8021e-01 (-8.7090e-01)\n",
            "Epoch: [187][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7131e-01 (-8.7146e-01)\n",
            "Epoch: [187][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6639e-01 (-8.7336e-01)\n",
            "Epoch: [187][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8654e-01 (-8.7535e-01)\n",
            "Epoch: [187][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7912e-01 (-8.7567e-01)\n",
            "Epoch: [187][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7164e-01 (-8.7565e-01)\n",
            "Epoch: [187][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7598e-01 (-8.7524e-01)\n",
            "Epoch: [187][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7695e-01 (-8.7507e-01)\n",
            "Training...\n",
            "Epoch: [188][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7518e-01 (-8.7518e-01)\n",
            "Epoch: [188][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7276e-01 (-8.7702e-01)\n",
            "Epoch: [188][20/97]\tTime  0.177 ( 0.191)\tLoss -8.6605e-01 (-8.7556e-01)\n",
            "Epoch: [188][30/97]\tTime  0.178 ( 0.186)\tLoss -8.6647e-01 (-8.7502e-01)\n",
            "Epoch: [188][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6617e-01 (-8.7564e-01)\n",
            "Epoch: [188][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8017e-01 (-8.7657e-01)\n",
            "Epoch: [188][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7661e-01 (-8.7747e-01)\n",
            "Epoch: [188][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7839e-01 (-8.7848e-01)\n",
            "Epoch: [188][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8612e-01 (-8.7922e-01)\n",
            "Epoch: [188][90/97]\tTime  0.179 ( 0.181)\tLoss -8.9769e-01 (-8.8000e-01)\n",
            "Training...\n",
            "Epoch: [189][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.8322e-01 (-8.8322e-01)\n",
            "Epoch: [189][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8364e-01 (-8.8752e-01)\n",
            "Epoch: [189][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8296e-01 (-8.8464e-01)\n",
            "Epoch: [189][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8274e-01 (-8.8244e-01)\n",
            "Epoch: [189][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7260e-01 (-8.8151e-01)\n",
            "Epoch: [189][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7921e-01 (-8.8015e-01)\n",
            "Epoch: [189][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8066e-01 (-8.7913e-01)\n",
            "Epoch: [189][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8146e-01 (-8.7974e-01)\n",
            "Epoch: [189][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9500e-01 (-8.8010e-01)\n",
            "Epoch: [189][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7410e-01 (-8.7993e-01)\n",
            "Training...\n",
            "Epoch: [190][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8118e-01 (-8.8118e-01)\n",
            "Epoch: [190][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7572e-01 (-8.7761e-01)\n",
            "Epoch: [190][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7694e-01 (-8.7644e-01)\n",
            "Epoch: [190][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8236e-01 (-8.7801e-01)\n",
            "Epoch: [190][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8951e-01 (-8.8079e-01)\n",
            "Epoch: [190][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8662e-01 (-8.8278e-01)\n",
            "Epoch: [190][60/97]\tTime  0.178 ( 0.182)\tLoss -9.0524e-01 (-8.8477e-01)\n",
            "Epoch: [190][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7473e-01 (-8.8512e-01)\n",
            "Epoch: [190][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7710e-01 (-8.8470e-01)\n",
            "Epoch: [190][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8045e-01 (-8.8460e-01)\n",
            "Validating...\n",
            "Top1: 0.7494860197368421\n",
            "Training...\n",
            "Epoch: [191][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8706e-01 (-8.8706e-01)\n",
            "Epoch: [191][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9001e-01 (-8.8035e-01)\n",
            "Epoch: [191][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7191e-01 (-8.7946e-01)\n",
            "Epoch: [191][30/97]\tTime  0.177 ( 0.186)\tLoss -8.6613e-01 (-8.7846e-01)\n",
            "Epoch: [191][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9025e-01 (-8.7870e-01)\n",
            "Epoch: [191][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9026e-01 (-8.7949e-01)\n",
            "Epoch: [191][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7731e-01 (-8.7908e-01)\n",
            "Epoch: [191][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9096e-01 (-8.7791e-01)\n",
            "Epoch: [191][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8903e-01 (-8.7815e-01)\n",
            "Epoch: [191][90/97]\tTime  0.178 ( 0.180)\tLoss -8.6874e-01 (-8.7824e-01)\n",
            "Training...\n",
            "Epoch: [192][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.6783e-01 (-8.6783e-01)\n",
            "Epoch: [192][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7959e-01 (-8.7769e-01)\n",
            "Epoch: [192][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7817e-01 (-8.7768e-01)\n",
            "Epoch: [192][30/97]\tTime  0.177 ( 0.186)\tLoss -8.5980e-01 (-8.7821e-01)\n",
            "Epoch: [192][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6741e-01 (-8.7626e-01)\n",
            "Epoch: [192][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7265e-01 (-8.7588e-01)\n",
            "Epoch: [192][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7432e-01 (-8.7518e-01)\n",
            "Epoch: [192][70/97]\tTime  0.178 ( 0.181)\tLoss -8.6963e-01 (-8.7498e-01)\n",
            "Epoch: [192][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8525e-01 (-8.7534e-01)\n",
            "Epoch: [192][90/97]\tTime  0.178 ( 0.181)\tLoss -8.6808e-01 (-8.7609e-01)\n",
            "Training...\n",
            "Epoch: [193][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8143e-01 (-8.8143e-01)\n",
            "Epoch: [193][10/97]\tTime  0.177 ( 0.202)\tLoss -8.6411e-01 (-8.7222e-01)\n",
            "Epoch: [193][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8019e-01 (-8.7445e-01)\n",
            "Epoch: [193][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8357e-01 (-8.7844e-01)\n",
            "Epoch: [193][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7712e-01 (-8.7933e-01)\n",
            "Epoch: [193][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7404e-01 (-8.8035e-01)\n",
            "Epoch: [193][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7473e-01 (-8.8099e-01)\n",
            "Epoch: [193][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6674e-01 (-8.8009e-01)\n",
            "Epoch: [193][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7463e-01 (-8.7876e-01)\n",
            "Epoch: [193][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7670e-01 (-8.7857e-01)\n",
            "Training...\n",
            "Epoch: [194][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8428e-01 (-8.8428e-01)\n",
            "Epoch: [194][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8532e-01 (-8.7973e-01)\n",
            "Epoch: [194][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7767e-01 (-8.7783e-01)\n",
            "Epoch: [194][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7106e-01 (-8.7549e-01)\n",
            "Epoch: [194][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6871e-01 (-8.7528e-01)\n",
            "Epoch: [194][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6232e-01 (-8.7473e-01)\n",
            "Epoch: [194][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7258e-01 (-8.7486e-01)\n",
            "Epoch: [194][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8645e-01 (-8.7567e-01)\n",
            "Epoch: [194][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7776e-01 (-8.7592e-01)\n",
            "Epoch: [194][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7619e-01 (-8.7648e-01)\n",
            "Training...\n",
            "Epoch: [195][ 0/97]\tTime  0.461 ( 0.461)\tLoss -8.7706e-01 (-8.7706e-01)\n",
            "Epoch: [195][10/97]\tTime  0.177 ( 0.203)\tLoss -8.6906e-01 (-8.7451e-01)\n",
            "Epoch: [195][20/97]\tTime  0.177 ( 0.191)\tLoss -8.6621e-01 (-8.7661e-01)\n",
            "Epoch: [195][30/97]\tTime  0.177 ( 0.187)\tLoss -8.7308e-01 (-8.7713e-01)\n",
            "Epoch: [195][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6229e-01 (-8.7793e-01)\n",
            "Epoch: [195][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8132e-01 (-8.7816e-01)\n",
            "Epoch: [195][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8487e-01 (-8.7790e-01)\n",
            "Epoch: [195][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7827e-01 (-8.7770e-01)\n",
            "Epoch: [195][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6744e-01 (-8.7733e-01)\n",
            "Epoch: [195][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7170e-01 (-8.7720e-01)\n",
            "Validating...\n",
            "Top1: 0.751953125\n",
            "Training...\n",
            "Epoch: [196][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.8105e-01 (-8.8105e-01)\n",
            "Epoch: [196][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8058e-01 (-8.7798e-01)\n",
            "Epoch: [196][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7862e-01 (-8.7781e-01)\n",
            "Epoch: [196][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7405e-01 (-8.7890e-01)\n",
            "Epoch: [196][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7412e-01 (-8.7851e-01)\n",
            "Epoch: [196][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7455e-01 (-8.7753e-01)\n",
            "Epoch: [196][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8917e-01 (-8.7720e-01)\n",
            "Epoch: [196][70/97]\tTime  0.177 ( 0.182)\tLoss -8.7094e-01 (-8.7787e-01)\n",
            "Epoch: [196][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7836e-01 (-8.7843e-01)\n",
            "Epoch: [196][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8625e-01 (-8.7858e-01)\n",
            "Training...\n",
            "Epoch: [197][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.7246e-01 (-8.7246e-01)\n",
            "Epoch: [197][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9072e-01 (-8.7593e-01)\n",
            "Epoch: [197][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8367e-01 (-8.7963e-01)\n",
            "Epoch: [197][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8302e-01 (-8.7971e-01)\n",
            "Epoch: [197][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7900e-01 (-8.7884e-01)\n",
            "Epoch: [197][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8750e-01 (-8.7839e-01)\n",
            "Epoch: [197][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7359e-01 (-8.7827e-01)\n",
            "Epoch: [197][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7933e-01 (-8.7844e-01)\n",
            "Epoch: [197][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9101e-01 (-8.7847e-01)\n",
            "Epoch: [197][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7595e-01 (-8.7884e-01)\n",
            "Training...\n",
            "Epoch: [198][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8680e-01 (-8.8680e-01)\n",
            "Epoch: [198][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8132e-01 (-8.8562e-01)\n",
            "Epoch: [198][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9629e-01 (-8.8392e-01)\n",
            "Epoch: [198][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8338e-01 (-8.8284e-01)\n",
            "Epoch: [198][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9171e-01 (-8.8230e-01)\n",
            "Epoch: [198][50/97]\tTime  0.179 ( 0.183)\tLoss -8.7719e-01 (-8.8154e-01)\n",
            "Epoch: [198][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7941e-01 (-8.8199e-01)\n",
            "Epoch: [198][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7560e-01 (-8.8143e-01)\n",
            "Epoch: [198][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6692e-01 (-8.8031e-01)\n",
            "Epoch: [198][90/97]\tTime  0.178 ( 0.181)\tLoss -8.5978e-01 (-8.7960e-01)\n",
            "Training...\n",
            "Epoch: [199][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.8234e-01 (-8.8234e-01)\n",
            "Epoch: [199][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8482e-01 (-8.8866e-01)\n",
            "Epoch: [199][20/97]\tTime  0.177 ( 0.191)\tLoss -8.6998e-01 (-8.8382e-01)\n",
            "Epoch: [199][30/97]\tTime  0.177 ( 0.187)\tLoss -8.6460e-01 (-8.7909e-01)\n",
            "Epoch: [199][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8147e-01 (-8.7807e-01)\n",
            "Epoch: [199][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8993e-01 (-8.7872e-01)\n",
            "Epoch: [199][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8222e-01 (-8.8006e-01)\n",
            "Epoch: [199][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8873e-01 (-8.8052e-01)\n",
            "Epoch: [199][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8315e-01 (-8.8200e-01)\n",
            "Epoch: [199][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7752e-01 (-8.8218e-01)\n",
            "Training...\n",
            "Epoch: [200][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9371e-01 (-8.9371e-01)\n",
            "Epoch: [200][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7235e-01 (-8.8055e-01)\n",
            "Epoch: [200][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8820e-01 (-8.8080e-01)\n",
            "Epoch: [200][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8113e-01 (-8.8098e-01)\n",
            "Epoch: [200][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9075e-01 (-8.7970e-01)\n",
            "Epoch: [200][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7636e-01 (-8.7955e-01)\n",
            "Epoch: [200][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7427e-01 (-8.7972e-01)\n",
            "Epoch: [200][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8160e-01 (-8.8013e-01)\n",
            "Epoch: [200][80/97]\tTime  0.177 ( 0.181)\tLoss -8.6027e-01 (-8.7978e-01)\n",
            "Epoch: [200][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7021e-01 (-8.7988e-01)\n",
            "Validating...\n",
            "Top1: 0.7602796052631579\n",
            "Saving the best model!\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [201][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.7921e-01 (-8.7921e-01)\n",
            "Epoch: [201][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7760e-01 (-8.7871e-01)\n",
            "Epoch: [201][20/97]\tTime  0.179 ( 0.191)\tLoss -8.8232e-01 (-8.7896e-01)\n",
            "Epoch: [201][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7572e-01 (-8.7671e-01)\n",
            "Epoch: [201][40/97]\tTime  0.178 ( 0.185)\tLoss -8.8597e-01 (-8.7780e-01)\n",
            "Epoch: [201][50/97]\tTime  0.179 ( 0.184)\tLoss -8.7509e-01 (-8.7854e-01)\n",
            "Epoch: [201][60/97]\tTime  0.179 ( 0.183)\tLoss -8.7427e-01 (-8.7840e-01)\n",
            "Epoch: [201][70/97]\tTime  0.178 ( 0.182)\tLoss -8.6926e-01 (-8.7774e-01)\n",
            "Epoch: [201][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7837e-01 (-8.7718e-01)\n",
            "Epoch: [201][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8609e-01 (-8.7773e-01)\n",
            "Training...\n",
            "Epoch: [202][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9169e-01 (-8.9169e-01)\n",
            "Epoch: [202][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7078e-01 (-8.8427e-01)\n",
            "Epoch: [202][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8248e-01 (-8.8401e-01)\n",
            "Epoch: [202][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7852e-01 (-8.8228e-01)\n",
            "Epoch: [202][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7886e-01 (-8.8081e-01)\n",
            "Epoch: [202][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7632e-01 (-8.8104e-01)\n",
            "Epoch: [202][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7986e-01 (-8.8104e-01)\n",
            "Epoch: [202][70/97]\tTime  0.177 ( 0.182)\tLoss -8.7118e-01 (-8.8146e-01)\n",
            "Epoch: [202][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8529e-01 (-8.8199e-01)\n",
            "Epoch: [202][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7665e-01 (-8.8162e-01)\n",
            "Training...\n",
            "Epoch: [203][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.9210e-01 (-8.9210e-01)\n",
            "Epoch: [203][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7349e-01 (-8.7774e-01)\n",
            "Epoch: [203][20/97]\tTime  0.178 ( 0.191)\tLoss -8.6893e-01 (-8.7548e-01)\n",
            "Epoch: [203][30/97]\tTime  0.177 ( 0.187)\tLoss -8.7952e-01 (-8.7618e-01)\n",
            "Epoch: [203][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7957e-01 (-8.7663e-01)\n",
            "Epoch: [203][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7894e-01 (-8.7647e-01)\n",
            "Epoch: [203][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6334e-01 (-8.7583e-01)\n",
            "Epoch: [203][70/97]\tTime  0.177 ( 0.182)\tLoss -8.6752e-01 (-8.7576e-01)\n",
            "Epoch: [203][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7150e-01 (-8.7519e-01)\n",
            "Epoch: [203][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9629e-01 (-8.7626e-01)\n",
            "Training...\n",
            "Epoch: [204][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9487e-01 (-8.9487e-01)\n",
            "Epoch: [204][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9757e-01 (-8.8999e-01)\n",
            "Epoch: [204][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8219e-01 (-8.8890e-01)\n",
            "Epoch: [204][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7635e-01 (-8.8913e-01)\n",
            "Epoch: [204][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7915e-01 (-8.8749e-01)\n",
            "Epoch: [204][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8575e-01 (-8.8568e-01)\n",
            "Epoch: [204][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7775e-01 (-8.8441e-01)\n",
            "Epoch: [204][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6610e-01 (-8.8315e-01)\n",
            "Epoch: [204][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7770e-01 (-8.8215e-01)\n",
            "Epoch: [204][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8544e-01 (-8.8145e-01)\n",
            "Training...\n",
            "Epoch: [205][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7850e-01 (-8.7850e-01)\n",
            "Epoch: [205][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8204e-01 (-8.7626e-01)\n",
            "Epoch: [205][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0250e-01 (-8.7676e-01)\n",
            "Epoch: [205][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7710e-01 (-8.7827e-01)\n",
            "Epoch: [205][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7113e-01 (-8.7805e-01)\n",
            "Epoch: [205][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6179e-01 (-8.7797e-01)\n",
            "Epoch: [205][60/97]\tTime  0.178 ( 0.182)\tLoss -8.6948e-01 (-8.7724e-01)\n",
            "Epoch: [205][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7700e-01 (-8.7752e-01)\n",
            "Epoch: [205][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8691e-01 (-8.7867e-01)\n",
            "Epoch: [205][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7565e-01 (-8.7887e-01)\n",
            "Validating...\n",
            "Top1: 0.7701480263157895\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [206][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8136e-01 (-8.8136e-01)\n",
            "Epoch: [206][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8403e-01 (-8.8442e-01)\n",
            "Epoch: [206][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7887e-01 (-8.8414e-01)\n",
            "Epoch: [206][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7604e-01 (-8.8274e-01)\n",
            "Epoch: [206][40/97]\tTime  0.179 ( 0.185)\tLoss -8.7932e-01 (-8.8217e-01)\n",
            "Epoch: [206][50/97]\tTime  0.178 ( 0.184)\tLoss -8.9175e-01 (-8.8148e-01)\n",
            "Epoch: [206][60/97]\tTime  0.178 ( 0.183)\tLoss -8.7522e-01 (-8.8157e-01)\n",
            "Epoch: [206][70/97]\tTime  0.179 ( 0.182)\tLoss -8.8110e-01 (-8.8123e-01)\n",
            "Epoch: [206][80/97]\tTime  0.178 ( 0.182)\tLoss -8.9724e-01 (-8.8180e-01)\n",
            "Epoch: [206][90/97]\tTime  0.179 ( 0.181)\tLoss -8.6671e-01 (-8.8181e-01)\n",
            "Training...\n",
            "Epoch: [207][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8798e-01 (-8.8798e-01)\n",
            "Epoch: [207][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8799e-01 (-8.8312e-01)\n",
            "Epoch: [207][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7230e-01 (-8.8234e-01)\n",
            "Epoch: [207][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7472e-01 (-8.8017e-01)\n",
            "Epoch: [207][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8249e-01 (-8.8011e-01)\n",
            "Epoch: [207][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8630e-01 (-8.8037e-01)\n",
            "Epoch: [207][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7972e-01 (-8.7950e-01)\n",
            "Epoch: [207][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8457e-01 (-8.7913e-01)\n",
            "Epoch: [207][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8540e-01 (-8.7967e-01)\n",
            "Epoch: [207][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9920e-01 (-8.8067e-01)\n",
            "Training...\n",
            "Epoch: [208][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.9017e-01 (-8.9017e-01)\n",
            "Epoch: [208][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8954e-01 (-8.8254e-01)\n",
            "Epoch: [208][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7667e-01 (-8.8125e-01)\n",
            "Epoch: [208][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8268e-01 (-8.8050e-01)\n",
            "Epoch: [208][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6356e-01 (-8.8028e-01)\n",
            "Epoch: [208][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8944e-01 (-8.8002e-01)\n",
            "Epoch: [208][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7749e-01 (-8.7990e-01)\n",
            "Epoch: [208][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8091e-01 (-8.7962e-01)\n",
            "Epoch: [208][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8739e-01 (-8.7929e-01)\n",
            "Epoch: [208][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8172e-01 (-8.7979e-01)\n",
            "Training...\n",
            "Epoch: [209][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8078e-01 (-8.8078e-01)\n",
            "Epoch: [209][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7576e-01 (-8.7858e-01)\n",
            "Epoch: [209][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8988e-01 (-8.7920e-01)\n",
            "Epoch: [209][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7511e-01 (-8.7882e-01)\n",
            "Epoch: [209][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8593e-01 (-8.7924e-01)\n",
            "Epoch: [209][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8100e-01 (-8.7939e-01)\n",
            "Epoch: [209][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9613e-01 (-8.8095e-01)\n",
            "Epoch: [209][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8721e-01 (-8.8138e-01)\n",
            "Epoch: [209][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8723e-01 (-8.8126e-01)\n",
            "Epoch: [209][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7812e-01 (-8.8061e-01)\n",
            "Training...\n",
            "Epoch: [210][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.8580e-01 (-8.8580e-01)\n",
            "Epoch: [210][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8255e-01 (-8.8409e-01)\n",
            "Epoch: [210][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8403e-01 (-8.8183e-01)\n",
            "Epoch: [210][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7829e-01 (-8.8119e-01)\n",
            "Epoch: [210][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7380e-01 (-8.8083e-01)\n",
            "Epoch: [210][50/97]\tTime  0.177 ( 0.183)\tLoss -8.6295e-01 (-8.8032e-01)\n",
            "Epoch: [210][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8002e-01 (-8.7994e-01)\n",
            "Epoch: [210][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8151e-01 (-8.7995e-01)\n",
            "Epoch: [210][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7788e-01 (-8.8038e-01)\n",
            "Epoch: [210][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7924e-01 (-8.8027e-01)\n",
            "Validating...\n",
            "Top1: 0.7648026315789473\n",
            "Training...\n",
            "Epoch: [211][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.7807e-01 (-8.7807e-01)\n",
            "Epoch: [211][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8915e-01 (-8.7829e-01)\n",
            "Epoch: [211][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7994e-01 (-8.7940e-01)\n",
            "Epoch: [211][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7351e-01 (-8.7959e-01)\n",
            "Epoch: [211][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8969e-01 (-8.7872e-01)\n",
            "Epoch: [211][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7800e-01 (-8.7808e-01)\n",
            "Epoch: [211][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8498e-01 (-8.7867e-01)\n",
            "Epoch: [211][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9637e-01 (-8.7946e-01)\n",
            "Epoch: [211][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8898e-01 (-8.7935e-01)\n",
            "Epoch: [211][90/97]\tTime  0.177 ( 0.181)\tLoss -8.7317e-01 (-8.7900e-01)\n",
            "Training...\n",
            "Epoch: [212][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8025e-01 (-8.8025e-01)\n",
            "Epoch: [212][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7441e-01 (-8.8214e-01)\n",
            "Epoch: [212][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7615e-01 (-8.8126e-01)\n",
            "Epoch: [212][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8206e-01 (-8.8218e-01)\n",
            "Epoch: [212][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7949e-01 (-8.8250e-01)\n",
            "Epoch: [212][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7650e-01 (-8.8279e-01)\n",
            "Epoch: [212][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7237e-01 (-8.8245e-01)\n",
            "Epoch: [212][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6674e-01 (-8.8173e-01)\n",
            "Epoch: [212][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7245e-01 (-8.8143e-01)\n",
            "Epoch: [212][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7688e-01 (-8.8150e-01)\n",
            "Training...\n",
            "Epoch: [213][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.9097e-01 (-8.9097e-01)\n",
            "Epoch: [213][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7912e-01 (-8.7982e-01)\n",
            "Epoch: [213][20/97]\tTime  0.178 ( 0.191)\tLoss -8.6952e-01 (-8.7842e-01)\n",
            "Epoch: [213][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8452e-01 (-8.7821e-01)\n",
            "Epoch: [213][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8574e-01 (-8.7910e-01)\n",
            "Epoch: [213][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8084e-01 (-8.7883e-01)\n",
            "Epoch: [213][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7783e-01 (-8.7882e-01)\n",
            "Epoch: [213][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7876e-01 (-8.7844e-01)\n",
            "Epoch: [213][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7405e-01 (-8.7853e-01)\n",
            "Epoch: [213][90/97]\tTime  0.177 ( 0.181)\tLoss -8.6358e-01 (-8.7841e-01)\n",
            "Training...\n",
            "Epoch: [214][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.6921e-01 (-8.6921e-01)\n",
            "Epoch: [214][10/97]\tTime  0.178 ( 0.203)\tLoss -8.8615e-01 (-8.8154e-01)\n",
            "Epoch: [214][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9232e-01 (-8.8246e-01)\n",
            "Epoch: [214][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7211e-01 (-8.8141e-01)\n",
            "Epoch: [214][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7761e-01 (-8.8122e-01)\n",
            "Epoch: [214][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7530e-01 (-8.8083e-01)\n",
            "Epoch: [214][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7325e-01 (-8.8024e-01)\n",
            "Epoch: [214][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8512e-01 (-8.8005e-01)\n",
            "Epoch: [214][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8557e-01 (-8.8040e-01)\n",
            "Epoch: [214][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7145e-01 (-8.8008e-01)\n",
            "Training...\n",
            "Epoch: [215][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8855e-01 (-8.8855e-01)\n",
            "Epoch: [215][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7361e-01 (-8.8641e-01)\n",
            "Epoch: [215][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8574e-01 (-8.8755e-01)\n",
            "Epoch: [215][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8620e-01 (-8.8701e-01)\n",
            "Epoch: [215][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9390e-01 (-8.8823e-01)\n",
            "Epoch: [215][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8403e-01 (-8.8781e-01)\n",
            "Epoch: [215][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8368e-01 (-8.8727e-01)\n",
            "Epoch: [215][70/97]\tTime  0.178 ( 0.181)\tLoss -8.7920e-01 (-8.8688e-01)\n",
            "Epoch: [215][80/97]\tTime  0.178 ( 0.181)\tLoss -8.6996e-01 (-8.8598e-01)\n",
            "Epoch: [215][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6749e-01 (-8.8523e-01)\n",
            "Validating...\n",
            "Top1: 0.7733347039473685\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [216][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.8999e-01 (-8.8999e-01)\n",
            "Epoch: [216][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7856e-01 (-8.8270e-01)\n",
            "Epoch: [216][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7988e-01 (-8.8141e-01)\n",
            "Epoch: [216][30/97]\tTime  0.178 ( 0.187)\tLoss -8.7521e-01 (-8.8063e-01)\n",
            "Epoch: [216][40/97]\tTime  0.179 ( 0.185)\tLoss -8.8419e-01 (-8.8065e-01)\n",
            "Epoch: [216][50/97]\tTime  0.179 ( 0.184)\tLoss -8.8355e-01 (-8.8085e-01)\n",
            "Epoch: [216][60/97]\tTime  0.178 ( 0.183)\tLoss -8.7707e-01 (-8.8094e-01)\n",
            "Epoch: [216][70/97]\tTime  0.178 ( 0.182)\tLoss -8.7116e-01 (-8.8080e-01)\n",
            "Epoch: [216][80/97]\tTime  0.178 ( 0.182)\tLoss -8.8387e-01 (-8.8095e-01)\n",
            "Epoch: [216][90/97]\tTime  0.179 ( 0.181)\tLoss -8.8589e-01 (-8.8033e-01)\n",
            "Training...\n",
            "Epoch: [217][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8818e-01 (-8.8818e-01)\n",
            "Epoch: [217][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7795e-01 (-8.7706e-01)\n",
            "Epoch: [217][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8142e-01 (-8.7832e-01)\n",
            "Epoch: [217][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8421e-01 (-8.8085e-01)\n",
            "Epoch: [217][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9289e-01 (-8.8368e-01)\n",
            "Epoch: [217][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9572e-01 (-8.8537e-01)\n",
            "Epoch: [217][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8182e-01 (-8.8542e-01)\n",
            "Epoch: [217][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8860e-01 (-8.8513e-01)\n",
            "Epoch: [217][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8718e-01 (-8.8493e-01)\n",
            "Epoch: [217][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7803e-01 (-8.8469e-01)\n",
            "Training...\n",
            "Epoch: [218][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.7583e-01 (-8.7583e-01)\n",
            "Epoch: [218][10/97]\tTime  0.177 ( 0.203)\tLoss -8.7136e-01 (-8.7883e-01)\n",
            "Epoch: [218][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8028e-01 (-8.7861e-01)\n",
            "Epoch: [218][30/97]\tTime  0.177 ( 0.187)\tLoss -8.7064e-01 (-8.7858e-01)\n",
            "Epoch: [218][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7875e-01 (-8.7971e-01)\n",
            "Epoch: [218][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8280e-01 (-8.8001e-01)\n",
            "Epoch: [218][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7373e-01 (-8.7948e-01)\n",
            "Epoch: [218][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7713e-01 (-8.7993e-01)\n",
            "Epoch: [218][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9254e-01 (-8.8105e-01)\n",
            "Epoch: [218][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8750e-01 (-8.8140e-01)\n",
            "Training...\n",
            "Epoch: [219][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8806e-01 (-8.8806e-01)\n",
            "Epoch: [219][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8987e-01 (-8.8632e-01)\n",
            "Epoch: [219][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8643e-01 (-8.8613e-01)\n",
            "Epoch: [219][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8295e-01 (-8.8555e-01)\n",
            "Epoch: [219][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7402e-01 (-8.8435e-01)\n",
            "Epoch: [219][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7521e-01 (-8.8253e-01)\n",
            "Epoch: [219][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8952e-01 (-8.8215e-01)\n",
            "Epoch: [219][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7843e-01 (-8.8217e-01)\n",
            "Epoch: [219][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7302e-01 (-8.8141e-01)\n",
            "Epoch: [219][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7896e-01 (-8.8165e-01)\n",
            "Training...\n",
            "Epoch: [220][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.7803e-01 (-8.7803e-01)\n",
            "Epoch: [220][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9131e-01 (-8.8099e-01)\n",
            "Epoch: [220][20/97]\tTime  0.177 ( 0.191)\tLoss -8.6898e-01 (-8.7960e-01)\n",
            "Epoch: [220][30/97]\tTime  0.178 ( 0.186)\tLoss -8.6921e-01 (-8.7890e-01)\n",
            "Epoch: [220][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8547e-01 (-8.7949e-01)\n",
            "Epoch: [220][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8639e-01 (-8.8071e-01)\n",
            "Epoch: [220][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8880e-01 (-8.8165e-01)\n",
            "Epoch: [220][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8336e-01 (-8.8189e-01)\n",
            "Epoch: [220][80/97]\tTime  0.179 ( 0.181)\tLoss -8.7909e-01 (-8.8177e-01)\n",
            "Epoch: [220][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7653e-01 (-8.8132e-01)\n",
            "Validating...\n",
            "Top1: 0.7772409539473685\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [221][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9656e-01 (-8.9656e-01)\n",
            "Epoch: [221][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7831e-01 (-8.8255e-01)\n",
            "Epoch: [221][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8130e-01 (-8.8137e-01)\n",
            "Epoch: [221][30/97]\tTime  0.178 ( 0.187)\tLoss -8.8071e-01 (-8.8109e-01)\n",
            "Epoch: [221][40/97]\tTime  0.179 ( 0.185)\tLoss -8.7367e-01 (-8.8145e-01)\n",
            "Epoch: [221][50/97]\tTime  0.178 ( 0.184)\tLoss -8.8271e-01 (-8.8128e-01)\n",
            "Epoch: [221][60/97]\tTime  0.178 ( 0.183)\tLoss -8.9624e-01 (-8.8231e-01)\n",
            "Epoch: [221][70/97]\tTime  0.178 ( 0.182)\tLoss -8.8437e-01 (-8.8225e-01)\n",
            "Epoch: [221][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7987e-01 (-8.8276e-01)\n",
            "Epoch: [221][90/97]\tTime  0.179 ( 0.181)\tLoss -8.7975e-01 (-8.8299e-01)\n",
            "Training...\n",
            "Epoch: [222][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9489e-01 (-8.9489e-01)\n",
            "Epoch: [222][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8801e-01 (-8.8051e-01)\n",
            "Epoch: [222][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8133e-01 (-8.8153e-01)\n",
            "Epoch: [222][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8854e-01 (-8.8279e-01)\n",
            "Epoch: [222][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6624e-01 (-8.8174e-01)\n",
            "Epoch: [222][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7191e-01 (-8.8138e-01)\n",
            "Epoch: [222][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7792e-01 (-8.8081e-01)\n",
            "Epoch: [222][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9422e-01 (-8.8143e-01)\n",
            "Epoch: [222][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9367e-01 (-8.8282e-01)\n",
            "Epoch: [222][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8819e-01 (-8.8328e-01)\n",
            "Training...\n",
            "Epoch: [223][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.8289e-01 (-8.8289e-01)\n",
            "Epoch: [223][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8838e-01 (-8.9166e-01)\n",
            "Epoch: [223][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8687e-01 (-8.8905e-01)\n",
            "Epoch: [223][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7691e-01 (-8.8675e-01)\n",
            "Epoch: [223][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7775e-01 (-8.8606e-01)\n",
            "Epoch: [223][50/97]\tTime  0.178 ( 0.183)\tLoss -8.6837e-01 (-8.8489e-01)\n",
            "Epoch: [223][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7586e-01 (-8.8398e-01)\n",
            "Epoch: [223][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6823e-01 (-8.8390e-01)\n",
            "Epoch: [223][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7061e-01 (-8.8297e-01)\n",
            "Epoch: [223][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6971e-01 (-8.8248e-01)\n",
            "Training...\n",
            "Epoch: [224][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.7875e-01 (-8.7875e-01)\n",
            "Epoch: [224][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8353e-01 (-8.7817e-01)\n",
            "Epoch: [224][20/97]\tTime  0.178 ( 0.190)\tLoss -8.7612e-01 (-8.7941e-01)\n",
            "Epoch: [224][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8939e-01 (-8.7892e-01)\n",
            "Epoch: [224][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8933e-01 (-8.8033e-01)\n",
            "Epoch: [224][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8042e-01 (-8.8090e-01)\n",
            "Epoch: [224][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8615e-01 (-8.8199e-01)\n",
            "Epoch: [224][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8918e-01 (-8.8240e-01)\n",
            "Epoch: [224][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7836e-01 (-8.8240e-01)\n",
            "Epoch: [224][90/97]\tTime  0.178 ( 0.181)\tLoss -8.9244e-01 (-8.8249e-01)\n",
            "Training...\n",
            "Epoch: [225][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.7650e-01 (-8.7650e-01)\n",
            "Epoch: [225][10/97]\tTime  0.177 ( 0.203)\tLoss -8.7561e-01 (-8.8585e-01)\n",
            "Epoch: [225][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7972e-01 (-8.8434e-01)\n",
            "Epoch: [225][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8892e-01 (-8.8416e-01)\n",
            "Epoch: [225][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9529e-01 (-8.8471e-01)\n",
            "Epoch: [225][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7864e-01 (-8.8415e-01)\n",
            "Epoch: [225][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8705e-01 (-8.8448e-01)\n",
            "Epoch: [225][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8447e-01 (-8.8416e-01)\n",
            "Epoch: [225][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7802e-01 (-8.8381e-01)\n",
            "Epoch: [225][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8342e-01 (-8.8299e-01)\n",
            "Validating...\n",
            "Top1: 0.7733347039473685\n",
            "Training...\n",
            "Epoch: [226][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8042e-01 (-8.8042e-01)\n",
            "Epoch: [226][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8825e-01 (-8.8769e-01)\n",
            "Epoch: [226][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9259e-01 (-8.8999e-01)\n",
            "Epoch: [226][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8468e-01 (-8.8958e-01)\n",
            "Epoch: [226][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8355e-01 (-8.8863e-01)\n",
            "Epoch: [226][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7848e-01 (-8.8701e-01)\n",
            "Epoch: [226][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6782e-01 (-8.8559e-01)\n",
            "Epoch: [226][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8587e-01 (-8.8498e-01)\n",
            "Epoch: [226][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8388e-01 (-8.8464e-01)\n",
            "Epoch: [226][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8809e-01 (-8.8462e-01)\n",
            "Training...\n",
            "Epoch: [227][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.7895e-01 (-8.7895e-01)\n",
            "Epoch: [227][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8895e-01 (-8.9139e-01)\n",
            "Epoch: [227][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9596e-01 (-8.9210e-01)\n",
            "Epoch: [227][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9725e-01 (-8.9233e-01)\n",
            "Epoch: [227][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0250e-01 (-8.9290e-01)\n",
            "Epoch: [227][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9451e-01 (-8.9263e-01)\n",
            "Epoch: [227][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9680e-01 (-8.9237e-01)\n",
            "Epoch: [227][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8639e-01 (-8.9232e-01)\n",
            "Epoch: [227][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8322e-01 (-8.9137e-01)\n",
            "Epoch: [227][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7266e-01 (-8.9043e-01)\n",
            "Training...\n",
            "Epoch: [228][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7181e-01 (-8.7181e-01)\n",
            "Epoch: [228][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9382e-01 (-8.7673e-01)\n",
            "Epoch: [228][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8576e-01 (-8.8107e-01)\n",
            "Epoch: [228][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8245e-01 (-8.8269e-01)\n",
            "Epoch: [228][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7209e-01 (-8.8394e-01)\n",
            "Epoch: [228][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9131e-01 (-8.8398e-01)\n",
            "Epoch: [228][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8612e-01 (-8.8388e-01)\n",
            "Epoch: [228][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6589e-01 (-8.8358e-01)\n",
            "Epoch: [228][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9301e-01 (-8.8341e-01)\n",
            "Epoch: [228][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9152e-01 (-8.8365e-01)\n",
            "Training...\n",
            "Epoch: [229][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.7590e-01 (-8.7590e-01)\n",
            "Epoch: [229][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8712e-01 (-8.8327e-01)\n",
            "Epoch: [229][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8288e-01 (-8.8363e-01)\n",
            "Epoch: [229][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8108e-01 (-8.8259e-01)\n",
            "Epoch: [229][40/97]\tTime  0.178 ( 0.184)\tLoss -8.6721e-01 (-8.8260e-01)\n",
            "Epoch: [229][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8581e-01 (-8.8168e-01)\n",
            "Epoch: [229][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7622e-01 (-8.8146e-01)\n",
            "Epoch: [229][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8619e-01 (-8.8163e-01)\n",
            "Epoch: [229][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0006e-01 (-8.8224e-01)\n",
            "Epoch: [229][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7401e-01 (-8.8216e-01)\n",
            "Training...\n",
            "Epoch: [230][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8872e-01 (-8.8872e-01)\n",
            "Epoch: [230][10/97]\tTime  0.178 ( 0.203)\tLoss -8.9436e-01 (-8.8810e-01)\n",
            "Epoch: [230][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8589e-01 (-8.8820e-01)\n",
            "Epoch: [230][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8381e-01 (-8.8804e-01)\n",
            "Epoch: [230][40/97]\tTime  0.178 ( 0.184)\tLoss -8.7664e-01 (-8.8724e-01)\n",
            "Epoch: [230][50/97]\tTime  0.178 ( 0.183)\tLoss -8.7599e-01 (-8.8710e-01)\n",
            "Epoch: [230][60/97]\tTime  0.178 ( 0.182)\tLoss -8.7188e-01 (-8.8597e-01)\n",
            "Epoch: [230][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8289e-01 (-8.8547e-01)\n",
            "Epoch: [230][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7085e-01 (-8.8508e-01)\n",
            "Epoch: [230][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7317e-01 (-8.8429e-01)\n",
            "Validating...\n",
            "Top1: 0.7691200657894737\n",
            "Training...\n",
            "Epoch: [231][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0196e-01 (-9.0196e-01)\n",
            "Epoch: [231][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7153e-01 (-8.8319e-01)\n",
            "Epoch: [231][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8306e-01 (-8.8376e-01)\n",
            "Epoch: [231][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8244e-01 (-8.8300e-01)\n",
            "Epoch: [231][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8412e-01 (-8.8398e-01)\n",
            "Epoch: [231][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0695e-01 (-8.8386e-01)\n",
            "Epoch: [231][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9553e-01 (-8.8472e-01)\n",
            "Epoch: [231][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9223e-01 (-8.8496e-01)\n",
            "Epoch: [231][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7638e-01 (-8.8551e-01)\n",
            "Epoch: [231][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8779e-01 (-8.8544e-01)\n",
            "Training...\n",
            "Epoch: [232][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8973e-01 (-8.8973e-01)\n",
            "Epoch: [232][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8730e-01 (-8.9256e-01)\n",
            "Epoch: [232][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7739e-01 (-8.8945e-01)\n",
            "Epoch: [232][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8172e-01 (-8.8686e-01)\n",
            "Epoch: [232][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6882e-01 (-8.8628e-01)\n",
            "Epoch: [232][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8303e-01 (-8.8500e-01)\n",
            "Epoch: [232][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8011e-01 (-8.8482e-01)\n",
            "Epoch: [232][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8614e-01 (-8.8413e-01)\n",
            "Epoch: [232][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8714e-01 (-8.8369e-01)\n",
            "Epoch: [232][90/97]\tTime  0.178 ( 0.180)\tLoss -8.7660e-01 (-8.8361e-01)\n",
            "Training...\n",
            "Epoch: [233][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8618e-01 (-8.8618e-01)\n",
            "Epoch: [233][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9394e-01 (-8.8996e-01)\n",
            "Epoch: [233][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7943e-01 (-8.8878e-01)\n",
            "Epoch: [233][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8433e-01 (-8.8789e-01)\n",
            "Epoch: [233][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8591e-01 (-8.8719e-01)\n",
            "Epoch: [233][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8986e-01 (-8.8681e-01)\n",
            "Epoch: [233][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9264e-01 (-8.8653e-01)\n",
            "Epoch: [233][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9357e-01 (-8.8665e-01)\n",
            "Epoch: [233][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8072e-01 (-8.8667e-01)\n",
            "Epoch: [233][90/97]\tTime  0.178 ( 0.181)\tLoss -8.8747e-01 (-8.8638e-01)\n",
            "Training...\n",
            "Epoch: [234][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8377e-01 (-8.8377e-01)\n",
            "Epoch: [234][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7923e-01 (-8.8274e-01)\n",
            "Epoch: [234][20/97]\tTime  0.178 ( 0.191)\tLoss -8.8399e-01 (-8.8521e-01)\n",
            "Epoch: [234][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8863e-01 (-8.8600e-01)\n",
            "Epoch: [234][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9312e-01 (-8.8725e-01)\n",
            "Epoch: [234][50/97]\tTime  0.178 ( 0.183)\tLoss -8.9375e-01 (-8.8834e-01)\n",
            "Epoch: [234][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8207e-01 (-8.8812e-01)\n",
            "Epoch: [234][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8847e-01 (-8.8756e-01)\n",
            "Epoch: [234][80/97]\tTime  0.178 ( 0.181)\tLoss -8.7728e-01 (-8.8727e-01)\n",
            "Epoch: [234][90/97]\tTime  0.177 ( 0.181)\tLoss -9.0154e-01 (-8.8726e-01)\n",
            "Training...\n",
            "Epoch: [235][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.8235e-01 (-8.8235e-01)\n",
            "Epoch: [235][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8682e-01 (-8.8386e-01)\n",
            "Epoch: [235][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8331e-01 (-8.8367e-01)\n",
            "Epoch: [235][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8776e-01 (-8.8263e-01)\n",
            "Epoch: [235][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7599e-01 (-8.8305e-01)\n",
            "Epoch: [235][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8222e-01 (-8.8318e-01)\n",
            "Epoch: [235][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8835e-01 (-8.8388e-01)\n",
            "Epoch: [235][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9683e-01 (-8.8439e-01)\n",
            "Epoch: [235][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9275e-01 (-8.8480e-01)\n",
            "Epoch: [235][90/97]\tTime  0.178 ( 0.180)\tLoss -8.6966e-01 (-8.8480e-01)\n",
            "Validating...\n",
            "Top1: 0.7768297697368421\n",
            "Training...\n",
            "Epoch: [236][ 0/97]\tTime  0.465 ( 0.465)\tLoss -8.7072e-01 (-8.7072e-01)\n",
            "Epoch: [236][10/97]\tTime  0.178 ( 0.203)\tLoss -8.7900e-01 (-8.8300e-01)\n",
            "Epoch: [236][20/97]\tTime  0.177 ( 0.191)\tLoss -8.7784e-01 (-8.8185e-01)\n",
            "Epoch: [236][30/97]\tTime  0.177 ( 0.187)\tLoss -8.8378e-01 (-8.8206e-01)\n",
            "Epoch: [236][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8774e-01 (-8.8349e-01)\n",
            "Epoch: [236][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8004e-01 (-8.8312e-01)\n",
            "Epoch: [236][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8437e-01 (-8.8313e-01)\n",
            "Epoch: [236][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9125e-01 (-8.8412e-01)\n",
            "Epoch: [236][80/97]\tTime  0.177 ( 0.181)\tLoss -8.7412e-01 (-8.8451e-01)\n",
            "Epoch: [236][90/97]\tTime  0.178 ( 0.181)\tLoss -8.9043e-01 (-8.8474e-01)\n",
            "Training...\n",
            "Epoch: [237][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.7374e-01 (-8.7374e-01)\n",
            "Epoch: [237][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8698e-01 (-8.8351e-01)\n",
            "Epoch: [237][20/97]\tTime  0.178 ( 0.191)\tLoss -8.7380e-01 (-8.8117e-01)\n",
            "Epoch: [237][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8973e-01 (-8.8234e-01)\n",
            "Epoch: [237][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8602e-01 (-8.8279e-01)\n",
            "Epoch: [237][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8150e-01 (-8.8233e-01)\n",
            "Epoch: [237][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7704e-01 (-8.8242e-01)\n",
            "Epoch: [237][70/97]\tTime  0.179 ( 0.181)\tLoss -8.9071e-01 (-8.8266e-01)\n",
            "Epoch: [237][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9260e-01 (-8.8338e-01)\n",
            "Epoch: [237][90/97]\tTime  0.177 ( 0.181)\tLoss -8.8562e-01 (-8.8417e-01)\n",
            "Training...\n",
            "Epoch: [238][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9404e-01 (-8.9404e-01)\n",
            "Epoch: [238][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8315e-01 (-8.8995e-01)\n",
            "Epoch: [238][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7934e-01 (-8.8759e-01)\n",
            "Epoch: [238][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8246e-01 (-8.8626e-01)\n",
            "Epoch: [238][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8016e-01 (-8.8543e-01)\n",
            "Epoch: [238][50/97]\tTime  0.178 ( 0.183)\tLoss -8.8821e-01 (-8.8527e-01)\n",
            "Epoch: [238][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7609e-01 (-8.8475e-01)\n",
            "Epoch: [238][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9163e-01 (-8.8433e-01)\n",
            "Epoch: [238][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8272e-01 (-8.8382e-01)\n",
            "Epoch: [238][90/97]\tTime  0.178 ( 0.180)\tLoss -9.0117e-01 (-8.8460e-01)\n",
            "Training...\n",
            "Epoch: [239][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8025e-01 (-8.8025e-01)\n",
            "Epoch: [239][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9158e-01 (-8.8625e-01)\n",
            "Epoch: [239][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9433e-01 (-8.8499e-01)\n",
            "Epoch: [239][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7677e-01 (-8.8444e-01)\n",
            "Epoch: [239][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8074e-01 (-8.8422e-01)\n",
            "Epoch: [239][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7648e-01 (-8.8359e-01)\n",
            "Epoch: [239][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8489e-01 (-8.8414e-01)\n",
            "Epoch: [239][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8300e-01 (-8.8477e-01)\n",
            "Epoch: [239][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0006e-01 (-8.8450e-01)\n",
            "Epoch: [239][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7342e-01 (-8.8411e-01)\n",
            "Training...\n",
            "Epoch: [240][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7709e-01 (-8.7709e-01)\n",
            "Epoch: [240][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8919e-01 (-8.8211e-01)\n",
            "Epoch: [240][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8184e-01 (-8.8101e-01)\n",
            "Epoch: [240][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7887e-01 (-8.8137e-01)\n",
            "Epoch: [240][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9550e-01 (-8.8455e-01)\n",
            "Epoch: [240][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9879e-01 (-8.8570e-01)\n",
            "Epoch: [240][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9251e-01 (-8.8593e-01)\n",
            "Epoch: [240][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8452e-01 (-8.8540e-01)\n",
            "Epoch: [240][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8955e-01 (-8.8536e-01)\n",
            "Epoch: [240][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7986e-01 (-8.8528e-01)\n",
            "Validating...\n",
            "Top1: 0.77734375\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [241][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9365e-01 (-8.9365e-01)\n",
            "Epoch: [241][10/97]\tTime  0.180 ( 0.204)\tLoss -8.8244e-01 (-8.8275e-01)\n",
            "Epoch: [241][20/97]\tTime  0.178 ( 0.191)\tLoss -8.9627e-01 (-8.8273e-01)\n",
            "Epoch: [241][30/97]\tTime  0.178 ( 0.187)\tLoss -8.8103e-01 (-8.8275e-01)\n",
            "Epoch: [241][40/97]\tTime  0.178 ( 0.185)\tLoss -8.9285e-01 (-8.8379e-01)\n",
            "Epoch: [241][50/97]\tTime  0.179 ( 0.184)\tLoss -8.8534e-01 (-8.8435e-01)\n",
            "Epoch: [241][60/97]\tTime  0.178 ( 0.183)\tLoss -8.7872e-01 (-8.8382e-01)\n",
            "Epoch: [241][70/97]\tTime  0.179 ( 0.182)\tLoss -8.8381e-01 (-8.8376e-01)\n",
            "Epoch: [241][80/97]\tTime  0.178 ( 0.182)\tLoss -8.7684e-01 (-8.8413e-01)\n",
            "Epoch: [241][90/97]\tTime  0.178 ( 0.181)\tLoss -8.7347e-01 (-8.8413e-01)\n",
            "Training...\n",
            "Epoch: [242][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.7573e-01 (-8.7573e-01)\n",
            "Epoch: [242][10/97]\tTime  0.177 ( 0.201)\tLoss -8.7557e-01 (-8.8059e-01)\n",
            "Epoch: [242][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8059e-01 (-8.8309e-01)\n",
            "Epoch: [242][30/97]\tTime  0.177 ( 0.185)\tLoss -8.7328e-01 (-8.8346e-01)\n",
            "Epoch: [242][40/97]\tTime  0.177 ( 0.183)\tLoss -8.6800e-01 (-8.8279e-01)\n",
            "Epoch: [242][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9904e-01 (-8.8333e-01)\n",
            "Epoch: [242][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8721e-01 (-8.8345e-01)\n",
            "Epoch: [242][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9754e-01 (-8.8476e-01)\n",
            "Epoch: [242][80/97]\tTime  0.176 ( 0.180)\tLoss -8.8470e-01 (-8.8556e-01)\n",
            "Epoch: [242][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7922e-01 (-8.8586e-01)\n",
            "Training...\n",
            "Epoch: [243][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.7939e-01 (-8.7939e-01)\n",
            "Epoch: [243][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7894e-01 (-8.7937e-01)\n",
            "Epoch: [243][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7639e-01 (-8.8102e-01)\n",
            "Epoch: [243][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9401e-01 (-8.8166e-01)\n",
            "Epoch: [243][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7682e-01 (-8.8221e-01)\n",
            "Epoch: [243][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8108e-01 (-8.8201e-01)\n",
            "Epoch: [243][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0122e-01 (-8.8241e-01)\n",
            "Epoch: [243][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8101e-01 (-8.8235e-01)\n",
            "Epoch: [243][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8586e-01 (-8.8279e-01)\n",
            "Epoch: [243][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8823e-01 (-8.8296e-01)\n",
            "Training...\n",
            "Epoch: [244][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7318e-01 (-8.7318e-01)\n",
            "Epoch: [244][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7581e-01 (-8.7729e-01)\n",
            "Epoch: [244][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8743e-01 (-8.8037e-01)\n",
            "Epoch: [244][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7614e-01 (-8.8119e-01)\n",
            "Epoch: [244][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8413e-01 (-8.8225e-01)\n",
            "Epoch: [244][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7569e-01 (-8.8188e-01)\n",
            "Epoch: [244][60/97]\tTime  0.177 ( 0.181)\tLoss -8.7110e-01 (-8.8176e-01)\n",
            "Epoch: [244][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9500e-01 (-8.8208e-01)\n",
            "Epoch: [244][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8116e-01 (-8.8279e-01)\n",
            "Epoch: [244][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9520e-01 (-8.8262e-01)\n",
            "Training...\n",
            "Epoch: [245][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9238e-01 (-8.9238e-01)\n",
            "Epoch: [245][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7115e-01 (-8.8399e-01)\n",
            "Epoch: [245][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7949e-01 (-8.8481e-01)\n",
            "Epoch: [245][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7460e-01 (-8.8456e-01)\n",
            "Epoch: [245][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6994e-01 (-8.8360e-01)\n",
            "Epoch: [245][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9056e-01 (-8.8426e-01)\n",
            "Epoch: [245][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8023e-01 (-8.8521e-01)\n",
            "Epoch: [245][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8166e-01 (-8.8523e-01)\n",
            "Epoch: [245][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8458e-01 (-8.8478e-01)\n",
            "Epoch: [245][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9636e-01 (-8.8461e-01)\n",
            "Validating...\n",
            "Top1: 0.7872121710526315\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [246][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9587e-01 (-8.9587e-01)\n",
            "Epoch: [246][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8036e-01 (-8.8944e-01)\n",
            "Epoch: [246][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9311e-01 (-8.8686e-01)\n",
            "Epoch: [246][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7225e-01 (-8.8583e-01)\n",
            "Epoch: [246][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8090e-01 (-8.8477e-01)\n",
            "Epoch: [246][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8497e-01 (-8.8591e-01)\n",
            "Epoch: [246][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9250e-01 (-8.8671e-01)\n",
            "Epoch: [246][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9332e-01 (-8.8685e-01)\n",
            "Epoch: [246][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8782e-01 (-8.8695e-01)\n",
            "Epoch: [246][90/97]\tTime  0.176 ( 0.180)\tLoss -8.8127e-01 (-8.8699e-01)\n",
            "Training...\n",
            "Epoch: [247][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9960e-01 (-8.9960e-01)\n",
            "Epoch: [247][10/97]\tTime  0.176 ( 0.202)\tLoss -8.9790e-01 (-8.8870e-01)\n",
            "Epoch: [247][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9330e-01 (-8.8827e-01)\n",
            "Epoch: [247][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8486e-01 (-8.8633e-01)\n",
            "Epoch: [247][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7845e-01 (-8.8645e-01)\n",
            "Epoch: [247][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8267e-01 (-8.8623e-01)\n",
            "Epoch: [247][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9598e-01 (-8.8582e-01)\n",
            "Epoch: [247][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9617e-01 (-8.8626e-01)\n",
            "Epoch: [247][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8118e-01 (-8.8587e-01)\n",
            "Epoch: [247][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7639e-01 (-8.8559e-01)\n",
            "Training...\n",
            "Epoch: [248][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9742e-01 (-8.9742e-01)\n",
            "Epoch: [248][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8307e-01 (-8.8661e-01)\n",
            "Epoch: [248][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8128e-01 (-8.8581e-01)\n",
            "Epoch: [248][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8194e-01 (-8.8374e-01)\n",
            "Epoch: [248][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7032e-01 (-8.8272e-01)\n",
            "Epoch: [248][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7713e-01 (-8.8302e-01)\n",
            "Epoch: [248][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8983e-01 (-8.8346e-01)\n",
            "Epoch: [248][70/97]\tTime  0.176 ( 0.181)\tLoss -8.8986e-01 (-8.8336e-01)\n",
            "Epoch: [248][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7984e-01 (-8.8373e-01)\n",
            "Epoch: [248][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9027e-01 (-8.8341e-01)\n",
            "Training...\n",
            "Epoch: [249][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8384e-01 (-8.8384e-01)\n",
            "Epoch: [249][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8294e-01 (-8.8686e-01)\n",
            "Epoch: [249][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8684e-01 (-8.8636e-01)\n",
            "Epoch: [249][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8141e-01 (-8.8613e-01)\n",
            "Epoch: [249][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7485e-01 (-8.8624e-01)\n",
            "Epoch: [249][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7876e-01 (-8.8578e-01)\n",
            "Epoch: [249][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7979e-01 (-8.8595e-01)\n",
            "Epoch: [249][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8031e-01 (-8.8583e-01)\n",
            "Epoch: [249][80/97]\tTime  0.177 ( 0.180)\tLoss -8.6726e-01 (-8.8514e-01)\n",
            "Epoch: [249][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7579e-01 (-8.8440e-01)\n",
            "Training...\n",
            "Epoch: [250][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.9483e-01 (-8.9483e-01)\n",
            "Epoch: [250][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8932e-01 (-8.9034e-01)\n",
            "Epoch: [250][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9429e-01 (-8.8872e-01)\n",
            "Epoch: [250][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8582e-01 (-8.8848e-01)\n",
            "Epoch: [250][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8145e-01 (-8.8769e-01)\n",
            "Epoch: [250][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8471e-01 (-8.8621e-01)\n",
            "Epoch: [250][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9111e-01 (-8.8685e-01)\n",
            "Epoch: [250][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8488e-01 (-8.8724e-01)\n",
            "Epoch: [250][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8323e-01 (-8.8697e-01)\n",
            "Epoch: [250][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8793e-01 (-8.8668e-01)\n",
            "Validating...\n",
            "Top1: 0.776624177631579\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [251][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.7640e-01 (-8.7640e-01)\n",
            "Epoch: [251][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9561e-01 (-8.8642e-01)\n",
            "Epoch: [251][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8099e-01 (-8.8533e-01)\n",
            "Epoch: [251][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8578e-01 (-8.8595e-01)\n",
            "Epoch: [251][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9434e-01 (-8.8704e-01)\n",
            "Epoch: [251][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8884e-01 (-8.8716e-01)\n",
            "Epoch: [251][60/97]\tTime  0.177 ( 0.181)\tLoss -8.7748e-01 (-8.8698e-01)\n",
            "Epoch: [251][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8979e-01 (-8.8675e-01)\n",
            "Epoch: [251][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9291e-01 (-8.8643e-01)\n",
            "Epoch: [251][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9212e-01 (-8.8713e-01)\n",
            "Training...\n",
            "Epoch: [252][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.9074e-01 (-8.9074e-01)\n",
            "Epoch: [252][10/97]\tTime  0.176 ( 0.201)\tLoss -8.9733e-01 (-8.9389e-01)\n",
            "Epoch: [252][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8202e-01 (-8.9205e-01)\n",
            "Epoch: [252][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8778e-01 (-8.9130e-01)\n",
            "Epoch: [252][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8207e-01 (-8.8910e-01)\n",
            "Epoch: [252][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8604e-01 (-8.8796e-01)\n",
            "Epoch: [252][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8069e-01 (-8.8694e-01)\n",
            "Epoch: [252][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8490e-01 (-8.8637e-01)\n",
            "Epoch: [252][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9044e-01 (-8.8628e-01)\n",
            "Epoch: [252][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9832e-01 (-8.8702e-01)\n",
            "Training...\n",
            "Epoch: [253][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.8486e-01 (-8.8486e-01)\n",
            "Epoch: [253][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8483e-01 (-8.8742e-01)\n",
            "Epoch: [253][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9311e-01 (-8.8783e-01)\n",
            "Epoch: [253][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8665e-01 (-8.8681e-01)\n",
            "Epoch: [253][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8110e-01 (-8.8521e-01)\n",
            "Epoch: [253][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8861e-01 (-8.8398e-01)\n",
            "Epoch: [253][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8885e-01 (-8.8381e-01)\n",
            "Epoch: [253][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0798e-01 (-8.8355e-01)\n",
            "Epoch: [253][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0086e-01 (-8.8433e-01)\n",
            "Epoch: [253][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9350e-01 (-8.8493e-01)\n",
            "Training...\n",
            "Epoch: [254][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.7825e-01 (-8.7825e-01)\n",
            "Epoch: [254][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9100e-01 (-8.8595e-01)\n",
            "Epoch: [254][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8702e-01 (-8.8544e-01)\n",
            "Epoch: [254][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8259e-01 (-8.8426e-01)\n",
            "Epoch: [254][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0479e-01 (-8.8464e-01)\n",
            "Epoch: [254][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9466e-01 (-8.8639e-01)\n",
            "Epoch: [254][60/97]\tTime  0.178 ( 0.181)\tLoss -8.9023e-01 (-8.8705e-01)\n",
            "Epoch: [254][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9783e-01 (-8.8738e-01)\n",
            "Epoch: [254][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8018e-01 (-8.8688e-01)\n",
            "Epoch: [254][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8566e-01 (-8.8658e-01)\n",
            "Training...\n",
            "Epoch: [255][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8486e-01 (-8.8486e-01)\n",
            "Epoch: [255][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9233e-01 (-8.8462e-01)\n",
            "Epoch: [255][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8521e-01 (-8.8787e-01)\n",
            "Epoch: [255][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8887e-01 (-8.8786e-01)\n",
            "Epoch: [255][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9326e-01 (-8.8715e-01)\n",
            "Epoch: [255][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8034e-01 (-8.8723e-01)\n",
            "Epoch: [255][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9558e-01 (-8.8668e-01)\n",
            "Epoch: [255][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8070e-01 (-8.8666e-01)\n",
            "Epoch: [255][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8648e-01 (-8.8709e-01)\n",
            "Epoch: [255][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8745e-01 (-8.8709e-01)\n",
            "Validating...\n",
            "Top1: 0.7809416118421053\n",
            "Training...\n",
            "Epoch: [256][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.8573e-01 (-8.8573e-01)\n",
            "Epoch: [256][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7783e-01 (-8.8642e-01)\n",
            "Epoch: [256][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8720e-01 (-8.8661e-01)\n",
            "Epoch: [256][30/97]\tTime  0.176 ( 0.186)\tLoss -8.8979e-01 (-8.8765e-01)\n",
            "Epoch: [256][40/97]\tTime  0.176 ( 0.184)\tLoss -8.8670e-01 (-8.8795e-01)\n",
            "Epoch: [256][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9049e-01 (-8.8702e-01)\n",
            "Epoch: [256][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8554e-01 (-8.8675e-01)\n",
            "Epoch: [256][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8569e-01 (-8.8612e-01)\n",
            "Epoch: [256][80/97]\tTime  0.178 ( 0.180)\tLoss -8.7908e-01 (-8.8582e-01)\n",
            "Epoch: [256][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8837e-01 (-8.8596e-01)\n",
            "Training...\n",
            "Epoch: [257][ 0/97]\tTime  0.459 ( 0.459)\tLoss -8.8393e-01 (-8.8393e-01)\n",
            "Epoch: [257][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8466e-01 (-8.8501e-01)\n",
            "Epoch: [257][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8538e-01 (-8.8187e-01)\n",
            "Epoch: [257][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0227e-01 (-8.8319e-01)\n",
            "Epoch: [257][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8209e-01 (-8.8422e-01)\n",
            "Epoch: [257][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0193e-01 (-8.8638e-01)\n",
            "Epoch: [257][60/97]\tTime  0.176 ( 0.181)\tLoss -8.8820e-01 (-8.8674e-01)\n",
            "Epoch: [257][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8459e-01 (-8.8628e-01)\n",
            "Epoch: [257][80/97]\tTime  0.176 ( 0.180)\tLoss -8.8248e-01 (-8.8628e-01)\n",
            "Epoch: [257][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8284e-01 (-8.8599e-01)\n",
            "Training...\n",
            "Epoch: [258][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0060e-01 (-9.0060e-01)\n",
            "Epoch: [258][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9414e-01 (-8.8642e-01)\n",
            "Epoch: [258][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8515e-01 (-8.8579e-01)\n",
            "Epoch: [258][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8269e-01 (-8.8548e-01)\n",
            "Epoch: [258][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9424e-01 (-8.8559e-01)\n",
            "Epoch: [258][50/97]\tTime  0.176 ( 0.182)\tLoss -8.8776e-01 (-8.8586e-01)\n",
            "Epoch: [258][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9119e-01 (-8.8627e-01)\n",
            "Epoch: [258][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9460e-01 (-8.8561e-01)\n",
            "Epoch: [258][80/97]\tTime  0.176 ( 0.180)\tLoss -9.0095e-01 (-8.8566e-01)\n",
            "Epoch: [258][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8156e-01 (-8.8578e-01)\n",
            "Training...\n",
            "Epoch: [259][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8518e-01 (-8.8518e-01)\n",
            "Epoch: [259][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9504e-01 (-8.8647e-01)\n",
            "Epoch: [259][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8273e-01 (-8.8578e-01)\n",
            "Epoch: [259][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9200e-01 (-8.8421e-01)\n",
            "Epoch: [259][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9169e-01 (-8.8550e-01)\n",
            "Epoch: [259][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9623e-01 (-8.8627e-01)\n",
            "Epoch: [259][60/97]\tTime  0.176 ( 0.181)\tLoss -8.8206e-01 (-8.8664e-01)\n",
            "Epoch: [259][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8185e-01 (-8.8682e-01)\n",
            "Epoch: [259][80/97]\tTime  0.176 ( 0.180)\tLoss -8.9255e-01 (-8.8665e-01)\n",
            "Epoch: [259][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9025e-01 (-8.8634e-01)\n",
            "Training...\n",
            "Epoch: [260][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9368e-01 (-8.9368e-01)\n",
            "Epoch: [260][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8541e-01 (-8.8783e-01)\n",
            "Epoch: [260][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8914e-01 (-8.8755e-01)\n",
            "Epoch: [260][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8659e-01 (-8.8559e-01)\n",
            "Epoch: [260][40/97]\tTime  0.177 ( 0.184)\tLoss -8.6324e-01 (-8.8511e-01)\n",
            "Epoch: [260][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8908e-01 (-8.8517e-01)\n",
            "Epoch: [260][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8267e-01 (-8.8504e-01)\n",
            "Epoch: [260][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8809e-01 (-8.8520e-01)\n",
            "Epoch: [260][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8976e-01 (-8.8534e-01)\n",
            "Epoch: [260][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9332e-01 (-8.8552e-01)\n",
            "Validating...\n",
            "Top1: 0.79296875\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [261][ 0/97]\tTime  0.458 ( 0.458)\tLoss -8.8022e-01 (-8.8022e-01)\n",
            "Epoch: [261][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8136e-01 (-8.8432e-01)\n",
            "Epoch: [261][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9610e-01 (-8.8635e-01)\n",
            "Epoch: [261][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7942e-01 (-8.8592e-01)\n",
            "Epoch: [261][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9013e-01 (-8.8556e-01)\n",
            "Epoch: [261][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9059e-01 (-8.8507e-01)\n",
            "Epoch: [261][60/97]\tTime  0.177 ( 0.182)\tLoss -8.6544e-01 (-8.8447e-01)\n",
            "Epoch: [261][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8494e-01 (-8.8421e-01)\n",
            "Epoch: [261][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8578e-01 (-8.8443e-01)\n",
            "Epoch: [261][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7414e-01 (-8.8395e-01)\n",
            "Training...\n",
            "Epoch: [262][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7798e-01 (-8.7798e-01)\n",
            "Epoch: [262][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9098e-01 (-8.8552e-01)\n",
            "Epoch: [262][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9062e-01 (-8.8648e-01)\n",
            "Epoch: [262][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8573e-01 (-8.8622e-01)\n",
            "Epoch: [262][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8605e-01 (-8.8556e-01)\n",
            "Epoch: [262][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8331e-01 (-8.8527e-01)\n",
            "Epoch: [262][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8927e-01 (-8.8571e-01)\n",
            "Epoch: [262][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8504e-01 (-8.8640e-01)\n",
            "Epoch: [262][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7843e-01 (-8.8637e-01)\n",
            "Epoch: [262][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8706e-01 (-8.8589e-01)\n",
            "Training...\n",
            "Epoch: [263][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8520e-01 (-8.8520e-01)\n",
            "Epoch: [263][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8762e-01 (-8.8725e-01)\n",
            "Epoch: [263][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9924e-01 (-8.8912e-01)\n",
            "Epoch: [263][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9210e-01 (-8.8950e-01)\n",
            "Epoch: [263][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8540e-01 (-8.8919e-01)\n",
            "Epoch: [263][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8428e-01 (-8.8840e-01)\n",
            "Epoch: [263][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8653e-01 (-8.8811e-01)\n",
            "Epoch: [263][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8755e-01 (-8.8800e-01)\n",
            "Epoch: [263][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8895e-01 (-8.8776e-01)\n",
            "Epoch: [263][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8291e-01 (-8.8753e-01)\n",
            "Training...\n",
            "Epoch: [264][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7769e-01 (-8.7769e-01)\n",
            "Epoch: [264][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9775e-01 (-8.8856e-01)\n",
            "Epoch: [264][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8954e-01 (-8.8819e-01)\n",
            "Epoch: [264][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8325e-01 (-8.8881e-01)\n",
            "Epoch: [264][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8629e-01 (-8.8871e-01)\n",
            "Epoch: [264][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7920e-01 (-8.8814e-01)\n",
            "Epoch: [264][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7824e-01 (-8.8743e-01)\n",
            "Epoch: [264][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8951e-01 (-8.8747e-01)\n",
            "Epoch: [264][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9441e-01 (-8.8727e-01)\n",
            "Epoch: [264][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8567e-01 (-8.8721e-01)\n",
            "Training...\n",
            "Epoch: [265][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9343e-01 (-8.9343e-01)\n",
            "Epoch: [265][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7864e-01 (-8.8659e-01)\n",
            "Epoch: [265][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8580e-01 (-8.8667e-01)\n",
            "Epoch: [265][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8697e-01 (-8.8699e-01)\n",
            "Epoch: [265][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9582e-01 (-8.8718e-01)\n",
            "Epoch: [265][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8815e-01 (-8.8609e-01)\n",
            "Epoch: [265][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8036e-01 (-8.8632e-01)\n",
            "Epoch: [265][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8085e-01 (-8.8619e-01)\n",
            "Epoch: [265][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8130e-01 (-8.8584e-01)\n",
            "Epoch: [265][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8248e-01 (-8.8523e-01)\n",
            "Validating...\n",
            "Top1: 0.7939967105263158\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [266][ 0/97]\tTime  0.459 ( 0.459)\tLoss -8.8516e-01 (-8.8516e-01)\n",
            "Epoch: [266][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9276e-01 (-8.8834e-01)\n",
            "Epoch: [266][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9124e-01 (-8.8618e-01)\n",
            "Epoch: [266][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8541e-01 (-8.8396e-01)\n",
            "Epoch: [266][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8224e-01 (-8.8356e-01)\n",
            "Epoch: [266][50/97]\tTime  0.177 ( 0.183)\tLoss -8.7486e-01 (-8.8310e-01)\n",
            "Epoch: [266][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8988e-01 (-8.8326e-01)\n",
            "Epoch: [266][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9183e-01 (-8.8355e-01)\n",
            "Epoch: [266][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9184e-01 (-8.8382e-01)\n",
            "Epoch: [266][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9836e-01 (-8.8477e-01)\n",
            "Training...\n",
            "Epoch: [267][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.7938e-01 (-8.7938e-01)\n",
            "Epoch: [267][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9351e-01 (-8.8719e-01)\n",
            "Epoch: [267][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9071e-01 (-8.8660e-01)\n",
            "Epoch: [267][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8771e-01 (-8.8690e-01)\n",
            "Epoch: [267][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8157e-01 (-8.8641e-01)\n",
            "Epoch: [267][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7793e-01 (-8.8579e-01)\n",
            "Epoch: [267][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8389e-01 (-8.8559e-01)\n",
            "Epoch: [267][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9078e-01 (-8.8555e-01)\n",
            "Epoch: [267][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7517e-01 (-8.8507e-01)\n",
            "Epoch: [267][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8680e-01 (-8.8502e-01)\n",
            "Training...\n",
            "Epoch: [268][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.8630e-01 (-8.8630e-01)\n",
            "Epoch: [268][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8271e-01 (-8.8867e-01)\n",
            "Epoch: [268][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9215e-01 (-8.8795e-01)\n",
            "Epoch: [268][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9271e-01 (-8.8821e-01)\n",
            "Epoch: [268][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8877e-01 (-8.8780e-01)\n",
            "Epoch: [268][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7656e-01 (-8.8760e-01)\n",
            "Epoch: [268][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8984e-01 (-8.8684e-01)\n",
            "Epoch: [268][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8344e-01 (-8.8680e-01)\n",
            "Epoch: [268][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9120e-01 (-8.8685e-01)\n",
            "Epoch: [268][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8517e-01 (-8.8694e-01)\n",
            "Training...\n",
            "Epoch: [269][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.7375e-01 (-8.7375e-01)\n",
            "Epoch: [269][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8049e-01 (-8.8183e-01)\n",
            "Epoch: [269][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7472e-01 (-8.8283e-01)\n",
            "Epoch: [269][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8941e-01 (-8.8233e-01)\n",
            "Epoch: [269][40/97]\tTime  0.178 ( 0.183)\tLoss -9.0210e-01 (-8.8511e-01)\n",
            "Epoch: [269][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9405e-01 (-8.8593e-01)\n",
            "Epoch: [269][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8886e-01 (-8.8645e-01)\n",
            "Epoch: [269][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9262e-01 (-8.8668e-01)\n",
            "Epoch: [269][80/97]\tTime  0.178 ( 0.180)\tLoss -8.8889e-01 (-8.8652e-01)\n",
            "Epoch: [269][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9372e-01 (-8.8692e-01)\n",
            "Training...\n",
            "Epoch: [270][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7892e-01 (-8.7892e-01)\n",
            "Epoch: [270][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7719e-01 (-8.8333e-01)\n",
            "Epoch: [270][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9294e-01 (-8.8707e-01)\n",
            "Epoch: [270][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9561e-01 (-8.8825e-01)\n",
            "Epoch: [270][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8290e-01 (-8.8843e-01)\n",
            "Epoch: [270][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8538e-01 (-8.8872e-01)\n",
            "Epoch: [270][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8080e-01 (-8.8828e-01)\n",
            "Epoch: [270][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8659e-01 (-8.8785e-01)\n",
            "Epoch: [270][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8638e-01 (-8.8759e-01)\n",
            "Epoch: [270][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7944e-01 (-8.8729e-01)\n",
            "Validating...\n",
            "Top1: 0.7869037828947368\n",
            "Training...\n",
            "Epoch: [271][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.7975e-01 (-8.7975e-01)\n",
            "Epoch: [271][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8761e-01 (-8.8796e-01)\n",
            "Epoch: [271][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8182e-01 (-8.8809e-01)\n",
            "Epoch: [271][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8758e-01 (-8.8801e-01)\n",
            "Epoch: [271][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9124e-01 (-8.8747e-01)\n",
            "Epoch: [271][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8494e-01 (-8.8615e-01)\n",
            "Epoch: [271][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8780e-01 (-8.8648e-01)\n",
            "Epoch: [271][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9156e-01 (-8.8654e-01)\n",
            "Epoch: [271][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7901e-01 (-8.8625e-01)\n",
            "Epoch: [271][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9664e-01 (-8.8619e-01)\n",
            "Training...\n",
            "Epoch: [272][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.7638e-01 (-8.7638e-01)\n",
            "Epoch: [272][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8968e-01 (-8.9157e-01)\n",
            "Epoch: [272][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9218e-01 (-8.9108e-01)\n",
            "Epoch: [272][30/97]\tTime  0.178 ( 0.186)\tLoss -8.7960e-01 (-8.9023e-01)\n",
            "Epoch: [272][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9143e-01 (-8.8893e-01)\n",
            "Epoch: [272][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8788e-01 (-8.8926e-01)\n",
            "Epoch: [272][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9096e-01 (-8.8925e-01)\n",
            "Epoch: [272][70/97]\tTime  0.177 ( 0.181)\tLoss -8.6311e-01 (-8.8774e-01)\n",
            "Epoch: [272][80/97]\tTime  0.178 ( 0.180)\tLoss -8.7692e-01 (-8.8734e-01)\n",
            "Epoch: [272][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9065e-01 (-8.8679e-01)\n",
            "Training...\n",
            "Epoch: [273][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7174e-01 (-8.7174e-01)\n",
            "Epoch: [273][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8424e-01 (-8.7987e-01)\n",
            "Epoch: [273][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8910e-01 (-8.8222e-01)\n",
            "Epoch: [273][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8752e-01 (-8.8396e-01)\n",
            "Epoch: [273][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9042e-01 (-8.8554e-01)\n",
            "Epoch: [273][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9768e-01 (-8.8586e-01)\n",
            "Epoch: [273][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9659e-01 (-8.8558e-01)\n",
            "Epoch: [273][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7517e-01 (-8.8580e-01)\n",
            "Epoch: [273][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9425e-01 (-8.8549e-01)\n",
            "Epoch: [273][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8591e-01 (-8.8518e-01)\n",
            "Training...\n",
            "Epoch: [274][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8616e-01 (-8.8616e-01)\n",
            "Epoch: [274][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8461e-01 (-8.8548e-01)\n",
            "Epoch: [274][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8302e-01 (-8.8610e-01)\n",
            "Epoch: [274][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8606e-01 (-8.8677e-01)\n",
            "Epoch: [274][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8314e-01 (-8.8676e-01)\n",
            "Epoch: [274][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9067e-01 (-8.8731e-01)\n",
            "Epoch: [274][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8232e-01 (-8.8707e-01)\n",
            "Epoch: [274][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8943e-01 (-8.8690e-01)\n",
            "Epoch: [274][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9192e-01 (-8.8693e-01)\n",
            "Epoch: [274][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9109e-01 (-8.8693e-01)\n",
            "Training...\n",
            "Epoch: [275][ 0/97]\tTime  0.441 ( 0.441)\tLoss -8.8070e-01 (-8.8070e-01)\n",
            "Epoch: [275][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8046e-01 (-8.8797e-01)\n",
            "Epoch: [275][20/97]\tTime  0.177 ( 0.189)\tLoss -8.8377e-01 (-8.8876e-01)\n",
            "Epoch: [275][30/97]\tTime  0.177 ( 0.185)\tLoss -8.7484e-01 (-8.8822e-01)\n",
            "Epoch: [275][40/97]\tTime  0.177 ( 0.183)\tLoss -8.7797e-01 (-8.8758e-01)\n",
            "Epoch: [275][50/97]\tTime  0.178 ( 0.182)\tLoss -8.7556e-01 (-8.8746e-01)\n",
            "Epoch: [275][60/97]\tTime  0.177 ( 0.181)\tLoss -8.7451e-01 (-8.8758e-01)\n",
            "Epoch: [275][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8992e-01 (-8.8763e-01)\n",
            "Epoch: [275][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8389e-01 (-8.8757e-01)\n",
            "Epoch: [275][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9122e-01 (-8.8755e-01)\n",
            "Validating...\n",
            "Top1: 0.7933799342105263\n",
            "Training...\n",
            "Epoch: [276][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8707e-01 (-8.8707e-01)\n",
            "Epoch: [276][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8389e-01 (-8.8561e-01)\n",
            "Epoch: [276][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8607e-01 (-8.8312e-01)\n",
            "Epoch: [276][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9023e-01 (-8.8473e-01)\n",
            "Epoch: [276][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0641e-01 (-8.8629e-01)\n",
            "Epoch: [276][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9538e-01 (-8.8721e-01)\n",
            "Epoch: [276][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9246e-01 (-8.8714e-01)\n",
            "Epoch: [276][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8239e-01 (-8.8712e-01)\n",
            "Epoch: [276][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9347e-01 (-8.8718e-01)\n",
            "Epoch: [276][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8397e-01 (-8.8686e-01)\n",
            "Training...\n",
            "Epoch: [277][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8103e-01 (-8.8103e-01)\n",
            "Epoch: [277][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9804e-01 (-8.9168e-01)\n",
            "Epoch: [277][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8656e-01 (-8.9219e-01)\n",
            "Epoch: [277][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9034e-01 (-8.9143e-01)\n",
            "Epoch: [277][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0108e-01 (-8.9083e-01)\n",
            "Epoch: [277][50/97]\tTime  0.176 ( 0.182)\tLoss -8.8679e-01 (-8.8942e-01)\n",
            "Epoch: [277][60/97]\tTime  0.176 ( 0.181)\tLoss -8.8982e-01 (-8.8873e-01)\n",
            "Epoch: [277][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7855e-01 (-8.8762e-01)\n",
            "Epoch: [277][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8801e-01 (-8.8773e-01)\n",
            "Epoch: [277][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7248e-01 (-8.8726e-01)\n",
            "Training...\n",
            "Epoch: [278][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8558e-01 (-8.8558e-01)\n",
            "Epoch: [278][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9932e-01 (-8.8448e-01)\n",
            "Epoch: [278][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8146e-01 (-8.8374e-01)\n",
            "Epoch: [278][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8862e-01 (-8.8495e-01)\n",
            "Epoch: [278][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8192e-01 (-8.8533e-01)\n",
            "Epoch: [278][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8218e-01 (-8.8550e-01)\n",
            "Epoch: [278][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8285e-01 (-8.8560e-01)\n",
            "Epoch: [278][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8750e-01 (-8.8480e-01)\n",
            "Epoch: [278][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9440e-01 (-8.8480e-01)\n",
            "Epoch: [278][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8380e-01 (-8.8484e-01)\n",
            "Training...\n",
            "Epoch: [279][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8384e-01 (-8.8384e-01)\n",
            "Epoch: [279][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9117e-01 (-8.8770e-01)\n",
            "Epoch: [279][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8173e-01 (-8.8880e-01)\n",
            "Epoch: [279][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8443e-01 (-8.8936e-01)\n",
            "Epoch: [279][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7822e-01 (-8.8881e-01)\n",
            "Epoch: [279][50/97]\tTime  0.178 ( 0.182)\tLoss -8.8866e-01 (-8.8850e-01)\n",
            "Epoch: [279][60/97]\tTime  0.178 ( 0.181)\tLoss -8.9455e-01 (-8.8922e-01)\n",
            "Epoch: [279][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8257e-01 (-8.8951e-01)\n",
            "Epoch: [279][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9116e-01 (-8.8903e-01)\n",
            "Epoch: [279][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6342e-01 (-8.8879e-01)\n",
            "Training...\n",
            "Epoch: [280][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9043e-01 (-8.9043e-01)\n",
            "Epoch: [280][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8905e-01 (-8.8693e-01)\n",
            "Epoch: [280][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8313e-01 (-8.8777e-01)\n",
            "Epoch: [280][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8424e-01 (-8.8781e-01)\n",
            "Epoch: [280][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9080e-01 (-8.8816e-01)\n",
            "Epoch: [280][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8690e-01 (-8.8699e-01)\n",
            "Epoch: [280][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9304e-01 (-8.8675e-01)\n",
            "Epoch: [280][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8299e-01 (-8.8655e-01)\n",
            "Epoch: [280][80/97]\tTime  0.178 ( 0.180)\tLoss -8.8202e-01 (-8.8575e-01)\n",
            "Epoch: [280][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8962e-01 (-8.8567e-01)\n",
            "Validating...\n",
            "Top1: 0.7908100328947368\n",
            "Training...\n",
            "Epoch: [281][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.8175e-01 (-8.8175e-01)\n",
            "Epoch: [281][10/97]\tTime  0.176 ( 0.201)\tLoss -8.7929e-01 (-8.8533e-01)\n",
            "Epoch: [281][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8022e-01 (-8.8646e-01)\n",
            "Epoch: [281][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9309e-01 (-8.8693e-01)\n",
            "Epoch: [281][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8855e-01 (-8.8810e-01)\n",
            "Epoch: [281][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9619e-01 (-8.8881e-01)\n",
            "Epoch: [281][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9358e-01 (-8.8890e-01)\n",
            "Epoch: [281][70/97]\tTime  0.176 ( 0.181)\tLoss -8.8273e-01 (-8.8933e-01)\n",
            "Epoch: [281][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8523e-01 (-8.8930e-01)\n",
            "Epoch: [281][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9256e-01 (-8.8901e-01)\n",
            "Training...\n",
            "Epoch: [282][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8397e-01 (-8.8397e-01)\n",
            "Epoch: [282][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7752e-01 (-8.8559e-01)\n",
            "Epoch: [282][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9454e-01 (-8.8925e-01)\n",
            "Epoch: [282][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8113e-01 (-8.8798e-01)\n",
            "Epoch: [282][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8022e-01 (-8.8705e-01)\n",
            "Epoch: [282][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8046e-01 (-8.8694e-01)\n",
            "Epoch: [282][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9535e-01 (-8.8728e-01)\n",
            "Epoch: [282][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8567e-01 (-8.8643e-01)\n",
            "Epoch: [282][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8474e-01 (-8.8643e-01)\n",
            "Epoch: [282][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9101e-01 (-8.8709e-01)\n",
            "Training...\n",
            "Epoch: [283][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9399e-01 (-8.9399e-01)\n",
            "Epoch: [283][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8886e-01 (-8.9283e-01)\n",
            "Epoch: [283][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8633e-01 (-8.9125e-01)\n",
            "Epoch: [283][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8651e-01 (-8.9104e-01)\n",
            "Epoch: [283][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8512e-01 (-8.8956e-01)\n",
            "Epoch: [283][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8419e-01 (-8.8907e-01)\n",
            "Epoch: [283][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9080e-01 (-8.8940e-01)\n",
            "Epoch: [283][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9118e-01 (-8.8935e-01)\n",
            "Epoch: [283][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9013e-01 (-8.8940e-01)\n",
            "Epoch: [283][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8643e-01 (-8.8919e-01)\n",
            "Training...\n",
            "Epoch: [284][ 0/97]\tTime  0.461 ( 0.461)\tLoss -8.8253e-01 (-8.8253e-01)\n",
            "Epoch: [284][10/97]\tTime  0.177 ( 0.203)\tLoss -8.8537e-01 (-8.8648e-01)\n",
            "Epoch: [284][20/97]\tTime  0.176 ( 0.190)\tLoss -8.9457e-01 (-8.8826e-01)\n",
            "Epoch: [284][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9034e-01 (-8.8682e-01)\n",
            "Epoch: [284][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9230e-01 (-8.8668e-01)\n",
            "Epoch: [284][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9099e-01 (-8.8693e-01)\n",
            "Epoch: [284][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0439e-01 (-8.8775e-01)\n",
            "Epoch: [284][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7812e-01 (-8.8725e-01)\n",
            "Epoch: [284][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8439e-01 (-8.8660e-01)\n",
            "Epoch: [284][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9870e-01 (-8.8636e-01)\n",
            "Training...\n",
            "Epoch: [285][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9651e-01 (-8.9651e-01)\n",
            "Epoch: [285][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0234e-01 (-8.9266e-01)\n",
            "Epoch: [285][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9248e-01 (-8.9252e-01)\n",
            "Epoch: [285][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8560e-01 (-8.9114e-01)\n",
            "Epoch: [285][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8535e-01 (-8.9086e-01)\n",
            "Epoch: [285][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8620e-01 (-8.8921e-01)\n",
            "Epoch: [285][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0052e-01 (-8.8871e-01)\n",
            "Epoch: [285][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8592e-01 (-8.8842e-01)\n",
            "Epoch: [285][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8761e-01 (-8.8894e-01)\n",
            "Epoch: [285][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8307e-01 (-8.8878e-01)\n",
            "Validating...\n",
            "Top1: 0.7971833881578947\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [286][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8289e-01 (-8.8289e-01)\n",
            "Epoch: [286][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7365e-01 (-8.8547e-01)\n",
            "Epoch: [286][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8266e-01 (-8.8758e-01)\n",
            "Epoch: [286][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8823e-01 (-8.8963e-01)\n",
            "Epoch: [286][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7329e-01 (-8.8874e-01)\n",
            "Epoch: [286][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9402e-01 (-8.8818e-01)\n",
            "Epoch: [286][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8618e-01 (-8.8832e-01)\n",
            "Epoch: [286][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8604e-01 (-8.8846e-01)\n",
            "Epoch: [286][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9228e-01 (-8.8832e-01)\n",
            "Epoch: [286][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9416e-01 (-8.8802e-01)\n",
            "Training...\n",
            "Epoch: [287][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8506e-01 (-8.8506e-01)\n",
            "Epoch: [287][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9263e-01 (-8.9164e-01)\n",
            "Epoch: [287][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8548e-01 (-8.9238e-01)\n",
            "Epoch: [287][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8675e-01 (-8.9199e-01)\n",
            "Epoch: [287][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9118e-01 (-8.9130e-01)\n",
            "Epoch: [287][50/97]\tTime  0.178 ( 0.182)\tLoss -8.8508e-01 (-8.9053e-01)\n",
            "Epoch: [287][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8331e-01 (-8.9022e-01)\n",
            "Epoch: [287][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7650e-01 (-8.9017e-01)\n",
            "Epoch: [287][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8424e-01 (-8.8967e-01)\n",
            "Epoch: [287][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7580e-01 (-8.8910e-01)\n",
            "Training...\n",
            "Epoch: [288][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8777e-01 (-8.8777e-01)\n",
            "Epoch: [288][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0012e-01 (-8.9112e-01)\n",
            "Epoch: [288][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8117e-01 (-8.8890e-01)\n",
            "Epoch: [288][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8611e-01 (-8.8867e-01)\n",
            "Epoch: [288][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8177e-01 (-8.8877e-01)\n",
            "Epoch: [288][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8224e-01 (-8.8906e-01)\n",
            "Epoch: [288][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9089e-01 (-8.8887e-01)\n",
            "Epoch: [288][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8249e-01 (-8.8868e-01)\n",
            "Epoch: [288][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9318e-01 (-8.8858e-01)\n",
            "Epoch: [288][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8450e-01 (-8.8785e-01)\n",
            "Training...\n",
            "Epoch: [289][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9459e-01 (-8.9459e-01)\n",
            "Epoch: [289][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8266e-01 (-8.8590e-01)\n",
            "Epoch: [289][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8522e-01 (-8.8652e-01)\n",
            "Epoch: [289][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8602e-01 (-8.8641e-01)\n",
            "Epoch: [289][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8768e-01 (-8.8659e-01)\n",
            "Epoch: [289][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8972e-01 (-8.8731e-01)\n",
            "Epoch: [289][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9826e-01 (-8.8770e-01)\n",
            "Epoch: [289][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9416e-01 (-8.8798e-01)\n",
            "Epoch: [289][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8460e-01 (-8.8804e-01)\n",
            "Epoch: [289][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0007e-01 (-8.8832e-01)\n",
            "Training...\n",
            "Epoch: [290][ 0/97]\tTime  0.458 ( 0.458)\tLoss -8.7948e-01 (-8.7948e-01)\n",
            "Epoch: [290][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8217e-01 (-8.8424e-01)\n",
            "Epoch: [290][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0486e-01 (-8.8797e-01)\n",
            "Epoch: [290][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8136e-01 (-8.8785e-01)\n",
            "Epoch: [290][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9032e-01 (-8.8957e-01)\n",
            "Epoch: [290][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9408e-01 (-8.8972e-01)\n",
            "Epoch: [290][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8817e-01 (-8.8976e-01)\n",
            "Epoch: [290][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8795e-01 (-8.8958e-01)\n",
            "Epoch: [290][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9379e-01 (-8.8921e-01)\n",
            "Epoch: [290][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8696e-01 (-8.8919e-01)\n",
            "Validating...\n",
            "Top1: 0.8021175986842105\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [291][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8389e-01 (-8.8389e-01)\n",
            "Epoch: [291][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8990e-01 (-8.8807e-01)\n",
            "Epoch: [291][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7788e-01 (-8.8517e-01)\n",
            "Epoch: [291][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9529e-01 (-8.8561e-01)\n",
            "Epoch: [291][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9839e-01 (-8.8744e-01)\n",
            "Epoch: [291][50/97]\tTime  0.176 ( 0.182)\tLoss -8.8751e-01 (-8.8860e-01)\n",
            "Epoch: [291][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8304e-01 (-8.8784e-01)\n",
            "Epoch: [291][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8098e-01 (-8.8752e-01)\n",
            "Epoch: [291][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8347e-01 (-8.8778e-01)\n",
            "Epoch: [291][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8950e-01 (-8.8773e-01)\n",
            "Training...\n",
            "Epoch: [292][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.9776e-01 (-8.9776e-01)\n",
            "Epoch: [292][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8496e-01 (-8.9098e-01)\n",
            "Epoch: [292][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9340e-01 (-8.9008e-01)\n",
            "Epoch: [292][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9881e-01 (-8.9013e-01)\n",
            "Epoch: [292][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8667e-01 (-8.8889e-01)\n",
            "Epoch: [292][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7146e-01 (-8.8900e-01)\n",
            "Epoch: [292][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9032e-01 (-8.8892e-01)\n",
            "Epoch: [292][70/97]\tTime  0.176 ( 0.181)\tLoss -8.8458e-01 (-8.8902e-01)\n",
            "Epoch: [292][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7789e-01 (-8.8878e-01)\n",
            "Epoch: [292][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8467e-01 (-8.8872e-01)\n",
            "Training...\n",
            "Epoch: [293][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9090e-01 (-8.9090e-01)\n",
            "Epoch: [293][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8883e-01 (-8.8844e-01)\n",
            "Epoch: [293][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0657e-01 (-8.9148e-01)\n",
            "Epoch: [293][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9276e-01 (-8.9046e-01)\n",
            "Epoch: [293][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7970e-01 (-8.8920e-01)\n",
            "Epoch: [293][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9470e-01 (-8.8890e-01)\n",
            "Epoch: [293][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8020e-01 (-8.8870e-01)\n",
            "Epoch: [293][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8176e-01 (-8.8857e-01)\n",
            "Epoch: [293][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8600e-01 (-8.8905e-01)\n",
            "Epoch: [293][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8789e-01 (-8.8951e-01)\n",
            "Training...\n",
            "Epoch: [294][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8437e-01 (-8.8437e-01)\n",
            "Epoch: [294][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7549e-01 (-8.8803e-01)\n",
            "Epoch: [294][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9687e-01 (-8.8950e-01)\n",
            "Epoch: [294][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0088e-01 (-8.9050e-01)\n",
            "Epoch: [294][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8870e-01 (-8.9116e-01)\n",
            "Epoch: [294][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8717e-01 (-8.9072e-01)\n",
            "Epoch: [294][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9613e-01 (-8.9096e-01)\n",
            "Epoch: [294][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9072e-01 (-8.9090e-01)\n",
            "Epoch: [294][80/97]\tTime  0.178 ( 0.180)\tLoss -8.8618e-01 (-8.9048e-01)\n",
            "Epoch: [294][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8924e-01 (-8.8982e-01)\n",
            "Training...\n",
            "Epoch: [295][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8105e-01 (-8.8105e-01)\n",
            "Epoch: [295][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9060e-01 (-8.8686e-01)\n",
            "Epoch: [295][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8226e-01 (-8.8621e-01)\n",
            "Epoch: [295][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8478e-01 (-8.8631e-01)\n",
            "Epoch: [295][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0079e-01 (-8.8761e-01)\n",
            "Epoch: [295][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8862e-01 (-8.8877e-01)\n",
            "Epoch: [295][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9354e-01 (-8.8917e-01)\n",
            "Epoch: [295][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8868e-01 (-8.8886e-01)\n",
            "Epoch: [295][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9266e-01 (-8.8932e-01)\n",
            "Epoch: [295][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9009e-01 (-8.8967e-01)\n",
            "Validating...\n",
            "Top1: 0.8058182565789473\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [296][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.7427e-01 (-8.7427e-01)\n",
            "Epoch: [296][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9144e-01 (-8.8767e-01)\n",
            "Epoch: [296][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8421e-01 (-8.8714e-01)\n",
            "Epoch: [296][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7556e-01 (-8.8638e-01)\n",
            "Epoch: [296][40/97]\tTime  0.176 ( 0.183)\tLoss -8.9395e-01 (-8.8650e-01)\n",
            "Epoch: [296][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8443e-01 (-8.8640e-01)\n",
            "Epoch: [296][60/97]\tTime  0.178 ( 0.181)\tLoss -8.8726e-01 (-8.8602e-01)\n",
            "Epoch: [296][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8473e-01 (-8.8514e-01)\n",
            "Epoch: [296][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9621e-01 (-8.8559e-01)\n",
            "Epoch: [296][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9049e-01 (-8.8616e-01)\n",
            "Training...\n",
            "Epoch: [297][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9076e-01 (-8.9076e-01)\n",
            "Epoch: [297][10/97]\tTime  0.176 ( 0.202)\tLoss -8.9838e-01 (-8.9231e-01)\n",
            "Epoch: [297][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9986e-01 (-8.9343e-01)\n",
            "Epoch: [297][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9332e-01 (-8.9221e-01)\n",
            "Epoch: [297][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8957e-01 (-8.9161e-01)\n",
            "Epoch: [297][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8901e-01 (-8.9041e-01)\n",
            "Epoch: [297][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9576e-01 (-8.9059e-01)\n",
            "Epoch: [297][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0068e-01 (-8.9017e-01)\n",
            "Epoch: [297][80/97]\tTime  0.178 ( 0.180)\tLoss -8.9347e-01 (-8.9006e-01)\n",
            "Epoch: [297][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7598e-01 (-8.8973e-01)\n",
            "Training...\n",
            "Epoch: [298][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.8930e-01 (-8.8930e-01)\n",
            "Epoch: [298][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9317e-01 (-8.8789e-01)\n",
            "Epoch: [298][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8840e-01 (-8.8950e-01)\n",
            "Epoch: [298][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8562e-01 (-8.8911e-01)\n",
            "Epoch: [298][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8032e-01 (-8.8948e-01)\n",
            "Epoch: [298][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9717e-01 (-8.8827e-01)\n",
            "Epoch: [298][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9006e-01 (-8.8868e-01)\n",
            "Epoch: [298][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8967e-01 (-8.8929e-01)\n",
            "Epoch: [298][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8392e-01 (-8.8903e-01)\n",
            "Epoch: [298][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9619e-01 (-8.8895e-01)\n",
            "Training...\n",
            "Epoch: [299][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8519e-01 (-8.8519e-01)\n",
            "Epoch: [299][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9582e-01 (-8.8757e-01)\n",
            "Epoch: [299][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8216e-01 (-8.8748e-01)\n",
            "Epoch: [299][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8703e-01 (-8.8727e-01)\n",
            "Epoch: [299][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8125e-01 (-8.8742e-01)\n",
            "Epoch: [299][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8248e-01 (-8.8791e-01)\n",
            "Epoch: [299][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8234e-01 (-8.8769e-01)\n",
            "Epoch: [299][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9281e-01 (-8.8804e-01)\n",
            "Epoch: [299][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8399e-01 (-8.8773e-01)\n",
            "Epoch: [299][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8289e-01 (-8.8764e-01)\n",
            "Training...\n",
            "Epoch: [300][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.8755e-01 (-8.8755e-01)\n",
            "Epoch: [300][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8748e-01 (-8.9024e-01)\n",
            "Epoch: [300][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8406e-01 (-8.9222e-01)\n",
            "Epoch: [300][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8516e-01 (-8.9105e-01)\n",
            "Epoch: [300][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8446e-01 (-8.9012e-01)\n",
            "Epoch: [300][50/97]\tTime  0.178 ( 0.182)\tLoss -8.7489e-01 (-8.8951e-01)\n",
            "Epoch: [300][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8054e-01 (-8.8953e-01)\n",
            "Epoch: [300][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8091e-01 (-8.8927e-01)\n",
            "Epoch: [300][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9893e-01 (-8.8940e-01)\n",
            "Epoch: [300][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0032e-01 (-8.8941e-01)\n",
            "Validating...\n",
            "Top1: 0.8067434210526315\n",
            "Saving the best model!\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [301][ 0/97]\tTime  0.442 ( 0.442)\tLoss -8.9268e-01 (-8.9268e-01)\n",
            "Epoch: [301][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8941e-01 (-8.9056e-01)\n",
            "Epoch: [301][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9312e-01 (-8.9017e-01)\n",
            "Epoch: [301][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7260e-01 (-8.8817e-01)\n",
            "Epoch: [301][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9358e-01 (-8.8851e-01)\n",
            "Epoch: [301][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8390e-01 (-8.8849e-01)\n",
            "Epoch: [301][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9042e-01 (-8.8896e-01)\n",
            "Epoch: [301][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8943e-01 (-8.8839e-01)\n",
            "Epoch: [301][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8020e-01 (-8.8782e-01)\n",
            "Epoch: [301][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9366e-01 (-8.8814e-01)\n",
            "Training...\n",
            "Epoch: [302][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.9416e-01 (-8.9416e-01)\n",
            "Epoch: [302][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8626e-01 (-8.9593e-01)\n",
            "Epoch: [302][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0131e-01 (-8.9564e-01)\n",
            "Epoch: [302][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9206e-01 (-8.9478e-01)\n",
            "Epoch: [302][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0055e-01 (-8.9359e-01)\n",
            "Epoch: [302][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0051e-01 (-8.9371e-01)\n",
            "Epoch: [302][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9435e-01 (-8.9356e-01)\n",
            "Epoch: [302][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9063e-01 (-8.9237e-01)\n",
            "Epoch: [302][80/97]\tTime  0.179 ( 0.180)\tLoss -8.8672e-01 (-8.9152e-01)\n",
            "Epoch: [302][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7427e-01 (-8.9083e-01)\n",
            "Training...\n",
            "Epoch: [303][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.7815e-01 (-8.7815e-01)\n",
            "Epoch: [303][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8683e-01 (-8.8986e-01)\n",
            "Epoch: [303][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8993e-01 (-8.8849e-01)\n",
            "Epoch: [303][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9134e-01 (-8.8797e-01)\n",
            "Epoch: [303][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9897e-01 (-8.8757e-01)\n",
            "Epoch: [303][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8473e-01 (-8.8756e-01)\n",
            "Epoch: [303][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8134e-01 (-8.8704e-01)\n",
            "Epoch: [303][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7967e-01 (-8.8711e-01)\n",
            "Epoch: [303][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7652e-01 (-8.8740e-01)\n",
            "Epoch: [303][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8045e-01 (-8.8766e-01)\n",
            "Training...\n",
            "Epoch: [304][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8457e-01 (-8.8457e-01)\n",
            "Epoch: [304][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0692e-01 (-8.8990e-01)\n",
            "Epoch: [304][20/97]\tTime  0.176 ( 0.190)\tLoss -8.8578e-01 (-8.8812e-01)\n",
            "Epoch: [304][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7988e-01 (-8.8762e-01)\n",
            "Epoch: [304][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8957e-01 (-8.8844e-01)\n",
            "Epoch: [304][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8660e-01 (-8.8908e-01)\n",
            "Epoch: [304][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9852e-01 (-8.8996e-01)\n",
            "Epoch: [304][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9290e-01 (-8.9053e-01)\n",
            "Epoch: [304][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9756e-01 (-8.9068e-01)\n",
            "Epoch: [304][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8334e-01 (-8.9012e-01)\n",
            "Training...\n",
            "Epoch: [305][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9207e-01 (-8.9207e-01)\n",
            "Epoch: [305][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8745e-01 (-8.8617e-01)\n",
            "Epoch: [305][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8978e-01 (-8.8793e-01)\n",
            "Epoch: [305][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9320e-01 (-8.8871e-01)\n",
            "Epoch: [305][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9456e-01 (-8.8957e-01)\n",
            "Epoch: [305][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9161e-01 (-8.9031e-01)\n",
            "Epoch: [305][60/97]\tTime  0.176 ( 0.181)\tLoss -8.8423e-01 (-8.8985e-01)\n",
            "Epoch: [305][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8453e-01 (-8.8993e-01)\n",
            "Epoch: [305][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9202e-01 (-8.8987e-01)\n",
            "Epoch: [305][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7579e-01 (-8.8940e-01)\n",
            "Validating...\n",
            "Top1: 0.8118832236842105\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [306][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8345e-01 (-8.8345e-01)\n",
            "Epoch: [306][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8742e-01 (-8.8375e-01)\n",
            "Epoch: [306][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8254e-01 (-8.8709e-01)\n",
            "Epoch: [306][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9896e-01 (-8.8837e-01)\n",
            "Epoch: [306][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0358e-01 (-8.8936e-01)\n",
            "Epoch: [306][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9413e-01 (-8.8996e-01)\n",
            "Epoch: [306][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8793e-01 (-8.8952e-01)\n",
            "Epoch: [306][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7190e-01 (-8.8883e-01)\n",
            "Epoch: [306][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8812e-01 (-8.8835e-01)\n",
            "Epoch: [306][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8838e-01 (-8.8797e-01)\n",
            "Training...\n",
            "Epoch: [307][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.8443e-01 (-8.8443e-01)\n",
            "Epoch: [307][10/97]\tTime  0.177 ( 0.201)\tLoss -8.7396e-01 (-8.8849e-01)\n",
            "Epoch: [307][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9059e-01 (-8.9007e-01)\n",
            "Epoch: [307][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9802e-01 (-8.9098e-01)\n",
            "Epoch: [307][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8406e-01 (-8.9071e-01)\n",
            "Epoch: [307][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9811e-01 (-8.9010e-01)\n",
            "Epoch: [307][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8728e-01 (-8.8963e-01)\n",
            "Epoch: [307][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8722e-01 (-8.8914e-01)\n",
            "Epoch: [307][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8196e-01 (-8.8886e-01)\n",
            "Epoch: [307][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9218e-01 (-8.8868e-01)\n",
            "Training...\n",
            "Epoch: [308][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.8749e-01 (-8.8749e-01)\n",
            "Epoch: [308][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9355e-01 (-8.8815e-01)\n",
            "Epoch: [308][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0204e-01 (-8.8946e-01)\n",
            "Epoch: [308][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9177e-01 (-8.8964e-01)\n",
            "Epoch: [308][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8947e-01 (-8.8938e-01)\n",
            "Epoch: [308][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8072e-01 (-8.8884e-01)\n",
            "Epoch: [308][60/97]\tTime  0.176 ( 0.181)\tLoss -8.8893e-01 (-8.8825e-01)\n",
            "Epoch: [308][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8638e-01 (-8.8783e-01)\n",
            "Epoch: [308][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0204e-01 (-8.8829e-01)\n",
            "Epoch: [308][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9016e-01 (-8.8805e-01)\n",
            "Training...\n",
            "Epoch: [309][ 0/97]\tTime  0.464 ( 0.464)\tLoss -9.0045e-01 (-9.0045e-01)\n",
            "Epoch: [309][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9188e-01 (-8.9104e-01)\n",
            "Epoch: [309][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9017e-01 (-8.8917e-01)\n",
            "Epoch: [309][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0100e-01 (-8.8964e-01)\n",
            "Epoch: [309][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8400e-01 (-8.8958e-01)\n",
            "Epoch: [309][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9150e-01 (-8.8903e-01)\n",
            "Epoch: [309][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7933e-01 (-8.8796e-01)\n",
            "Epoch: [309][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8223e-01 (-8.8801e-01)\n",
            "Epoch: [309][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9638e-01 (-8.8781e-01)\n",
            "Epoch: [309][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8536e-01 (-8.8793e-01)\n",
            "Training...\n",
            "Epoch: [310][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.9250e-01 (-8.9250e-01)\n",
            "Epoch: [310][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9172e-01 (-8.9292e-01)\n",
            "Epoch: [310][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9505e-01 (-8.8979e-01)\n",
            "Epoch: [310][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9756e-01 (-8.8936e-01)\n",
            "Epoch: [310][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9139e-01 (-8.8896e-01)\n",
            "Epoch: [310][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0558e-01 (-8.8953e-01)\n",
            "Epoch: [310][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8600e-01 (-8.8901e-01)\n",
            "Epoch: [310][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0378e-01 (-8.8946e-01)\n",
            "Epoch: [310][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8977e-01 (-8.8932e-01)\n",
            "Epoch: [310][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8504e-01 (-8.8921e-01)\n",
            "Validating...\n",
            "Top1: 0.8148643092105263\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [311][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.8498e-01 (-8.8498e-01)\n",
            "Epoch: [311][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8742e-01 (-8.8424e-01)\n",
            "Epoch: [311][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9278e-01 (-8.8713e-01)\n",
            "Epoch: [311][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9629e-01 (-8.8840e-01)\n",
            "Epoch: [311][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8142e-01 (-8.8784e-01)\n",
            "Epoch: [311][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8587e-01 (-8.8816e-01)\n",
            "Epoch: [311][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9604e-01 (-8.8817e-01)\n",
            "Epoch: [311][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8100e-01 (-8.8778e-01)\n",
            "Epoch: [311][80/97]\tTime  0.176 ( 0.180)\tLoss -8.9467e-01 (-8.8794e-01)\n",
            "Epoch: [311][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8172e-01 (-8.8814e-01)\n",
            "Training...\n",
            "Epoch: [312][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.7316e-01 (-8.7316e-01)\n",
            "Epoch: [312][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9586e-01 (-8.8918e-01)\n",
            "Epoch: [312][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8976e-01 (-8.8776e-01)\n",
            "Epoch: [312][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8821e-01 (-8.8891e-01)\n",
            "Epoch: [312][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8679e-01 (-8.8906e-01)\n",
            "Epoch: [312][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9804e-01 (-8.8920e-01)\n",
            "Epoch: [312][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8352e-01 (-8.8788e-01)\n",
            "Epoch: [312][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8689e-01 (-8.8729e-01)\n",
            "Epoch: [312][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8340e-01 (-8.8747e-01)\n",
            "Epoch: [312][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7122e-01 (-8.8713e-01)\n",
            "Training...\n",
            "Epoch: [313][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8506e-01 (-8.8506e-01)\n",
            "Epoch: [313][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8966e-01 (-8.9173e-01)\n",
            "Epoch: [313][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9605e-01 (-8.9267e-01)\n",
            "Epoch: [313][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9688e-01 (-8.9238e-01)\n",
            "Epoch: [313][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9848e-01 (-8.9149e-01)\n",
            "Epoch: [313][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9011e-01 (-8.9046e-01)\n",
            "Epoch: [313][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9619e-01 (-8.9051e-01)\n",
            "Epoch: [313][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7242e-01 (-8.9001e-01)\n",
            "Epoch: [313][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9475e-01 (-8.8967e-01)\n",
            "Epoch: [313][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9163e-01 (-8.8952e-01)\n",
            "Training...\n",
            "Epoch: [314][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8800e-01 (-8.8800e-01)\n",
            "Epoch: [314][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8127e-01 (-8.9016e-01)\n",
            "Epoch: [314][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8062e-01 (-8.8886e-01)\n",
            "Epoch: [314][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8482e-01 (-8.8848e-01)\n",
            "Epoch: [314][40/97]\tTime  0.176 ( 0.184)\tLoss -9.0083e-01 (-8.8901e-01)\n",
            "Epoch: [314][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9670e-01 (-8.8883e-01)\n",
            "Epoch: [314][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8812e-01 (-8.8823e-01)\n",
            "Epoch: [314][70/97]\tTime  0.176 ( 0.181)\tLoss -8.7667e-01 (-8.8747e-01)\n",
            "Epoch: [314][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7943e-01 (-8.8713e-01)\n",
            "Epoch: [314][90/97]\tTime  0.176 ( 0.180)\tLoss -8.9381e-01 (-8.8725e-01)\n",
            "Training...\n",
            "Epoch: [315][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.9252e-01 (-8.9252e-01)\n",
            "Epoch: [315][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9793e-01 (-8.8997e-01)\n",
            "Epoch: [315][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8832e-01 (-8.8854e-01)\n",
            "Epoch: [315][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8173e-01 (-8.8743e-01)\n",
            "Epoch: [315][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7812e-01 (-8.8719e-01)\n",
            "Epoch: [315][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8877e-01 (-8.8748e-01)\n",
            "Epoch: [315][60/97]\tTime  0.177 ( 0.181)\tLoss -8.7062e-01 (-8.8703e-01)\n",
            "Epoch: [315][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8029e-01 (-8.8693e-01)\n",
            "Epoch: [315][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9970e-01 (-8.8781e-01)\n",
            "Epoch: [315][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9100e-01 (-8.8815e-01)\n",
            "Validating...\n",
            "Top1: 0.8120888157894737\n",
            "Training...\n",
            "Epoch: [316][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8514e-01 (-8.8514e-01)\n",
            "Epoch: [316][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9001e-01 (-8.8786e-01)\n",
            "Epoch: [316][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8088e-01 (-8.8572e-01)\n",
            "Epoch: [316][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8688e-01 (-8.8602e-01)\n",
            "Epoch: [316][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9648e-01 (-8.8668e-01)\n",
            "Epoch: [316][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8402e-01 (-8.8747e-01)\n",
            "Epoch: [316][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9863e-01 (-8.8821e-01)\n",
            "Epoch: [316][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8941e-01 (-8.8805e-01)\n",
            "Epoch: [316][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0565e-01 (-8.8765e-01)\n",
            "Epoch: [316][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8792e-01 (-8.8743e-01)\n",
            "Training...\n",
            "Epoch: [317][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.8321e-01 (-8.8321e-01)\n",
            "Epoch: [317][10/97]\tTime  0.177 ( 0.202)\tLoss -8.7701e-01 (-8.8698e-01)\n",
            "Epoch: [317][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8680e-01 (-8.8764e-01)\n",
            "Epoch: [317][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8977e-01 (-8.8809e-01)\n",
            "Epoch: [317][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9845e-01 (-8.8970e-01)\n",
            "Epoch: [317][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9164e-01 (-8.8876e-01)\n",
            "Epoch: [317][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8354e-01 (-8.8838e-01)\n",
            "Epoch: [317][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8976e-01 (-8.8725e-01)\n",
            "Epoch: [317][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8700e-01 (-8.8717e-01)\n",
            "Epoch: [317][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8999e-01 (-8.8735e-01)\n",
            "Training...\n",
            "Epoch: [318][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9296e-01 (-8.9296e-01)\n",
            "Epoch: [318][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8814e-01 (-8.8939e-01)\n",
            "Epoch: [318][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9105e-01 (-8.9082e-01)\n",
            "Epoch: [318][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8313e-01 (-8.8839e-01)\n",
            "Epoch: [318][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9043e-01 (-8.8716e-01)\n",
            "Epoch: [318][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9242e-01 (-8.8654e-01)\n",
            "Epoch: [318][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9829e-01 (-8.8646e-01)\n",
            "Epoch: [318][70/97]\tTime  0.176 ( 0.181)\tLoss -8.9499e-01 (-8.8737e-01)\n",
            "Epoch: [318][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8476e-01 (-8.8768e-01)\n",
            "Epoch: [318][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8401e-01 (-8.8738e-01)\n",
            "Training...\n",
            "Epoch: [319][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.6886e-01 (-8.6886e-01)\n",
            "Epoch: [319][10/97]\tTime  0.178 ( 0.202)\tLoss -8.7557e-01 (-8.8504e-01)\n",
            "Epoch: [319][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7472e-01 (-8.8644e-01)\n",
            "Epoch: [319][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8671e-01 (-8.8751e-01)\n",
            "Epoch: [319][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7682e-01 (-8.8748e-01)\n",
            "Epoch: [319][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0066e-01 (-8.8803e-01)\n",
            "Epoch: [319][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8954e-01 (-8.8778e-01)\n",
            "Epoch: [319][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9746e-01 (-8.8847e-01)\n",
            "Epoch: [319][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9545e-01 (-8.8853e-01)\n",
            "Epoch: [319][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9387e-01 (-8.8857e-01)\n",
            "Training...\n",
            "Epoch: [320][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8284e-01 (-8.8284e-01)\n",
            "Epoch: [320][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8923e-01 (-8.8646e-01)\n",
            "Epoch: [320][20/97]\tTime  0.176 ( 0.190)\tLoss -8.9031e-01 (-8.8573e-01)\n",
            "Epoch: [320][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9199e-01 (-8.8657e-01)\n",
            "Epoch: [320][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9063e-01 (-8.8700e-01)\n",
            "Epoch: [320][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8935e-01 (-8.8720e-01)\n",
            "Epoch: [320][60/97]\tTime  0.177 ( 0.181)\tLoss -8.7891e-01 (-8.8689e-01)\n",
            "Epoch: [320][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8682e-01 (-8.8677e-01)\n",
            "Epoch: [320][80/97]\tTime  0.176 ( 0.180)\tLoss -8.8117e-01 (-8.8648e-01)\n",
            "Epoch: [320][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7757e-01 (-8.8640e-01)\n",
            "Validating...\n",
            "Top1: 0.8173314144736842\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [321][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.8150e-01 (-8.8150e-01)\n",
            "Epoch: [321][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8737e-01 (-8.9003e-01)\n",
            "Epoch: [321][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8641e-01 (-8.8854e-01)\n",
            "Epoch: [321][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9682e-01 (-8.8971e-01)\n",
            "Epoch: [321][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8782e-01 (-8.8960e-01)\n",
            "Epoch: [321][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7901e-01 (-8.8899e-01)\n",
            "Epoch: [321][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8943e-01 (-8.8904e-01)\n",
            "Epoch: [321][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8935e-01 (-8.8931e-01)\n",
            "Epoch: [321][80/97]\tTime  0.178 ( 0.180)\tLoss -8.9629e-01 (-8.8937e-01)\n",
            "Epoch: [321][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9340e-01 (-8.8959e-01)\n",
            "Training...\n",
            "Epoch: [322][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.8807e-01 (-8.8807e-01)\n",
            "Epoch: [322][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9411e-01 (-8.9145e-01)\n",
            "Epoch: [322][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7931e-01 (-8.9017e-01)\n",
            "Epoch: [322][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8870e-01 (-8.9036e-01)\n",
            "Epoch: [322][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9244e-01 (-8.8994e-01)\n",
            "Epoch: [322][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9334e-01 (-8.8988e-01)\n",
            "Epoch: [322][60/97]\tTime  0.176 ( 0.181)\tLoss -8.9718e-01 (-8.9054e-01)\n",
            "Epoch: [322][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7913e-01 (-8.8984e-01)\n",
            "Epoch: [322][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8480e-01 (-8.8917e-01)\n",
            "Epoch: [322][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9722e-01 (-8.8922e-01)\n",
            "Training...\n",
            "Epoch: [323][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9019e-01 (-8.9019e-01)\n",
            "Epoch: [323][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8677e-01 (-8.8898e-01)\n",
            "Epoch: [323][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9276e-01 (-8.8790e-01)\n",
            "Epoch: [323][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8926e-01 (-8.8725e-01)\n",
            "Epoch: [323][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9425e-01 (-8.8786e-01)\n",
            "Epoch: [323][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8009e-01 (-8.8803e-01)\n",
            "Epoch: [323][60/97]\tTime  0.177 ( 0.181)\tLoss -8.7174e-01 (-8.8753e-01)\n",
            "Epoch: [323][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8833e-01 (-8.8746e-01)\n",
            "Epoch: [323][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8598e-01 (-8.8768e-01)\n",
            "Epoch: [323][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8965e-01 (-8.8792e-01)\n",
            "Training...\n",
            "Epoch: [324][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9836e-01 (-8.9836e-01)\n",
            "Epoch: [324][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0111e-01 (-8.9199e-01)\n",
            "Epoch: [324][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8084e-01 (-8.9082e-01)\n",
            "Epoch: [324][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8797e-01 (-8.8973e-01)\n",
            "Epoch: [324][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9779e-01 (-8.9035e-01)\n",
            "Epoch: [324][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9580e-01 (-8.9004e-01)\n",
            "Epoch: [324][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9539e-01 (-8.9025e-01)\n",
            "Epoch: [324][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9180e-01 (-8.9082e-01)\n",
            "Epoch: [324][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7879e-01 (-8.9066e-01)\n",
            "Epoch: [324][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9186e-01 (-8.9015e-01)\n",
            "Training...\n",
            "Epoch: [325][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9870e-01 (-8.9870e-01)\n",
            "Epoch: [325][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0238e-01 (-8.8937e-01)\n",
            "Epoch: [325][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8675e-01 (-8.9143e-01)\n",
            "Epoch: [325][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7481e-01 (-8.9172e-01)\n",
            "Epoch: [325][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9283e-01 (-8.9067e-01)\n",
            "Epoch: [325][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7671e-01 (-8.9010e-01)\n",
            "Epoch: [325][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9140e-01 (-8.8954e-01)\n",
            "Epoch: [325][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8826e-01 (-8.8949e-01)\n",
            "Epoch: [325][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9131e-01 (-8.8921e-01)\n",
            "Epoch: [325][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7875e-01 (-8.8877e-01)\n",
            "Validating...\n",
            "Top1: 0.8053042763157895\n",
            "Training...\n",
            "Epoch: [326][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8429e-01 (-8.8429e-01)\n",
            "Epoch: [326][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9805e-01 (-8.8992e-01)\n",
            "Epoch: [326][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8531e-01 (-8.9121e-01)\n",
            "Epoch: [326][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9076e-01 (-8.8962e-01)\n",
            "Epoch: [326][40/97]\tTime  0.176 ( 0.183)\tLoss -8.9086e-01 (-8.8932e-01)\n",
            "Epoch: [326][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7497e-01 (-8.8887e-01)\n",
            "Epoch: [326][60/97]\tTime  0.176 ( 0.181)\tLoss -8.9445e-01 (-8.8854e-01)\n",
            "Epoch: [326][70/97]\tTime  0.176 ( 0.181)\tLoss -8.7392e-01 (-8.8859e-01)\n",
            "Epoch: [326][80/97]\tTime  0.176 ( 0.180)\tLoss -8.8855e-01 (-8.8852e-01)\n",
            "Epoch: [326][90/97]\tTime  0.176 ( 0.180)\tLoss -8.8203e-01 (-8.8858e-01)\n",
            "Training...\n",
            "Epoch: [327][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8987e-01 (-8.8987e-01)\n",
            "Epoch: [327][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9505e-01 (-8.8924e-01)\n",
            "Epoch: [327][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9078e-01 (-8.8937e-01)\n",
            "Epoch: [327][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9656e-01 (-8.8951e-01)\n",
            "Epoch: [327][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8841e-01 (-8.8943e-01)\n",
            "Epoch: [327][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8011e-01 (-8.8928e-01)\n",
            "Epoch: [327][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9985e-01 (-8.8921e-01)\n",
            "Epoch: [327][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8515e-01 (-8.8921e-01)\n",
            "Epoch: [327][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9951e-01 (-8.8917e-01)\n",
            "Epoch: [327][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8737e-01 (-8.8897e-01)\n",
            "Training...\n",
            "Epoch: [328][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.9097e-01 (-8.9097e-01)\n",
            "Epoch: [328][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0419e-01 (-8.9712e-01)\n",
            "Epoch: [328][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9876e-01 (-8.9503e-01)\n",
            "Epoch: [328][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9266e-01 (-8.9331e-01)\n",
            "Epoch: [328][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9648e-01 (-8.9242e-01)\n",
            "Epoch: [328][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9410e-01 (-8.9231e-01)\n",
            "Epoch: [328][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9347e-01 (-8.9209e-01)\n",
            "Epoch: [328][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8036e-01 (-8.9172e-01)\n",
            "Epoch: [328][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9191e-01 (-8.9103e-01)\n",
            "Epoch: [328][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9372e-01 (-8.9085e-01)\n",
            "Training...\n",
            "Epoch: [329][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8135e-01 (-8.8135e-01)\n",
            "Epoch: [329][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8424e-01 (-8.8920e-01)\n",
            "Epoch: [329][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8842e-01 (-8.9127e-01)\n",
            "Epoch: [329][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9274e-01 (-8.9167e-01)\n",
            "Epoch: [329][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8210e-01 (-8.9148e-01)\n",
            "Epoch: [329][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0008e-01 (-8.9138e-01)\n",
            "Epoch: [329][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8503e-01 (-8.9030e-01)\n",
            "Epoch: [329][70/97]\tTime  0.177 ( 0.181)\tLoss -8.7596e-01 (-8.9021e-01)\n",
            "Epoch: [329][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0643e-01 (-8.8996e-01)\n",
            "Epoch: [329][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8257e-01 (-8.8986e-01)\n",
            "Training...\n",
            "Epoch: [330][ 0/97]\tTime  0.459 ( 0.459)\tLoss -8.8676e-01 (-8.8676e-01)\n",
            "Epoch: [330][10/97]\tTime  0.176 ( 0.202)\tLoss -8.8248e-01 (-8.8794e-01)\n",
            "Epoch: [330][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8869e-01 (-8.8980e-01)\n",
            "Epoch: [330][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9231e-01 (-8.8925e-01)\n",
            "Epoch: [330][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8182e-01 (-8.8884e-01)\n",
            "Epoch: [330][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8292e-01 (-8.8904e-01)\n",
            "Epoch: [330][60/97]\tTime  0.178 ( 0.181)\tLoss -8.8431e-01 (-8.8999e-01)\n",
            "Epoch: [330][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8800e-01 (-8.8948e-01)\n",
            "Epoch: [330][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8489e-01 (-8.8872e-01)\n",
            "Epoch: [330][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9048e-01 (-8.8902e-01)\n",
            "Validating...\n",
            "Top1: 0.8224712171052632\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [331][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.9412e-01 (-8.9412e-01)\n",
            "Epoch: [331][10/97]\tTime  0.176 ( 0.201)\tLoss -8.8211e-01 (-8.9384e-01)\n",
            "Epoch: [331][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9091e-01 (-8.9091e-01)\n",
            "Epoch: [331][30/97]\tTime  0.176 ( 0.185)\tLoss -8.8912e-01 (-8.9069e-01)\n",
            "Epoch: [331][40/97]\tTime  0.177 ( 0.183)\tLoss -8.7830e-01 (-8.9004e-01)\n",
            "Epoch: [331][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9414e-01 (-8.8875e-01)\n",
            "Epoch: [331][60/97]\tTime  0.177 ( 0.181)\tLoss -8.7500e-01 (-8.8792e-01)\n",
            "Epoch: [331][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8982e-01 (-8.8783e-01)\n",
            "Epoch: [331][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8972e-01 (-8.8784e-01)\n",
            "Epoch: [331][90/97]\tTime  0.176 ( 0.180)\tLoss -8.8435e-01 (-8.8772e-01)\n",
            "Training...\n",
            "Epoch: [332][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8869e-01 (-8.8869e-01)\n",
            "Epoch: [332][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9079e-01 (-8.9072e-01)\n",
            "Epoch: [332][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8299e-01 (-8.9129e-01)\n",
            "Epoch: [332][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8350e-01 (-8.9052e-01)\n",
            "Epoch: [332][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8092e-01 (-8.8979e-01)\n",
            "Epoch: [332][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9678e-01 (-8.8991e-01)\n",
            "Epoch: [332][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9560e-01 (-8.8957e-01)\n",
            "Epoch: [332][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8946e-01 (-8.8911e-01)\n",
            "Epoch: [332][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8663e-01 (-8.8914e-01)\n",
            "Epoch: [332][90/97]\tTime  0.176 ( 0.180)\tLoss -8.8602e-01 (-8.8911e-01)\n",
            "Training...\n",
            "Epoch: [333][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9522e-01 (-8.9522e-01)\n",
            "Epoch: [333][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8727e-01 (-8.8675e-01)\n",
            "Epoch: [333][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9634e-01 (-8.8879e-01)\n",
            "Epoch: [333][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9118e-01 (-8.8994e-01)\n",
            "Epoch: [333][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8825e-01 (-8.9048e-01)\n",
            "Epoch: [333][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8965e-01 (-8.9003e-01)\n",
            "Epoch: [333][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8579e-01 (-8.8941e-01)\n",
            "Epoch: [333][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8464e-01 (-8.8927e-01)\n",
            "Epoch: [333][80/97]\tTime  0.176 ( 0.180)\tLoss -8.9455e-01 (-8.8893e-01)\n",
            "Epoch: [333][90/97]\tTime  0.176 ( 0.180)\tLoss -8.9978e-01 (-8.8892e-01)\n",
            "Training...\n",
            "Epoch: [334][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8940e-01 (-8.8940e-01)\n",
            "Epoch: [334][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8641e-01 (-8.8842e-01)\n",
            "Epoch: [334][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9194e-01 (-8.8884e-01)\n",
            "Epoch: [334][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9775e-01 (-8.9050e-01)\n",
            "Epoch: [334][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9544e-01 (-8.9056e-01)\n",
            "Epoch: [334][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0127e-01 (-8.8989e-01)\n",
            "Epoch: [334][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9441e-01 (-8.8938e-01)\n",
            "Epoch: [334][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8426e-01 (-8.8941e-01)\n",
            "Epoch: [334][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8563e-01 (-8.8964e-01)\n",
            "Epoch: [334][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9067e-01 (-8.8948e-01)\n",
            "Training...\n",
            "Epoch: [335][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8505e-01 (-8.8505e-01)\n",
            "Epoch: [335][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8820e-01 (-8.8902e-01)\n",
            "Epoch: [335][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8704e-01 (-8.8766e-01)\n",
            "Epoch: [335][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8832e-01 (-8.8736e-01)\n",
            "Epoch: [335][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8551e-01 (-8.8724e-01)\n",
            "Epoch: [335][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9452e-01 (-8.8767e-01)\n",
            "Epoch: [335][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9127e-01 (-8.8796e-01)\n",
            "Epoch: [335][70/97]\tTime  0.176 ( 0.181)\tLoss -8.8369e-01 (-8.8763e-01)\n",
            "Epoch: [335][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8219e-01 (-8.8742e-01)\n",
            "Epoch: [335][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8735e-01 (-8.8744e-01)\n",
            "Validating...\n",
            "Top1: 0.809313322368421\n",
            "Training...\n",
            "Epoch: [336][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.9249e-01 (-8.9249e-01)\n",
            "Epoch: [336][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8562e-01 (-8.9636e-01)\n",
            "Epoch: [336][20/97]\tTime  0.176 ( 0.190)\tLoss -8.8969e-01 (-8.9249e-01)\n",
            "Epoch: [336][30/97]\tTime  0.177 ( 0.185)\tLoss -8.9561e-01 (-8.9270e-01)\n",
            "Epoch: [336][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8471e-01 (-8.9285e-01)\n",
            "Epoch: [336][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9621e-01 (-8.9336e-01)\n",
            "Epoch: [336][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8756e-01 (-8.9325e-01)\n",
            "Epoch: [336][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9028e-01 (-8.9349e-01)\n",
            "Epoch: [336][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7962e-01 (-8.9307e-01)\n",
            "Epoch: [336][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9641e-01 (-8.9285e-01)\n",
            "Training...\n",
            "Epoch: [337][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8502e-01 (-8.8502e-01)\n",
            "Epoch: [337][10/97]\tTime  0.176 ( 0.202)\tLoss -8.9617e-01 (-8.9286e-01)\n",
            "Epoch: [337][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8894e-01 (-8.9109e-01)\n",
            "Epoch: [337][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9879e-01 (-8.8914e-01)\n",
            "Epoch: [337][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8896e-01 (-8.8994e-01)\n",
            "Epoch: [337][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8820e-01 (-8.8946e-01)\n",
            "Epoch: [337][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9524e-01 (-8.8925e-01)\n",
            "Epoch: [337][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8615e-01 (-8.9032e-01)\n",
            "Epoch: [337][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9294e-01 (-8.9089e-01)\n",
            "Epoch: [337][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8001e-01 (-8.9115e-01)\n",
            "Training...\n",
            "Epoch: [338][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8263e-01 (-8.8263e-01)\n",
            "Epoch: [338][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9088e-01 (-8.8903e-01)\n",
            "Epoch: [338][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9784e-01 (-8.9096e-01)\n",
            "Epoch: [338][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7666e-01 (-8.8987e-01)\n",
            "Epoch: [338][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9228e-01 (-8.9016e-01)\n",
            "Epoch: [338][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0161e-01 (-8.9004e-01)\n",
            "Epoch: [338][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0255e-01 (-8.8946e-01)\n",
            "Epoch: [338][70/97]\tTime  0.176 ( 0.181)\tLoss -8.9476e-01 (-8.8925e-01)\n",
            "Epoch: [338][80/97]\tTime  0.176 ( 0.180)\tLoss -8.7317e-01 (-8.8894e-01)\n",
            "Epoch: [338][90/97]\tTime  0.176 ( 0.180)\tLoss -8.8767e-01 (-8.8879e-01)\n",
            "Training...\n",
            "Epoch: [339][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.8062e-01 (-8.8062e-01)\n",
            "Epoch: [339][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9712e-01 (-8.9466e-01)\n",
            "Epoch: [339][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9312e-01 (-8.9389e-01)\n",
            "Epoch: [339][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8364e-01 (-8.9257e-01)\n",
            "Epoch: [339][40/97]\tTime  0.177 ( 0.184)\tLoss -8.7930e-01 (-8.9195e-01)\n",
            "Epoch: [339][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9666e-01 (-8.9175e-01)\n",
            "Epoch: [339][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8947e-01 (-8.9109e-01)\n",
            "Epoch: [339][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8283e-01 (-8.9095e-01)\n",
            "Epoch: [339][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8996e-01 (-8.9071e-01)\n",
            "Epoch: [339][90/97]\tTime  0.176 ( 0.180)\tLoss -8.7526e-01 (-8.9042e-01)\n",
            "Training...\n",
            "Epoch: [340][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.8643e-01 (-8.8643e-01)\n",
            "Epoch: [340][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9212e-01 (-8.8823e-01)\n",
            "Epoch: [340][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8673e-01 (-8.8802e-01)\n",
            "Epoch: [340][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8949e-01 (-8.8834e-01)\n",
            "Epoch: [340][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9219e-01 (-8.8969e-01)\n",
            "Epoch: [340][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7893e-01 (-8.8972e-01)\n",
            "Epoch: [340][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9498e-01 (-8.8858e-01)\n",
            "Epoch: [340][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8177e-01 (-8.8811e-01)\n",
            "Epoch: [340][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0063e-01 (-8.8852e-01)\n",
            "Epoch: [340][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8821e-01 (-8.8867e-01)\n",
            "Validating...\n",
            "Top1: 0.819592927631579\n",
            "Training...\n",
            "Epoch: [341][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8948e-01 (-8.8948e-01)\n",
            "Epoch: [341][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9036e-01 (-8.9404e-01)\n",
            "Epoch: [341][20/97]\tTime  0.177 ( 0.192)\tLoss -8.8636e-01 (-8.9272e-01)\n",
            "Epoch: [341][30/97]\tTime  0.177 ( 0.187)\tLoss -8.9756e-01 (-8.9155e-01)\n",
            "Epoch: [341][40/97]\tTime  0.177 ( 0.185)\tLoss -8.9015e-01 (-8.9119e-01)\n",
            "Epoch: [341][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8198e-01 (-8.9063e-01)\n",
            "Epoch: [341][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8606e-01 (-8.9048e-01)\n",
            "Epoch: [341][70/97]\tTime  0.177 ( 0.182)\tLoss -8.7565e-01 (-8.8986e-01)\n",
            "Epoch: [341][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8422e-01 (-8.8985e-01)\n",
            "Epoch: [341][90/97]\tTime  0.177 ( 0.181)\tLoss -8.9820e-01 (-8.8979e-01)\n",
            "Training...\n",
            "Epoch: [342][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8913e-01 (-8.8913e-01)\n",
            "Epoch: [342][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9602e-01 (-8.9233e-01)\n",
            "Epoch: [342][20/97]\tTime  0.177 ( 0.190)\tLoss -8.7769e-01 (-8.9063e-01)\n",
            "Epoch: [342][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7945e-01 (-8.9041e-01)\n",
            "Epoch: [342][40/97]\tTime  0.178 ( 0.184)\tLoss -8.8726e-01 (-8.9021e-01)\n",
            "Epoch: [342][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9827e-01 (-8.9010e-01)\n",
            "Epoch: [342][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9873e-01 (-8.8965e-01)\n",
            "Epoch: [342][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8947e-01 (-8.9018e-01)\n",
            "Epoch: [342][80/97]\tTime  0.178 ( 0.181)\tLoss -8.8277e-01 (-8.9020e-01)\n",
            "Epoch: [342][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8302e-01 (-8.8999e-01)\n",
            "Training...\n",
            "Epoch: [343][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.0202e-01 (-9.0202e-01)\n",
            "Epoch: [343][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8844e-01 (-8.9204e-01)\n",
            "Epoch: [343][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9815e-01 (-8.9232e-01)\n",
            "Epoch: [343][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8935e-01 (-8.9207e-01)\n",
            "Epoch: [343][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9741e-01 (-8.9167e-01)\n",
            "Epoch: [343][50/97]\tTime  0.176 ( 0.183)\tLoss -8.9254e-01 (-8.9200e-01)\n",
            "Epoch: [343][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8578e-01 (-8.9232e-01)\n",
            "Epoch: [343][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9718e-01 (-8.9179e-01)\n",
            "Epoch: [343][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7907e-01 (-8.9170e-01)\n",
            "Epoch: [343][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9177e-01 (-8.9168e-01)\n",
            "Training...\n",
            "Epoch: [344][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.8831e-01 (-8.8831e-01)\n",
            "Epoch: [344][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9360e-01 (-8.9391e-01)\n",
            "Epoch: [344][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0205e-01 (-8.9426e-01)\n",
            "Epoch: [344][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9145e-01 (-8.9354e-01)\n",
            "Epoch: [344][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9686e-01 (-8.9270e-01)\n",
            "Epoch: [344][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9712e-01 (-8.9273e-01)\n",
            "Epoch: [344][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9406e-01 (-8.9286e-01)\n",
            "Epoch: [344][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8971e-01 (-8.9299e-01)\n",
            "Epoch: [344][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9022e-01 (-8.9242e-01)\n",
            "Epoch: [344][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8789e-01 (-8.9209e-01)\n",
            "Training...\n",
            "Epoch: [345][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.7012e-01 (-8.7012e-01)\n",
            "Epoch: [345][10/97]\tTime  0.176 ( 0.202)\tLoss -8.9336e-01 (-8.9049e-01)\n",
            "Epoch: [345][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9302e-01 (-8.9198e-01)\n",
            "Epoch: [345][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9326e-01 (-8.9320e-01)\n",
            "Epoch: [345][40/97]\tTime  0.176 ( 0.184)\tLoss -8.9412e-01 (-8.9130e-01)\n",
            "Epoch: [345][50/97]\tTime  0.177 ( 0.182)\tLoss -8.7417e-01 (-8.9006e-01)\n",
            "Epoch: [345][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0055e-01 (-8.9088e-01)\n",
            "Epoch: [345][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0048e-01 (-8.9146e-01)\n",
            "Epoch: [345][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9059e-01 (-8.9127e-01)\n",
            "Epoch: [345][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9632e-01 (-8.9083e-01)\n",
            "Validating...\n",
            "Top1: 0.8194901315789473\n",
            "Training...\n",
            "Epoch: [346][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8276e-01 (-8.8276e-01)\n",
            "Epoch: [346][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8451e-01 (-8.9288e-01)\n",
            "Epoch: [346][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9142e-01 (-8.9080e-01)\n",
            "Epoch: [346][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9025e-01 (-8.9030e-01)\n",
            "Epoch: [346][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0433e-01 (-8.9054e-01)\n",
            "Epoch: [346][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9719e-01 (-8.8993e-01)\n",
            "Epoch: [346][60/97]\tTime  0.178 ( 0.181)\tLoss -8.8866e-01 (-8.8975e-01)\n",
            "Epoch: [346][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0535e-01 (-8.8993e-01)\n",
            "Epoch: [346][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8512e-01 (-8.9007e-01)\n",
            "Epoch: [346][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8014e-01 (-8.8972e-01)\n",
            "Training...\n",
            "Epoch: [347][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.8978e-01 (-8.8978e-01)\n",
            "Epoch: [347][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9073e-01 (-8.9391e-01)\n",
            "Epoch: [347][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8827e-01 (-8.9061e-01)\n",
            "Epoch: [347][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8811e-01 (-8.9073e-01)\n",
            "Epoch: [347][40/97]\tTime  0.177 ( 0.183)\tLoss -8.7940e-01 (-8.9015e-01)\n",
            "Epoch: [347][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8128e-01 (-8.9004e-01)\n",
            "Epoch: [347][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8583e-01 (-8.8891e-01)\n",
            "Epoch: [347][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8762e-01 (-8.8908e-01)\n",
            "Epoch: [347][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7605e-01 (-8.8881e-01)\n",
            "Epoch: [347][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8142e-01 (-8.8907e-01)\n",
            "Training...\n",
            "Epoch: [348][ 0/97]\tTime  0.458 ( 0.458)\tLoss -9.0026e-01 (-9.0026e-01)\n",
            "Epoch: [348][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0002e-01 (-8.9444e-01)\n",
            "Epoch: [348][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9948e-01 (-8.9174e-01)\n",
            "Epoch: [348][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7154e-01 (-8.9013e-01)\n",
            "Epoch: [348][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0045e-01 (-8.9005e-01)\n",
            "Epoch: [348][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9216e-01 (-8.8988e-01)\n",
            "Epoch: [348][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9255e-01 (-8.8961e-01)\n",
            "Epoch: [348][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9357e-01 (-8.8919e-01)\n",
            "Epoch: [348][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8997e-01 (-8.8940e-01)\n",
            "Epoch: [348][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9345e-01 (-8.8992e-01)\n",
            "Training...\n",
            "Epoch: [349][ 0/97]\tTime  0.450 ( 0.450)\tLoss -8.9310e-01 (-8.9310e-01)\n",
            "Epoch: [349][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8664e-01 (-8.9341e-01)\n",
            "Epoch: [349][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9317e-01 (-8.9239e-01)\n",
            "Epoch: [349][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9498e-01 (-8.9154e-01)\n",
            "Epoch: [349][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9699e-01 (-8.9202e-01)\n",
            "Epoch: [349][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0031e-01 (-8.9195e-01)\n",
            "Epoch: [349][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8456e-01 (-8.9151e-01)\n",
            "Epoch: [349][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8488e-01 (-8.9095e-01)\n",
            "Epoch: [349][80/97]\tTime  0.177 ( 0.180)\tLoss -8.7888e-01 (-8.9051e-01)\n",
            "Epoch: [349][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9459e-01 (-8.9077e-01)\n",
            "Training...\n",
            "Epoch: [350][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.9249e-01 (-8.9249e-01)\n",
            "Epoch: [350][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9268e-01 (-8.9211e-01)\n",
            "Epoch: [350][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8102e-01 (-8.8992e-01)\n",
            "Epoch: [350][30/97]\tTime  0.176 ( 0.186)\tLoss -9.0310e-01 (-8.9055e-01)\n",
            "Epoch: [350][40/97]\tTime  0.176 ( 0.184)\tLoss -8.9572e-01 (-8.9191e-01)\n",
            "Epoch: [350][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9185e-01 (-8.9207e-01)\n",
            "Epoch: [350][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8802e-01 (-8.9160e-01)\n",
            "Epoch: [350][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8616e-01 (-8.9122e-01)\n",
            "Epoch: [350][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9263e-01 (-8.9053e-01)\n",
            "Epoch: [350][90/97]\tTime  0.178 ( 0.180)\tLoss -8.8073e-01 (-8.9034e-01)\n",
            "Validating...\n",
            "Top1: 0.8194901315789473\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [351][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.9537e-01 (-8.9537e-01)\n",
            "Epoch: [351][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9925e-01 (-8.9289e-01)\n",
            "Epoch: [351][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0523e-01 (-8.9234e-01)\n",
            "Epoch: [351][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9129e-01 (-8.9113e-01)\n",
            "Epoch: [351][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8813e-01 (-8.9056e-01)\n",
            "Epoch: [351][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9695e-01 (-8.9155e-01)\n",
            "Epoch: [351][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8316e-01 (-8.9155e-01)\n",
            "Epoch: [351][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9732e-01 (-8.9171e-01)\n",
            "Epoch: [351][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8989e-01 (-8.9133e-01)\n",
            "Epoch: [351][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9706e-01 (-8.9158e-01)\n",
            "Training...\n",
            "Epoch: [352][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.7426e-01 (-8.7426e-01)\n",
            "Epoch: [352][10/97]\tTime  0.176 ( 0.201)\tLoss -8.9312e-01 (-8.9093e-01)\n",
            "Epoch: [352][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8518e-01 (-8.8950e-01)\n",
            "Epoch: [352][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9116e-01 (-8.8782e-01)\n",
            "Epoch: [352][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8161e-01 (-8.8817e-01)\n",
            "Epoch: [352][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8673e-01 (-8.8866e-01)\n",
            "Epoch: [352][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9354e-01 (-8.8944e-01)\n",
            "Epoch: [352][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8914e-01 (-8.8945e-01)\n",
            "Epoch: [352][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9243e-01 (-8.8976e-01)\n",
            "Epoch: [352][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9243e-01 (-8.8984e-01)\n",
            "Training...\n",
            "Epoch: [353][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0202e-01 (-9.0202e-01)\n",
            "Epoch: [353][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9555e-01 (-8.9182e-01)\n",
            "Epoch: [353][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9503e-01 (-8.9156e-01)\n",
            "Epoch: [353][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8784e-01 (-8.9110e-01)\n",
            "Epoch: [353][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8981e-01 (-8.9046e-01)\n",
            "Epoch: [353][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0002e-01 (-8.9041e-01)\n",
            "Epoch: [353][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9688e-01 (-8.9086e-01)\n",
            "Epoch: [353][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8298e-01 (-8.9058e-01)\n",
            "Epoch: [353][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8946e-01 (-8.9041e-01)\n",
            "Epoch: [353][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8901e-01 (-8.8984e-01)\n",
            "Training...\n",
            "Epoch: [354][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.9283e-01 (-8.9283e-01)\n",
            "Epoch: [354][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8994e-01 (-8.9039e-01)\n",
            "Epoch: [354][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8702e-01 (-8.9357e-01)\n",
            "Epoch: [354][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9849e-01 (-8.9432e-01)\n",
            "Epoch: [354][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0083e-01 (-8.9410e-01)\n",
            "Epoch: [354][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0348e-01 (-8.9505e-01)\n",
            "Epoch: [354][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8533e-01 (-8.9502e-01)\n",
            "Epoch: [354][70/97]\tTime  0.176 ( 0.181)\tLoss -8.8167e-01 (-8.9402e-01)\n",
            "Epoch: [354][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8316e-01 (-8.9374e-01)\n",
            "Epoch: [354][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9398e-01 (-8.9328e-01)\n",
            "Training...\n",
            "Epoch: [355][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.9420e-01 (-8.9420e-01)\n",
            "Epoch: [355][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8553e-01 (-8.9128e-01)\n",
            "Epoch: [355][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9586e-01 (-8.9277e-01)\n",
            "Epoch: [355][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9127e-01 (-8.9312e-01)\n",
            "Epoch: [355][40/97]\tTime  0.178 ( 0.183)\tLoss -8.9889e-01 (-8.9377e-01)\n",
            "Epoch: [355][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0667e-01 (-8.9457e-01)\n",
            "Epoch: [355][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0069e-01 (-8.9470e-01)\n",
            "Epoch: [355][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9678e-01 (-8.9388e-01)\n",
            "Epoch: [355][80/97]\tTime  0.178 ( 0.180)\tLoss -8.8920e-01 (-8.9343e-01)\n",
            "Epoch: [355][90/97]\tTime  0.177 ( 0.180)\tLoss -8.7823e-01 (-8.9349e-01)\n",
            "Validating...\n",
            "Top1: 0.8196957236842105\n",
            "Training...\n",
            "Epoch: [356][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9751e-01 (-8.9751e-01)\n",
            "Epoch: [356][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8727e-01 (-8.9350e-01)\n",
            "Epoch: [356][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9683e-01 (-8.9268e-01)\n",
            "Epoch: [356][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9062e-01 (-8.9326e-01)\n",
            "Epoch: [356][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9522e-01 (-8.9096e-01)\n",
            "Epoch: [356][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9245e-01 (-8.9181e-01)\n",
            "Epoch: [356][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8825e-01 (-8.9225e-01)\n",
            "Epoch: [356][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8749e-01 (-8.9164e-01)\n",
            "Epoch: [356][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9605e-01 (-8.9153e-01)\n",
            "Epoch: [356][90/97]\tTime  0.177 ( 0.180)\tLoss -8.6880e-01 (-8.9135e-01)\n",
            "Training...\n",
            "Epoch: [357][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9047e-01 (-8.9047e-01)\n",
            "Epoch: [357][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9510e-01 (-8.9168e-01)\n",
            "Epoch: [357][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8527e-01 (-8.9137e-01)\n",
            "Epoch: [357][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9244e-01 (-8.9098e-01)\n",
            "Epoch: [357][40/97]\tTime  0.176 ( 0.183)\tLoss -9.0283e-01 (-8.9136e-01)\n",
            "Epoch: [357][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8725e-01 (-8.9177e-01)\n",
            "Epoch: [357][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9827e-01 (-8.9222e-01)\n",
            "Epoch: [357][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9596e-01 (-8.9192e-01)\n",
            "Epoch: [357][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8959e-01 (-8.9155e-01)\n",
            "Epoch: [357][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9102e-01 (-8.9134e-01)\n",
            "Training...\n",
            "Epoch: [358][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.7721e-01 (-8.7721e-01)\n",
            "Epoch: [358][10/97]\tTime  0.176 ( 0.202)\tLoss -8.9919e-01 (-8.9218e-01)\n",
            "Epoch: [358][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0138e-01 (-8.9385e-01)\n",
            "Epoch: [358][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8916e-01 (-8.9413e-01)\n",
            "Epoch: [358][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9449e-01 (-8.9544e-01)\n",
            "Epoch: [358][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8528e-01 (-8.9528e-01)\n",
            "Epoch: [358][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8921e-01 (-8.9464e-01)\n",
            "Epoch: [358][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8036e-01 (-8.9398e-01)\n",
            "Epoch: [358][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9608e-01 (-8.9333e-01)\n",
            "Epoch: [358][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8416e-01 (-8.9307e-01)\n",
            "Training...\n",
            "Epoch: [359][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9921e-01 (-8.9921e-01)\n",
            "Epoch: [359][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8464e-01 (-8.9601e-01)\n",
            "Epoch: [359][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8701e-01 (-8.9361e-01)\n",
            "Epoch: [359][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8677e-01 (-8.9304e-01)\n",
            "Epoch: [359][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9215e-01 (-8.9187e-01)\n",
            "Epoch: [359][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9463e-01 (-8.9172e-01)\n",
            "Epoch: [359][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0475e-01 (-8.9211e-01)\n",
            "Epoch: [359][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9991e-01 (-8.9228e-01)\n",
            "Epoch: [359][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0367e-01 (-8.9250e-01)\n",
            "Epoch: [359][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8507e-01 (-8.9249e-01)\n",
            "Training...\n",
            "Epoch: [360][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.9937e-01 (-8.9937e-01)\n",
            "Epoch: [360][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9919e-01 (-8.9379e-01)\n",
            "Epoch: [360][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9165e-01 (-8.9342e-01)\n",
            "Epoch: [360][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8643e-01 (-8.9215e-01)\n",
            "Epoch: [360][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8286e-01 (-8.9092e-01)\n",
            "Epoch: [360][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8806e-01 (-8.9158e-01)\n",
            "Epoch: [360][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9299e-01 (-8.9147e-01)\n",
            "Epoch: [360][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9693e-01 (-8.9176e-01)\n",
            "Epoch: [360][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9864e-01 (-8.9193e-01)\n",
            "Epoch: [360][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9608e-01 (-8.9224e-01)\n",
            "Validating...\n",
            "Top1: 0.821546052631579\n",
            "Training...\n",
            "Epoch: [361][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.9458e-01 (-8.9458e-01)\n",
            "Epoch: [361][10/97]\tTime  0.176 ( 0.201)\tLoss -9.0427e-01 (-8.9554e-01)\n",
            "Epoch: [361][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0218e-01 (-8.9449e-01)\n",
            "Epoch: [361][30/97]\tTime  0.177 ( 0.185)\tLoss -8.9259e-01 (-8.9468e-01)\n",
            "Epoch: [361][40/97]\tTime  0.177 ( 0.183)\tLoss -8.8743e-01 (-8.9381e-01)\n",
            "Epoch: [361][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8835e-01 (-8.9282e-01)\n",
            "Epoch: [361][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8577e-01 (-8.9307e-01)\n",
            "Epoch: [361][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9765e-01 (-8.9346e-01)\n",
            "Epoch: [361][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8915e-01 (-8.9373e-01)\n",
            "Epoch: [361][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9454e-01 (-8.9337e-01)\n",
            "Training...\n",
            "Epoch: [362][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.9363e-01 (-8.9363e-01)\n",
            "Epoch: [362][10/97]\tTime  0.178 ( 0.201)\tLoss -9.0787e-01 (-8.9615e-01)\n",
            "Epoch: [362][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9661e-01 (-8.9448e-01)\n",
            "Epoch: [362][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9707e-01 (-8.9373e-01)\n",
            "Epoch: [362][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9383e-01 (-8.9370e-01)\n",
            "Epoch: [362][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9309e-01 (-8.9371e-01)\n",
            "Epoch: [362][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9589e-01 (-8.9354e-01)\n",
            "Epoch: [362][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9999e-01 (-8.9377e-01)\n",
            "Epoch: [362][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0094e-01 (-8.9404e-01)\n",
            "Epoch: [362][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9102e-01 (-8.9412e-01)\n",
            "Training...\n",
            "Epoch: [363][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0544e-01 (-9.0544e-01)\n",
            "Epoch: [363][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0004e-01 (-8.9364e-01)\n",
            "Epoch: [363][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0230e-01 (-8.9404e-01)\n",
            "Epoch: [363][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9134e-01 (-8.9429e-01)\n",
            "Epoch: [363][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0133e-01 (-8.9521e-01)\n",
            "Epoch: [363][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9800e-01 (-8.9555e-01)\n",
            "Epoch: [363][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8830e-01 (-8.9528e-01)\n",
            "Epoch: [363][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8948e-01 (-8.9418e-01)\n",
            "Epoch: [363][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9241e-01 (-8.9395e-01)\n",
            "Epoch: [363][90/97]\tTime  0.176 ( 0.180)\tLoss -8.9627e-01 (-8.9386e-01)\n",
            "Training...\n",
            "Epoch: [364][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9532e-01 (-8.9532e-01)\n",
            "Epoch: [364][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8832e-01 (-8.9435e-01)\n",
            "Epoch: [364][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8994e-01 (-8.9277e-01)\n",
            "Epoch: [364][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0123e-01 (-8.9276e-01)\n",
            "Epoch: [364][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9733e-01 (-8.9387e-01)\n",
            "Epoch: [364][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9824e-01 (-8.9458e-01)\n",
            "Epoch: [364][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9378e-01 (-8.9491e-01)\n",
            "Epoch: [364][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8612e-01 (-8.9529e-01)\n",
            "Epoch: [364][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9467e-01 (-8.9546e-01)\n",
            "Epoch: [364][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8095e-01 (-8.9494e-01)\n",
            "Training...\n",
            "Epoch: [365][ 0/97]\tTime  0.458 ( 0.458)\tLoss -8.8899e-01 (-8.8899e-01)\n",
            "Epoch: [365][10/97]\tTime  0.178 ( 0.203)\tLoss -9.0745e-01 (-8.9435e-01)\n",
            "Epoch: [365][20/97]\tTime  0.176 ( 0.190)\tLoss -9.0304e-01 (-8.9579e-01)\n",
            "Epoch: [365][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9629e-01 (-8.9574e-01)\n",
            "Epoch: [365][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8311e-01 (-8.9425e-01)\n",
            "Epoch: [365][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9721e-01 (-8.9407e-01)\n",
            "Epoch: [365][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9150e-01 (-8.9382e-01)\n",
            "Epoch: [365][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9432e-01 (-8.9350e-01)\n",
            "Epoch: [365][80/97]\tTime  0.177 ( 0.181)\tLoss -8.8727e-01 (-8.9295e-01)\n",
            "Epoch: [365][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8506e-01 (-8.9289e-01)\n",
            "Validating...\n",
            "Top1: 0.823499177631579\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [366][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9584e-01 (-8.9584e-01)\n",
            "Epoch: [366][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0134e-01 (-8.9820e-01)\n",
            "Epoch: [366][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9389e-01 (-8.9506e-01)\n",
            "Epoch: [366][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9406e-01 (-8.9466e-01)\n",
            "Epoch: [366][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0336e-01 (-8.9455e-01)\n",
            "Epoch: [366][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9299e-01 (-8.9507e-01)\n",
            "Epoch: [366][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9608e-01 (-8.9561e-01)\n",
            "Epoch: [366][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9795e-01 (-8.9536e-01)\n",
            "Epoch: [366][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9003e-01 (-8.9524e-01)\n",
            "Epoch: [366][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8857e-01 (-8.9522e-01)\n",
            "Training...\n",
            "Epoch: [367][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.9157e-01 (-8.9157e-01)\n",
            "Epoch: [367][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9309e-01 (-8.9725e-01)\n",
            "Epoch: [367][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9478e-01 (-8.9518e-01)\n",
            "Epoch: [367][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8955e-01 (-8.9458e-01)\n",
            "Epoch: [367][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9830e-01 (-8.9473e-01)\n",
            "Epoch: [367][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9628e-01 (-8.9460e-01)\n",
            "Epoch: [367][60/97]\tTime  0.177 ( 0.182)\tLoss -8.7689e-01 (-8.9398e-01)\n",
            "Epoch: [367][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9829e-01 (-8.9420e-01)\n",
            "Epoch: [367][80/97]\tTime  0.178 ( 0.180)\tLoss -8.9932e-01 (-8.9400e-01)\n",
            "Epoch: [367][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8811e-01 (-8.9361e-01)\n",
            "Training...\n",
            "Epoch: [368][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8856e-01 (-8.8856e-01)\n",
            "Epoch: [368][10/97]\tTime  0.178 ( 0.202)\tLoss -8.8576e-01 (-8.9318e-01)\n",
            "Epoch: [368][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9081e-01 (-8.9229e-01)\n",
            "Epoch: [368][30/97]\tTime  0.176 ( 0.186)\tLoss -8.8331e-01 (-8.9265e-01)\n",
            "Epoch: [368][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9782e-01 (-8.9330e-01)\n",
            "Epoch: [368][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0581e-01 (-8.9395e-01)\n",
            "Epoch: [368][60/97]\tTime  0.178 ( 0.181)\tLoss -8.9555e-01 (-8.9425e-01)\n",
            "Epoch: [368][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9742e-01 (-8.9433e-01)\n",
            "Epoch: [368][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9122e-01 (-8.9417e-01)\n",
            "Epoch: [368][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8965e-01 (-8.9422e-01)\n",
            "Training...\n",
            "Epoch: [369][ 0/97]\tTime  0.459 ( 0.459)\tLoss -9.0010e-01 (-9.0010e-01)\n",
            "Epoch: [369][10/97]\tTime  0.176 ( 0.203)\tLoss -9.1168e-01 (-8.9897e-01)\n",
            "Epoch: [369][20/97]\tTime  0.177 ( 0.191)\tLoss -8.8992e-01 (-8.9771e-01)\n",
            "Epoch: [369][30/97]\tTime  0.178 ( 0.186)\tLoss -8.8908e-01 (-8.9637e-01)\n",
            "Epoch: [369][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0138e-01 (-8.9606e-01)\n",
            "Epoch: [369][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8710e-01 (-8.9471e-01)\n",
            "Epoch: [369][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8749e-01 (-8.9440e-01)\n",
            "Epoch: [369][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8734e-01 (-8.9458e-01)\n",
            "Epoch: [369][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9838e-01 (-8.9533e-01)\n",
            "Epoch: [369][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9443e-01 (-8.9564e-01)\n",
            "Training...\n",
            "Epoch: [370][ 0/97]\tTime  0.443 ( 0.443)\tLoss -8.9972e-01 (-8.9972e-01)\n",
            "Epoch: [370][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8871e-01 (-8.9332e-01)\n",
            "Epoch: [370][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8357e-01 (-8.9456e-01)\n",
            "Epoch: [370][30/97]\tTime  0.176 ( 0.185)\tLoss -8.9773e-01 (-8.9616e-01)\n",
            "Epoch: [370][40/97]\tTime  0.176 ( 0.183)\tLoss -8.8927e-01 (-8.9635e-01)\n",
            "Epoch: [370][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9336e-01 (-8.9573e-01)\n",
            "Epoch: [370][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9209e-01 (-8.9566e-01)\n",
            "Epoch: [370][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9492e-01 (-8.9554e-01)\n",
            "Epoch: [370][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9892e-01 (-8.9554e-01)\n",
            "Epoch: [370][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9331e-01 (-8.9549e-01)\n",
            "Validating...\n",
            "Top1: 0.836657072368421\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [371][ 0/97]\tTime  0.442 ( 0.442)\tLoss -9.0039e-01 (-9.0039e-01)\n",
            "Epoch: [371][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9207e-01 (-8.9357e-01)\n",
            "Epoch: [371][20/97]\tTime  0.177 ( 0.189)\tLoss -8.8568e-01 (-8.9383e-01)\n",
            "Epoch: [371][30/97]\tTime  0.176 ( 0.185)\tLoss -8.9595e-01 (-8.9495e-01)\n",
            "Epoch: [371][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9268e-01 (-8.9500e-01)\n",
            "Epoch: [371][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9926e-01 (-8.9527e-01)\n",
            "Epoch: [371][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0511e-01 (-8.9472e-01)\n",
            "Epoch: [371][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0981e-01 (-8.9553e-01)\n",
            "Epoch: [371][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9930e-01 (-8.9598e-01)\n",
            "Epoch: [371][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9570e-01 (-8.9626e-01)\n",
            "Training...\n",
            "Epoch: [372][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9135e-01 (-8.9135e-01)\n",
            "Epoch: [372][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9422e-01 (-8.9734e-01)\n",
            "Epoch: [372][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9696e-01 (-8.9660e-01)\n",
            "Epoch: [372][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9300e-01 (-8.9544e-01)\n",
            "Epoch: [372][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9739e-01 (-8.9620e-01)\n",
            "Epoch: [372][50/97]\tTime  0.177 ( 0.183)\tLoss -8.8795e-01 (-8.9702e-01)\n",
            "Epoch: [372][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9232e-01 (-8.9673e-01)\n",
            "Epoch: [372][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0743e-01 (-8.9715e-01)\n",
            "Epoch: [372][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9065e-01 (-8.9772e-01)\n",
            "Epoch: [372][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9849e-01 (-8.9759e-01)\n",
            "Training...\n",
            "Epoch: [373][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9464e-01 (-8.9464e-01)\n",
            "Epoch: [373][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0798e-01 (-8.9712e-01)\n",
            "Epoch: [373][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0899e-01 (-8.9862e-01)\n",
            "Epoch: [373][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1613e-01 (-8.9970e-01)\n",
            "Epoch: [373][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0203e-01 (-8.9947e-01)\n",
            "Epoch: [373][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8789e-01 (-8.9817e-01)\n",
            "Epoch: [373][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9100e-01 (-8.9709e-01)\n",
            "Epoch: [373][70/97]\tTime  0.176 ( 0.181)\tLoss -9.0381e-01 (-8.9732e-01)\n",
            "Epoch: [373][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0524e-01 (-8.9752e-01)\n",
            "Epoch: [373][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9398e-01 (-8.9722e-01)\n",
            "Training...\n",
            "Epoch: [374][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9950e-01 (-8.9950e-01)\n",
            "Epoch: [374][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0013e-01 (-8.9376e-01)\n",
            "Epoch: [374][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8664e-01 (-8.9354e-01)\n",
            "Epoch: [374][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9680e-01 (-8.9254e-01)\n",
            "Epoch: [374][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0025e-01 (-8.9314e-01)\n",
            "Epoch: [374][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0373e-01 (-8.9389e-01)\n",
            "Epoch: [374][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9512e-01 (-8.9483e-01)\n",
            "Epoch: [374][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0276e-01 (-8.9487e-01)\n",
            "Epoch: [374][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8999e-01 (-8.9491e-01)\n",
            "Epoch: [374][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8941e-01 (-8.9481e-01)\n",
            "Training...\n",
            "Epoch: [375][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9738e-01 (-8.9738e-01)\n",
            "Epoch: [375][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9965e-01 (-8.9798e-01)\n",
            "Epoch: [375][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0152e-01 (-8.9803e-01)\n",
            "Epoch: [375][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0351e-01 (-8.9765e-01)\n",
            "Epoch: [375][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8718e-01 (-8.9784e-01)\n",
            "Epoch: [375][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9529e-01 (-8.9683e-01)\n",
            "Epoch: [375][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9714e-01 (-8.9681e-01)\n",
            "Epoch: [375][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9018e-01 (-8.9634e-01)\n",
            "Epoch: [375][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8984e-01 (-8.9653e-01)\n",
            "Epoch: [375][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9505e-01 (-8.9666e-01)\n",
            "Validating...\n",
            "Top1: 0.833264802631579\n",
            "Training...\n",
            "Epoch: [376][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.8888e-01 (-8.8888e-01)\n",
            "Epoch: [376][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0071e-01 (-8.9797e-01)\n",
            "Epoch: [376][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0478e-01 (-8.9748e-01)\n",
            "Epoch: [376][30/97]\tTime  0.177 ( 0.185)\tLoss -8.9694e-01 (-8.9714e-01)\n",
            "Epoch: [376][40/97]\tTime  0.178 ( 0.183)\tLoss -9.0354e-01 (-8.9729e-01)\n",
            "Epoch: [376][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9063e-01 (-8.9644e-01)\n",
            "Epoch: [376][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9362e-01 (-8.9636e-01)\n",
            "Epoch: [376][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9546e-01 (-8.9665e-01)\n",
            "Epoch: [376][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9062e-01 (-8.9684e-01)\n",
            "Epoch: [376][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9336e-01 (-8.9663e-01)\n",
            "Training...\n",
            "Epoch: [377][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9343e-01 (-8.9343e-01)\n",
            "Epoch: [377][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9369e-01 (-8.9489e-01)\n",
            "Epoch: [377][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0495e-01 (-8.9609e-01)\n",
            "Epoch: [377][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9437e-01 (-8.9671e-01)\n",
            "Epoch: [377][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0237e-01 (-8.9710e-01)\n",
            "Epoch: [377][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9604e-01 (-8.9645e-01)\n",
            "Epoch: [377][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0799e-01 (-8.9640e-01)\n",
            "Epoch: [377][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9391e-01 (-8.9649e-01)\n",
            "Epoch: [377][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9846e-01 (-8.9621e-01)\n",
            "Epoch: [377][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9448e-01 (-8.9635e-01)\n",
            "Training...\n",
            "Epoch: [378][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0583e-01 (-9.0583e-01)\n",
            "Epoch: [378][10/97]\tTime  0.176 ( 0.202)\tLoss -8.9516e-01 (-8.9919e-01)\n",
            "Epoch: [378][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9196e-01 (-8.9844e-01)\n",
            "Epoch: [378][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9097e-01 (-8.9707e-01)\n",
            "Epoch: [378][40/97]\tTime  0.178 ( 0.184)\tLoss -8.9732e-01 (-8.9678e-01)\n",
            "Epoch: [378][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9687e-01 (-8.9689e-01)\n",
            "Epoch: [378][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9584e-01 (-8.9783e-01)\n",
            "Epoch: [378][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8214e-01 (-8.9718e-01)\n",
            "Epoch: [378][80/97]\tTime  0.178 ( 0.180)\tLoss -8.8162e-01 (-8.9667e-01)\n",
            "Epoch: [378][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9268e-01 (-8.9654e-01)\n",
            "Training...\n",
            "Epoch: [379][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.8811e-01 (-8.8811e-01)\n",
            "Epoch: [379][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9742e-01 (-8.9990e-01)\n",
            "Epoch: [379][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9803e-01 (-8.9872e-01)\n",
            "Epoch: [379][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0029e-01 (-8.9821e-01)\n",
            "Epoch: [379][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0076e-01 (-8.9905e-01)\n",
            "Epoch: [379][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8909e-01 (-8.9815e-01)\n",
            "Epoch: [379][60/97]\tTime  0.178 ( 0.181)\tLoss -8.8686e-01 (-8.9791e-01)\n",
            "Epoch: [379][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0017e-01 (-8.9747e-01)\n",
            "Epoch: [379][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9328e-01 (-8.9734e-01)\n",
            "Epoch: [379][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0472e-01 (-8.9725e-01)\n",
            "Training...\n",
            "Epoch: [380][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9682e-01 (-8.9682e-01)\n",
            "Epoch: [380][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9655e-01 (-9.0152e-01)\n",
            "Epoch: [380][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9676e-01 (-8.9956e-01)\n",
            "Epoch: [380][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0625e-01 (-8.9993e-01)\n",
            "Epoch: [380][40/97]\tTime  0.176 ( 0.184)\tLoss -8.9195e-01 (-8.9866e-01)\n",
            "Epoch: [380][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9813e-01 (-8.9922e-01)\n",
            "Epoch: [380][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0182e-01 (-8.9917e-01)\n",
            "Epoch: [380][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9535e-01 (-8.9878e-01)\n",
            "Epoch: [380][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0363e-01 (-8.9867e-01)\n",
            "Epoch: [380][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9459e-01 (-8.9869e-01)\n",
            "Validating...\n",
            "Top1: 0.8251439144736842\n",
            "Training...\n",
            "Epoch: [381][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0709e-01 (-9.0709e-01)\n",
            "Epoch: [381][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0885e-01 (-9.0080e-01)\n",
            "Epoch: [381][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9613e-01 (-9.0057e-01)\n",
            "Epoch: [381][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0177e-01 (-9.0027e-01)\n",
            "Epoch: [381][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8740e-01 (-8.9984e-01)\n",
            "Epoch: [381][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9750e-01 (-8.9925e-01)\n",
            "Epoch: [381][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0201e-01 (-8.9830e-01)\n",
            "Epoch: [381][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9577e-01 (-8.9791e-01)\n",
            "Epoch: [381][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0627e-01 (-8.9820e-01)\n",
            "Epoch: [381][90/97]\tTime  0.176 ( 0.180)\tLoss -8.9855e-01 (-8.9805e-01)\n",
            "Training...\n",
            "Epoch: [382][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.9253e-01 (-8.9253e-01)\n",
            "Epoch: [382][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8915e-01 (-8.9403e-01)\n",
            "Epoch: [382][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9995e-01 (-8.9435e-01)\n",
            "Epoch: [382][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9214e-01 (-8.9506e-01)\n",
            "Epoch: [382][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0221e-01 (-8.9501e-01)\n",
            "Epoch: [382][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9365e-01 (-8.9536e-01)\n",
            "Epoch: [382][60/97]\tTime  0.176 ( 0.181)\tLoss -8.9329e-01 (-8.9531e-01)\n",
            "Epoch: [382][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9489e-01 (-8.9530e-01)\n",
            "Epoch: [382][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9685e-01 (-8.9580e-01)\n",
            "Epoch: [382][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9514e-01 (-8.9619e-01)\n",
            "Training...\n",
            "Epoch: [383][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.9528e-01 (-8.9528e-01)\n",
            "Epoch: [383][10/97]\tTime  0.177 ( 0.203)\tLoss -8.9069e-01 (-8.9650e-01)\n",
            "Epoch: [383][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9229e-01 (-8.9651e-01)\n",
            "Epoch: [383][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9303e-01 (-8.9704e-01)\n",
            "Epoch: [383][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0446e-01 (-8.9740e-01)\n",
            "Epoch: [383][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9373e-01 (-8.9716e-01)\n",
            "Epoch: [383][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0159e-01 (-8.9698e-01)\n",
            "Epoch: [383][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9429e-01 (-8.9700e-01)\n",
            "Epoch: [383][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0549e-01 (-8.9716e-01)\n",
            "Epoch: [383][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9110e-01 (-8.9727e-01)\n",
            "Training...\n",
            "Epoch: [384][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.1072e-01 (-9.1072e-01)\n",
            "Epoch: [384][10/97]\tTime  0.177 ( 0.201)\tLoss -8.8789e-01 (-9.0320e-01)\n",
            "Epoch: [384][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9192e-01 (-9.0062e-01)\n",
            "Epoch: [384][30/97]\tTime  0.177 ( 0.185)\tLoss -9.0652e-01 (-9.0010e-01)\n",
            "Epoch: [384][40/97]\tTime  0.177 ( 0.183)\tLoss -8.7532e-01 (-8.9909e-01)\n",
            "Epoch: [384][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8870e-01 (-8.9858e-01)\n",
            "Epoch: [384][60/97]\tTime  0.176 ( 0.181)\tLoss -9.0465e-01 (-8.9848e-01)\n",
            "Epoch: [384][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0485e-01 (-8.9874e-01)\n",
            "Epoch: [384][80/97]\tTime  0.176 ( 0.180)\tLoss -8.9131e-01 (-8.9859e-01)\n",
            "Epoch: [384][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9798e-01 (-8.9841e-01)\n",
            "Training...\n",
            "Epoch: [385][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0393e-01 (-9.0393e-01)\n",
            "Epoch: [385][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9534e-01 (-8.9822e-01)\n",
            "Epoch: [385][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9997e-01 (-8.9862e-01)\n",
            "Epoch: [385][30/97]\tTime  0.179 ( 0.186)\tLoss -8.8860e-01 (-8.9907e-01)\n",
            "Epoch: [385][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9607e-01 (-8.9786e-01)\n",
            "Epoch: [385][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8996e-01 (-8.9739e-01)\n",
            "Epoch: [385][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0805e-01 (-8.9749e-01)\n",
            "Epoch: [385][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9177e-01 (-8.9715e-01)\n",
            "Epoch: [385][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9817e-01 (-8.9680e-01)\n",
            "Epoch: [385][90/97]\tTime  0.176 ( 0.180)\tLoss -9.0376e-01 (-8.9647e-01)\n",
            "Validating...\n",
            "Top1: 0.8291529605263158\n",
            "Training...\n",
            "Epoch: [386][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.0432e-01 (-9.0432e-01)\n",
            "Epoch: [386][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8917e-01 (-9.0133e-01)\n",
            "Epoch: [386][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9800e-01 (-8.9998e-01)\n",
            "Epoch: [386][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8338e-01 (-8.9908e-01)\n",
            "Epoch: [386][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0267e-01 (-8.9965e-01)\n",
            "Epoch: [386][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0952e-01 (-8.9931e-01)\n",
            "Epoch: [386][60/97]\tTime  0.178 ( 0.182)\tLoss -8.8979e-01 (-8.9939e-01)\n",
            "Epoch: [386][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0167e-01 (-9.0004e-01)\n",
            "Epoch: [386][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0034e-01 (-8.9966e-01)\n",
            "Epoch: [386][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9748e-01 (-8.9942e-01)\n",
            "Training...\n",
            "Epoch: [387][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9117e-01 (-8.9117e-01)\n",
            "Epoch: [387][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0402e-01 (-9.0250e-01)\n",
            "Epoch: [387][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9228e-01 (-9.0194e-01)\n",
            "Epoch: [387][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9673e-01 (-9.0058e-01)\n",
            "Epoch: [387][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9753e-01 (-8.9971e-01)\n",
            "Epoch: [387][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8969e-01 (-8.9845e-01)\n",
            "Epoch: [387][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9915e-01 (-8.9853e-01)\n",
            "Epoch: [387][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0391e-01 (-8.9886e-01)\n",
            "Epoch: [387][80/97]\tTime  0.178 ( 0.180)\tLoss -8.9832e-01 (-8.9908e-01)\n",
            "Epoch: [387][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0738e-01 (-8.9909e-01)\n",
            "Training...\n",
            "Epoch: [388][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.0551e-01 (-9.0551e-01)\n",
            "Epoch: [388][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0212e-01 (-8.9954e-01)\n",
            "Epoch: [388][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0192e-01 (-8.9983e-01)\n",
            "Epoch: [388][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0552e-01 (-9.0003e-01)\n",
            "Epoch: [388][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0071e-01 (-8.9984e-01)\n",
            "Epoch: [388][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0975e-01 (-8.9990e-01)\n",
            "Epoch: [388][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9854e-01 (-8.9967e-01)\n",
            "Epoch: [388][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9814e-01 (-8.9935e-01)\n",
            "Epoch: [388][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9776e-01 (-8.9927e-01)\n",
            "Epoch: [388][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9799e-01 (-8.9895e-01)\n",
            "Training...\n",
            "Epoch: [389][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0405e-01 (-9.0405e-01)\n",
            "Epoch: [389][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0375e-01 (-9.0150e-01)\n",
            "Epoch: [389][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9685e-01 (-8.9829e-01)\n",
            "Epoch: [389][30/97]\tTime  0.177 ( 0.186)\tLoss -8.8955e-01 (-8.9865e-01)\n",
            "Epoch: [389][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9222e-01 (-8.9926e-01)\n",
            "Epoch: [389][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0993e-01 (-8.9948e-01)\n",
            "Epoch: [389][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0302e-01 (-8.9893e-01)\n",
            "Epoch: [389][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9697e-01 (-8.9869e-01)\n",
            "Epoch: [389][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9894e-01 (-8.9836e-01)\n",
            "Epoch: [389][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9805e-01 (-8.9815e-01)\n",
            "Training...\n",
            "Epoch: [390][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0529e-01 (-9.0529e-01)\n",
            "Epoch: [390][10/97]\tTime  0.178 ( 0.201)\tLoss -9.0392e-01 (-8.9917e-01)\n",
            "Epoch: [390][20/97]\tTime  0.176 ( 0.190)\tLoss -9.0190e-01 (-9.0101e-01)\n",
            "Epoch: [390][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9593e-01 (-9.0064e-01)\n",
            "Epoch: [390][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0536e-01 (-8.9927e-01)\n",
            "Epoch: [390][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8868e-01 (-8.9932e-01)\n",
            "Epoch: [390][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8584e-01 (-8.9917e-01)\n",
            "Epoch: [390][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0379e-01 (-8.9912e-01)\n",
            "Epoch: [390][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0254e-01 (-8.9907e-01)\n",
            "Epoch: [390][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0833e-01 (-8.9920e-01)\n",
            "Validating...\n",
            "Top1: 0.83203125\n",
            "Training...\n",
            "Epoch: [391][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9627e-01 (-8.9627e-01)\n",
            "Epoch: [391][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0024e-01 (-9.0177e-01)\n",
            "Epoch: [391][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0371e-01 (-9.0207e-01)\n",
            "Epoch: [391][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9806e-01 (-9.0094e-01)\n",
            "Epoch: [391][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0313e-01 (-9.0049e-01)\n",
            "Epoch: [391][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0092e-01 (-9.0001e-01)\n",
            "Epoch: [391][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9364e-01 (-9.0044e-01)\n",
            "Epoch: [391][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9636e-01 (-8.9995e-01)\n",
            "Epoch: [391][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8615e-01 (-8.9988e-01)\n",
            "Epoch: [391][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0616e-01 (-9.0038e-01)\n",
            "Training...\n",
            "Epoch: [392][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.0466e-01 (-9.0466e-01)\n",
            "Epoch: [392][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0630e-01 (-9.0189e-01)\n",
            "Epoch: [392][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0107e-01 (-9.0120e-01)\n",
            "Epoch: [392][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0331e-01 (-9.0078e-01)\n",
            "Epoch: [392][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0303e-01 (-9.0050e-01)\n",
            "Epoch: [392][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0426e-01 (-9.0012e-01)\n",
            "Epoch: [392][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0836e-01 (-9.0015e-01)\n",
            "Epoch: [392][70/97]\tTime  0.178 ( 0.181)\tLoss -8.8952e-01 (-9.0018e-01)\n",
            "Epoch: [392][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0198e-01 (-9.0025e-01)\n",
            "Epoch: [392][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9997e-01 (-9.0033e-01)\n",
            "Training...\n",
            "Epoch: [393][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9809e-01 (-8.9809e-01)\n",
            "Epoch: [393][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0310e-01 (-9.0037e-01)\n",
            "Epoch: [393][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0623e-01 (-9.0045e-01)\n",
            "Epoch: [393][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0577e-01 (-8.9983e-01)\n",
            "Epoch: [393][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9328e-01 (-9.0009e-01)\n",
            "Epoch: [393][50/97]\tTime  0.177 ( 0.182)\tLoss -8.8714e-01 (-8.9904e-01)\n",
            "Epoch: [393][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8838e-01 (-8.9856e-01)\n",
            "Epoch: [393][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0702e-01 (-8.9832e-01)\n",
            "Epoch: [393][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9787e-01 (-8.9852e-01)\n",
            "Epoch: [393][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9984e-01 (-8.9882e-01)\n",
            "Training...\n",
            "Epoch: [394][ 0/97]\tTime  0.456 ( 0.456)\tLoss -8.9252e-01 (-8.9252e-01)\n",
            "Epoch: [394][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0066e-01 (-8.9912e-01)\n",
            "Epoch: [394][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9278e-01 (-8.9789e-01)\n",
            "Epoch: [394][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0836e-01 (-8.9865e-01)\n",
            "Epoch: [394][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0796e-01 (-8.9962e-01)\n",
            "Epoch: [394][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1028e-01 (-8.9978e-01)\n",
            "Epoch: [394][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0098e-01 (-8.9987e-01)\n",
            "Epoch: [394][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9978e-01 (-8.9938e-01)\n",
            "Epoch: [394][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9181e-01 (-8.9842e-01)\n",
            "Epoch: [394][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1263e-01 (-8.9875e-01)\n",
            "Training...\n",
            "Epoch: [395][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9459e-01 (-8.9459e-01)\n",
            "Epoch: [395][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1120e-01 (-9.0367e-01)\n",
            "Epoch: [395][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1100e-01 (-9.0137e-01)\n",
            "Epoch: [395][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0145e-01 (-9.0067e-01)\n",
            "Epoch: [395][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0327e-01 (-9.0098e-01)\n",
            "Epoch: [395][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0060e-01 (-9.0088e-01)\n",
            "Epoch: [395][60/97]\tTime  0.177 ( 0.182)\tLoss -8.8630e-01 (-9.0017e-01)\n",
            "Epoch: [395][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9030e-01 (-8.9895e-01)\n",
            "Epoch: [395][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0625e-01 (-8.9918e-01)\n",
            "Epoch: [395][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0173e-01 (-8.9942e-01)\n",
            "Validating...\n",
            "Top1: 0.8344983552631579\n",
            "Training...\n",
            "Epoch: [396][ 0/97]\tTime  0.446 ( 0.446)\tLoss -8.9774e-01 (-8.9774e-01)\n",
            "Epoch: [396][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0202e-01 (-9.0034e-01)\n",
            "Epoch: [396][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1014e-01 (-8.9912e-01)\n",
            "Epoch: [396][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0190e-01 (-8.9932e-01)\n",
            "Epoch: [396][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0775e-01 (-9.0021e-01)\n",
            "Epoch: [396][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0033e-01 (-9.0077e-01)\n",
            "Epoch: [396][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0632e-01 (-9.0111e-01)\n",
            "Epoch: [396][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9443e-01 (-9.0123e-01)\n",
            "Epoch: [396][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9263e-01 (-9.0050e-01)\n",
            "Epoch: [396][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1558e-01 (-9.0054e-01)\n",
            "Training...\n",
            "Epoch: [397][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0250e-01 (-9.0250e-01)\n",
            "Epoch: [397][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1817e-01 (-9.0120e-01)\n",
            "Epoch: [397][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9279e-01 (-9.0168e-01)\n",
            "Epoch: [397][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9548e-01 (-9.0057e-01)\n",
            "Epoch: [397][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9260e-01 (-9.0119e-01)\n",
            "Epoch: [397][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0668e-01 (-9.0124e-01)\n",
            "Epoch: [397][60/97]\tTime  0.178 ( 0.181)\tLoss -9.0245e-01 (-9.0083e-01)\n",
            "Epoch: [397][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8921e-01 (-9.0054e-01)\n",
            "Epoch: [397][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9700e-01 (-9.0046e-01)\n",
            "Epoch: [397][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0590e-01 (-9.0076e-01)\n",
            "Training...\n",
            "Epoch: [398][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0157e-01 (-9.0157e-01)\n",
            "Epoch: [398][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0474e-01 (-8.9985e-01)\n",
            "Epoch: [398][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0304e-01 (-9.0034e-01)\n",
            "Epoch: [398][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0263e-01 (-9.0056e-01)\n",
            "Epoch: [398][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0682e-01 (-9.0017e-01)\n",
            "Epoch: [398][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1114e-01 (-9.0043e-01)\n",
            "Epoch: [398][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9660e-01 (-8.9968e-01)\n",
            "Epoch: [398][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9003e-01 (-8.9929e-01)\n",
            "Epoch: [398][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8467e-01 (-8.9903e-01)\n",
            "Epoch: [398][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0388e-01 (-8.9935e-01)\n",
            "Training...\n",
            "Epoch: [399][ 0/97]\tTime  0.442 ( 0.442)\tLoss -8.9375e-01 (-8.9375e-01)\n",
            "Epoch: [399][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9424e-01 (-9.0053e-01)\n",
            "Epoch: [399][20/97]\tTime  0.176 ( 0.190)\tLoss -8.8937e-01 (-8.9808e-01)\n",
            "Epoch: [399][30/97]\tTime  0.177 ( 0.185)\tLoss -8.9526e-01 (-8.9918e-01)\n",
            "Epoch: [399][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0714e-01 (-8.9913e-01)\n",
            "Epoch: [399][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9550e-01 (-8.9832e-01)\n",
            "Epoch: [399][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0219e-01 (-8.9858e-01)\n",
            "Epoch: [399][70/97]\tTime  0.176 ( 0.181)\tLoss -8.9213e-01 (-8.9854e-01)\n",
            "Epoch: [399][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8717e-01 (-8.9830e-01)\n",
            "Epoch: [399][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9658e-01 (-8.9812e-01)\n",
            "Training...\n",
            "Epoch: [400][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9727e-01 (-8.9727e-01)\n",
            "Epoch: [400][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1086e-01 (-8.9980e-01)\n",
            "Epoch: [400][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0770e-01 (-8.9971e-01)\n",
            "Epoch: [400][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0221e-01 (-9.0016e-01)\n",
            "Epoch: [400][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0536e-01 (-9.0075e-01)\n",
            "Epoch: [400][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0717e-01 (-9.0054e-01)\n",
            "Epoch: [400][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9024e-01 (-9.0006e-01)\n",
            "Epoch: [400][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8507e-01 (-8.9988e-01)\n",
            "Epoch: [400][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9868e-01 (-8.9979e-01)\n",
            "Epoch: [400][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0109e-01 (-8.9943e-01)\n",
            "Validating...\n",
            "Top1: 0.8380962171052632\n",
            "Saving the best model!\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [401][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.0611e-01 (-9.0611e-01)\n",
            "Epoch: [401][10/97]\tTime  0.176 ( 0.201)\tLoss -8.9865e-01 (-9.0144e-01)\n",
            "Epoch: [401][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9685e-01 (-9.0077e-01)\n",
            "Epoch: [401][30/97]\tTime  0.176 ( 0.186)\tLoss -9.1284e-01 (-9.0101e-01)\n",
            "Epoch: [401][40/97]\tTime  0.178 ( 0.183)\tLoss -9.0066e-01 (-9.0133e-01)\n",
            "Epoch: [401][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9428e-01 (-9.0100e-01)\n",
            "Epoch: [401][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0204e-01 (-8.9988e-01)\n",
            "Epoch: [401][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9736e-01 (-8.9971e-01)\n",
            "Epoch: [401][80/97]\tTime  0.178 ( 0.180)\tLoss -9.1004e-01 (-8.9936e-01)\n",
            "Epoch: [401][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9974e-01 (-8.9940e-01)\n",
            "Training...\n",
            "Epoch: [402][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0301e-01 (-9.0301e-01)\n",
            "Epoch: [402][10/97]\tTime  0.178 ( 0.201)\tLoss -8.9692e-01 (-8.9626e-01)\n",
            "Epoch: [402][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9670e-01 (-8.9750e-01)\n",
            "Epoch: [402][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9464e-01 (-8.9710e-01)\n",
            "Epoch: [402][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0092e-01 (-8.9795e-01)\n",
            "Epoch: [402][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9928e-01 (-8.9850e-01)\n",
            "Epoch: [402][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0150e-01 (-8.9858e-01)\n",
            "Epoch: [402][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8965e-01 (-8.9906e-01)\n",
            "Epoch: [402][80/97]\tTime  0.176 ( 0.180)\tLoss -9.0299e-01 (-8.9938e-01)\n",
            "Epoch: [402][90/97]\tTime  0.176 ( 0.180)\tLoss -9.0677e-01 (-8.9958e-01)\n",
            "Training...\n",
            "Epoch: [403][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0461e-01 (-9.0461e-01)\n",
            "Epoch: [403][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9528e-01 (-8.9861e-01)\n",
            "Epoch: [403][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9358e-01 (-8.9894e-01)\n",
            "Epoch: [403][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0665e-01 (-9.0052e-01)\n",
            "Epoch: [403][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9993e-01 (-8.9920e-01)\n",
            "Epoch: [403][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0764e-01 (-8.9897e-01)\n",
            "Epoch: [403][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0299e-01 (-8.9963e-01)\n",
            "Epoch: [403][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0312e-01 (-8.9934e-01)\n",
            "Epoch: [403][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0084e-01 (-8.9953e-01)\n",
            "Epoch: [403][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9436e-01 (-8.9934e-01)\n",
            "Training...\n",
            "Epoch: [404][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9898e-01 (-8.9898e-01)\n",
            "Epoch: [404][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8751e-01 (-8.9869e-01)\n",
            "Epoch: [404][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9742e-01 (-8.9859e-01)\n",
            "Epoch: [404][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9741e-01 (-9.0024e-01)\n",
            "Epoch: [404][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9800e-01 (-9.0113e-01)\n",
            "Epoch: [404][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0113e-01 (-9.0014e-01)\n",
            "Epoch: [404][60/97]\tTime  0.176 ( 0.181)\tLoss -9.0055e-01 (-9.0044e-01)\n",
            "Epoch: [404][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0323e-01 (-9.0020e-01)\n",
            "Epoch: [404][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0631e-01 (-9.0013e-01)\n",
            "Epoch: [404][90/97]\tTime  0.176 ( 0.180)\tLoss -8.9875e-01 (-9.0027e-01)\n",
            "Training...\n",
            "Epoch: [405][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.0106e-01 (-9.0106e-01)\n",
            "Epoch: [405][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0387e-01 (-9.0169e-01)\n",
            "Epoch: [405][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9566e-01 (-9.0267e-01)\n",
            "Epoch: [405][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9024e-01 (-9.0221e-01)\n",
            "Epoch: [405][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0375e-01 (-9.0185e-01)\n",
            "Epoch: [405][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9503e-01 (-9.0075e-01)\n",
            "Epoch: [405][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9678e-01 (-9.0006e-01)\n",
            "Epoch: [405][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0614e-01 (-9.0023e-01)\n",
            "Epoch: [405][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0775e-01 (-9.0044e-01)\n",
            "Epoch: [405][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9758e-01 (-9.0012e-01)\n",
            "Validating...\n",
            "Top1: 0.8359375\n",
            "Training...\n",
            "Epoch: [406][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9852e-01 (-8.9852e-01)\n",
            "Epoch: [406][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9383e-01 (-8.9833e-01)\n",
            "Epoch: [406][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1333e-01 (-9.0106e-01)\n",
            "Epoch: [406][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9926e-01 (-9.0217e-01)\n",
            "Epoch: [406][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9238e-01 (-9.0166e-01)\n",
            "Epoch: [406][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0537e-01 (-9.0125e-01)\n",
            "Epoch: [406][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0421e-01 (-9.0135e-01)\n",
            "Epoch: [406][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0565e-01 (-9.0094e-01)\n",
            "Epoch: [406][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8922e-01 (-9.0017e-01)\n",
            "Epoch: [406][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0711e-01 (-8.9996e-01)\n",
            "Training...\n",
            "Epoch: [407][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0079e-01 (-9.0079e-01)\n",
            "Epoch: [407][10/97]\tTime  0.176 ( 0.202)\tLoss -9.0379e-01 (-8.9910e-01)\n",
            "Epoch: [407][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9196e-01 (-9.0064e-01)\n",
            "Epoch: [407][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9328e-01 (-9.0080e-01)\n",
            "Epoch: [407][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9722e-01 (-9.0093e-01)\n",
            "Epoch: [407][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0681e-01 (-9.0029e-01)\n",
            "Epoch: [407][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0601e-01 (-9.0005e-01)\n",
            "Epoch: [407][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0354e-01 (-8.9935e-01)\n",
            "Epoch: [407][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9734e-01 (-8.9895e-01)\n",
            "Epoch: [407][90/97]\tTime  0.178 ( 0.180)\tLoss -9.0151e-01 (-8.9916e-01)\n",
            "Training...\n",
            "Epoch: [408][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.9994e-01 (-8.9994e-01)\n",
            "Epoch: [408][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0841e-01 (-9.0240e-01)\n",
            "Epoch: [408][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0314e-01 (-9.0233e-01)\n",
            "Epoch: [408][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9689e-01 (-9.0099e-01)\n",
            "Epoch: [408][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0330e-01 (-9.0064e-01)\n",
            "Epoch: [408][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0654e-01 (-9.0048e-01)\n",
            "Epoch: [408][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9963e-01 (-9.0029e-01)\n",
            "Epoch: [408][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9437e-01 (-9.0074e-01)\n",
            "Epoch: [408][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9756e-01 (-9.0069e-01)\n",
            "Epoch: [408][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9980e-01 (-9.0059e-01)\n",
            "Training...\n",
            "Epoch: [409][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.9533e-01 (-8.9533e-01)\n",
            "Epoch: [409][10/97]\tTime  0.176 ( 0.201)\tLoss -9.0512e-01 (-9.0145e-01)\n",
            "Epoch: [409][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0157e-01 (-9.0164e-01)\n",
            "Epoch: [409][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9355e-01 (-9.0139e-01)\n",
            "Epoch: [409][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8828e-01 (-9.0058e-01)\n",
            "Epoch: [409][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9765e-01 (-9.0012e-01)\n",
            "Epoch: [409][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8687e-01 (-8.9915e-01)\n",
            "Epoch: [409][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0155e-01 (-8.9935e-01)\n",
            "Epoch: [409][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9202e-01 (-8.9892e-01)\n",
            "Epoch: [409][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9101e-01 (-8.9905e-01)\n",
            "Training...\n",
            "Epoch: [410][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.1751e-01 (-9.1751e-01)\n",
            "Epoch: [410][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0752e-01 (-9.0023e-01)\n",
            "Epoch: [410][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1043e-01 (-9.0203e-01)\n",
            "Epoch: [410][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9959e-01 (-9.0264e-01)\n",
            "Epoch: [410][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9863e-01 (-9.0289e-01)\n",
            "Epoch: [410][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0385e-01 (-9.0216e-01)\n",
            "Epoch: [410][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1042e-01 (-9.0123e-01)\n",
            "Epoch: [410][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9104e-01 (-9.0056e-01)\n",
            "Epoch: [410][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0085e-01 (-9.0024e-01)\n",
            "Epoch: [410][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0549e-01 (-9.0013e-01)\n",
            "Validating...\n",
            "Top1: 0.8344983552631579\n",
            "Training...\n",
            "Epoch: [411][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0205e-01 (-9.0205e-01)\n",
            "Epoch: [411][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8718e-01 (-9.0038e-01)\n",
            "Epoch: [411][20/97]\tTime  0.176 ( 0.190)\tLoss -8.9618e-01 (-9.0161e-01)\n",
            "Epoch: [411][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9406e-01 (-9.0069e-01)\n",
            "Epoch: [411][40/97]\tTime  0.176 ( 0.184)\tLoss -9.0480e-01 (-9.0096e-01)\n",
            "Epoch: [411][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0609e-01 (-9.0159e-01)\n",
            "Epoch: [411][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9418e-01 (-9.0184e-01)\n",
            "Epoch: [411][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0255e-01 (-9.0117e-01)\n",
            "Epoch: [411][80/97]\tTime  0.176 ( 0.180)\tLoss -9.1029e-01 (-9.0125e-01)\n",
            "Epoch: [411][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0156e-01 (-9.0124e-01)\n",
            "Training...\n",
            "Epoch: [412][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0656e-01 (-9.0656e-01)\n",
            "Epoch: [412][10/97]\tTime  0.176 ( 0.201)\tLoss -8.8262e-01 (-8.9921e-01)\n",
            "Epoch: [412][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0025e-01 (-8.9907e-01)\n",
            "Epoch: [412][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9488e-01 (-8.9895e-01)\n",
            "Epoch: [412][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0509e-01 (-8.9903e-01)\n",
            "Epoch: [412][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9677e-01 (-8.9893e-01)\n",
            "Epoch: [412][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9418e-01 (-8.9882e-01)\n",
            "Epoch: [412][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0634e-01 (-8.9905e-01)\n",
            "Epoch: [412][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9727e-01 (-8.9895e-01)\n",
            "Epoch: [412][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8919e-01 (-8.9888e-01)\n",
            "Training...\n",
            "Epoch: [413][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9525e-01 (-8.9525e-01)\n",
            "Epoch: [413][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8150e-01 (-8.9892e-01)\n",
            "Epoch: [413][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0318e-01 (-8.9957e-01)\n",
            "Epoch: [413][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9982e-01 (-9.0055e-01)\n",
            "Epoch: [413][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8948e-01 (-9.0112e-01)\n",
            "Epoch: [413][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0357e-01 (-9.0122e-01)\n",
            "Epoch: [413][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9421e-01 (-9.0099e-01)\n",
            "Epoch: [413][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0474e-01 (-9.0078e-01)\n",
            "Epoch: [413][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9362e-01 (-9.0109e-01)\n",
            "Epoch: [413][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9946e-01 (-9.0129e-01)\n",
            "Training...\n",
            "Epoch: [414][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.0163e-01 (-9.0163e-01)\n",
            "Epoch: [414][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1088e-01 (-9.0369e-01)\n",
            "Epoch: [414][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9462e-01 (-9.0099e-01)\n",
            "Epoch: [414][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0175e-01 (-9.0091e-01)\n",
            "Epoch: [414][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0001e-01 (-9.0041e-01)\n",
            "Epoch: [414][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1351e-01 (-9.0055e-01)\n",
            "Epoch: [414][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9942e-01 (-9.0111e-01)\n",
            "Epoch: [414][70/97]\tTime  0.178 ( 0.181)\tLoss -8.9864e-01 (-9.0078e-01)\n",
            "Epoch: [414][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8686e-01 (-9.0072e-01)\n",
            "Epoch: [414][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9705e-01 (-9.0075e-01)\n",
            "Training...\n",
            "Epoch: [415][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9615e-01 (-8.9615e-01)\n",
            "Epoch: [415][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9315e-01 (-9.0131e-01)\n",
            "Epoch: [415][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0102e-01 (-9.0004e-01)\n",
            "Epoch: [415][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0521e-01 (-9.0072e-01)\n",
            "Epoch: [415][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9290e-01 (-9.0079e-01)\n",
            "Epoch: [415][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0172e-01 (-9.0133e-01)\n",
            "Epoch: [415][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9162e-01 (-9.0150e-01)\n",
            "Epoch: [415][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9517e-01 (-9.0099e-01)\n",
            "Epoch: [415][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8978e-01 (-9.0056e-01)\n",
            "Epoch: [415][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0508e-01 (-9.0042e-01)\n",
            "Validating...\n",
            "Top1: 0.8377878289473685\n",
            "Training...\n",
            "Epoch: [416][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0734e-01 (-9.0734e-01)\n",
            "Epoch: [416][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9155e-01 (-9.0069e-01)\n",
            "Epoch: [416][20/97]\tTime  0.177 ( 0.190)\tLoss -8.8998e-01 (-8.9925e-01)\n",
            "Epoch: [416][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0413e-01 (-8.9962e-01)\n",
            "Epoch: [416][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0096e-01 (-8.9994e-01)\n",
            "Epoch: [416][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9063e-01 (-9.0052e-01)\n",
            "Epoch: [416][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9859e-01 (-9.0107e-01)\n",
            "Epoch: [416][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0161e-01 (-9.0089e-01)\n",
            "Epoch: [416][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0913e-01 (-9.0072e-01)\n",
            "Epoch: [416][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8379e-01 (-9.0024e-01)\n",
            "Training...\n",
            "Epoch: [417][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.8943e-01 (-8.8943e-01)\n",
            "Epoch: [417][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9389e-01 (-8.9898e-01)\n",
            "Epoch: [417][20/97]\tTime  0.178 ( 0.190)\tLoss -8.8944e-01 (-8.9979e-01)\n",
            "Epoch: [417][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0232e-01 (-8.9979e-01)\n",
            "Epoch: [417][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0330e-01 (-9.0036e-01)\n",
            "Epoch: [417][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9687e-01 (-9.0021e-01)\n",
            "Epoch: [417][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9852e-01 (-9.0017e-01)\n",
            "Epoch: [417][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9666e-01 (-9.0003e-01)\n",
            "Epoch: [417][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0219e-01 (-8.9946e-01)\n",
            "Epoch: [417][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0430e-01 (-8.9980e-01)\n",
            "Training...\n",
            "Epoch: [418][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9263e-01 (-8.9263e-01)\n",
            "Epoch: [418][10/97]\tTime  0.177 ( 0.202)\tLoss -8.8876e-01 (-8.9936e-01)\n",
            "Epoch: [418][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1780e-01 (-9.0151e-01)\n",
            "Epoch: [418][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9332e-01 (-8.9971e-01)\n",
            "Epoch: [418][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0021e-01 (-8.9951e-01)\n",
            "Epoch: [418][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9706e-01 (-8.9979e-01)\n",
            "Epoch: [418][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0281e-01 (-8.9987e-01)\n",
            "Epoch: [418][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9864e-01 (-9.0012e-01)\n",
            "Epoch: [418][80/97]\tTime  0.176 ( 0.180)\tLoss -8.9805e-01 (-9.0025e-01)\n",
            "Epoch: [418][90/97]\tTime  0.176 ( 0.180)\tLoss -8.9704e-01 (-9.0042e-01)\n",
            "Training...\n",
            "Epoch: [419][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.0496e-01 (-9.0496e-01)\n",
            "Epoch: [419][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0891e-01 (-9.0142e-01)\n",
            "Epoch: [419][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9185e-01 (-8.9985e-01)\n",
            "Epoch: [419][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9435e-01 (-9.0005e-01)\n",
            "Epoch: [419][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9934e-01 (-9.0097e-01)\n",
            "Epoch: [419][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9645e-01 (-9.0069e-01)\n",
            "Epoch: [419][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9619e-01 (-8.9985e-01)\n",
            "Epoch: [419][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0542e-01 (-9.0022e-01)\n",
            "Epoch: [419][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9863e-01 (-9.0040e-01)\n",
            "Epoch: [419][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0799e-01 (-9.0050e-01)\n",
            "Training...\n",
            "Epoch: [420][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.0999e-01 (-9.0999e-01)\n",
            "Epoch: [420][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9817e-01 (-9.0053e-01)\n",
            "Epoch: [420][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9302e-01 (-9.0184e-01)\n",
            "Epoch: [420][30/97]\tTime  0.176 ( 0.186)\tLoss -8.8259e-01 (-9.0071e-01)\n",
            "Epoch: [420][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0707e-01 (-9.0162e-01)\n",
            "Epoch: [420][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0518e-01 (-9.0158e-01)\n",
            "Epoch: [420][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0696e-01 (-9.0170e-01)\n",
            "Epoch: [420][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9684e-01 (-9.0126e-01)\n",
            "Epoch: [420][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8725e-01 (-9.0079e-01)\n",
            "Epoch: [420][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9638e-01 (-9.0076e-01)\n",
            "Validating...\n",
            "Top1: 0.8468338815789473\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [421][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.0220e-01 (-9.0220e-01)\n",
            "Epoch: [421][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0279e-01 (-9.0300e-01)\n",
            "Epoch: [421][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0001e-01 (-9.0221e-01)\n",
            "Epoch: [421][30/97]\tTime  0.177 ( 0.186)\tLoss -8.7971e-01 (-9.0183e-01)\n",
            "Epoch: [421][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9587e-01 (-9.0076e-01)\n",
            "Epoch: [421][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0577e-01 (-9.0022e-01)\n",
            "Epoch: [421][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0013e-01 (-9.0048e-01)\n",
            "Epoch: [421][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0606e-01 (-9.0045e-01)\n",
            "Epoch: [421][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0143e-01 (-9.0037e-01)\n",
            "Epoch: [421][90/97]\tTime  0.176 ( 0.180)\tLoss -9.0840e-01 (-9.0037e-01)\n",
            "Training...\n",
            "Epoch: [422][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0379e-01 (-9.0379e-01)\n",
            "Epoch: [422][10/97]\tTime  0.176 ( 0.201)\tLoss -9.1064e-01 (-9.0287e-01)\n",
            "Epoch: [422][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9322e-01 (-8.9990e-01)\n",
            "Epoch: [422][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9937e-01 (-9.0057e-01)\n",
            "Epoch: [422][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9419e-01 (-9.0128e-01)\n",
            "Epoch: [422][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0383e-01 (-9.0174e-01)\n",
            "Epoch: [422][60/97]\tTime  0.178 ( 0.181)\tLoss -9.0078e-01 (-9.0192e-01)\n",
            "Epoch: [422][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9749e-01 (-9.0149e-01)\n",
            "Epoch: [422][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0537e-01 (-9.0148e-01)\n",
            "Epoch: [422][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1293e-01 (-9.0159e-01)\n",
            "Training...\n",
            "Epoch: [423][ 0/97]\tTime  0.460 ( 0.460)\tLoss -9.0598e-01 (-9.0598e-01)\n",
            "Epoch: [423][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0957e-01 (-9.0235e-01)\n",
            "Epoch: [423][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9270e-01 (-9.0171e-01)\n",
            "Epoch: [423][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0505e-01 (-9.0209e-01)\n",
            "Epoch: [423][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9194e-01 (-9.0167e-01)\n",
            "Epoch: [423][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0608e-01 (-9.0124e-01)\n",
            "Epoch: [423][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0407e-01 (-9.0137e-01)\n",
            "Epoch: [423][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9810e-01 (-9.0109e-01)\n",
            "Epoch: [423][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0271e-01 (-9.0102e-01)\n",
            "Epoch: [423][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0346e-01 (-9.0104e-01)\n",
            "Training...\n",
            "Epoch: [424][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.0038e-01 (-9.0038e-01)\n",
            "Epoch: [424][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0842e-01 (-9.0551e-01)\n",
            "Epoch: [424][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9084e-01 (-9.0283e-01)\n",
            "Epoch: [424][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0566e-01 (-9.0291e-01)\n",
            "Epoch: [424][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0133e-01 (-9.0227e-01)\n",
            "Epoch: [424][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9925e-01 (-9.0137e-01)\n",
            "Epoch: [424][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0834e-01 (-9.0131e-01)\n",
            "Epoch: [424][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1061e-01 (-9.0136e-01)\n",
            "Epoch: [424][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0444e-01 (-9.0171e-01)\n",
            "Epoch: [424][90/97]\tTime  0.176 ( 0.180)\tLoss -8.9236e-01 (-9.0162e-01)\n",
            "Training...\n",
            "Epoch: [425][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.0063e-01 (-9.0063e-01)\n",
            "Epoch: [425][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9306e-01 (-9.0252e-01)\n",
            "Epoch: [425][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0574e-01 (-9.0135e-01)\n",
            "Epoch: [425][30/97]\tTime  0.178 ( 0.186)\tLoss -9.1068e-01 (-9.0145e-01)\n",
            "Epoch: [425][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0787e-01 (-9.0153e-01)\n",
            "Epoch: [425][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9289e-01 (-9.0175e-01)\n",
            "Epoch: [425][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0037e-01 (-9.0201e-01)\n",
            "Epoch: [425][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0339e-01 (-9.0163e-01)\n",
            "Epoch: [425][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0779e-01 (-9.0145e-01)\n",
            "Epoch: [425][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9984e-01 (-9.0123e-01)\n",
            "Validating...\n",
            "Top1: 0.8436472039473685\n",
            "Training...\n",
            "Epoch: [426][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.1153e-01 (-9.1153e-01)\n",
            "Epoch: [426][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0873e-01 (-9.0637e-01)\n",
            "Epoch: [426][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9636e-01 (-9.0518e-01)\n",
            "Epoch: [426][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0749e-01 (-9.0502e-01)\n",
            "Epoch: [426][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9294e-01 (-9.0403e-01)\n",
            "Epoch: [426][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1021e-01 (-9.0312e-01)\n",
            "Epoch: [426][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1018e-01 (-9.0349e-01)\n",
            "Epoch: [426][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1071e-01 (-9.0367e-01)\n",
            "Epoch: [426][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9990e-01 (-9.0335e-01)\n",
            "Epoch: [426][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1085e-01 (-9.0320e-01)\n",
            "Training...\n",
            "Epoch: [427][ 0/97]\tTime  0.444 ( 0.444)\tLoss -8.9260e-01 (-8.9260e-01)\n",
            "Epoch: [427][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0480e-01 (-9.0277e-01)\n",
            "Epoch: [427][20/97]\tTime  0.176 ( 0.190)\tLoss -9.0131e-01 (-9.0171e-01)\n",
            "Epoch: [427][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9782e-01 (-9.0249e-01)\n",
            "Epoch: [427][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9653e-01 (-9.0174e-01)\n",
            "Epoch: [427][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9607e-01 (-9.0189e-01)\n",
            "Epoch: [427][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0399e-01 (-9.0186e-01)\n",
            "Epoch: [427][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0113e-01 (-9.0201e-01)\n",
            "Epoch: [427][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0632e-01 (-9.0220e-01)\n",
            "Epoch: [427][90/97]\tTime  0.178 ( 0.180)\tLoss -9.0995e-01 (-9.0199e-01)\n",
            "Training...\n",
            "Epoch: [428][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.1074e-01 (-9.1074e-01)\n",
            "Epoch: [428][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0075e-01 (-8.9901e-01)\n",
            "Epoch: [428][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0181e-01 (-9.0119e-01)\n",
            "Epoch: [428][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0640e-01 (-9.0220e-01)\n",
            "Epoch: [428][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1079e-01 (-9.0279e-01)\n",
            "Epoch: [428][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0967e-01 (-9.0295e-01)\n",
            "Epoch: [428][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9918e-01 (-9.0282e-01)\n",
            "Epoch: [428][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0366e-01 (-9.0257e-01)\n",
            "Epoch: [428][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9417e-01 (-9.0209e-01)\n",
            "Epoch: [428][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1195e-01 (-9.0214e-01)\n",
            "Training...\n",
            "Epoch: [429][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0920e-01 (-9.0920e-01)\n",
            "Epoch: [429][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9597e-01 (-9.0451e-01)\n",
            "Epoch: [429][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0767e-01 (-9.0190e-01)\n",
            "Epoch: [429][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0614e-01 (-9.0156e-01)\n",
            "Epoch: [429][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0936e-01 (-9.0234e-01)\n",
            "Epoch: [429][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9610e-01 (-9.0306e-01)\n",
            "Epoch: [429][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9362e-01 (-9.0247e-01)\n",
            "Epoch: [429][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9430e-01 (-9.0158e-01)\n",
            "Epoch: [429][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1047e-01 (-9.0123e-01)\n",
            "Epoch: [429][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0204e-01 (-9.0118e-01)\n",
            "Training...\n",
            "Epoch: [430][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0610e-01 (-9.0610e-01)\n",
            "Epoch: [430][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0298e-01 (-9.0373e-01)\n",
            "Epoch: [430][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1072e-01 (-9.0194e-01)\n",
            "Epoch: [430][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0706e-01 (-9.0152e-01)\n",
            "Epoch: [430][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9837e-01 (-9.0044e-01)\n",
            "Epoch: [430][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9280e-01 (-9.0051e-01)\n",
            "Epoch: [430][60/97]\tTime  0.178 ( 0.181)\tLoss -9.0961e-01 (-8.9982e-01)\n",
            "Epoch: [430][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1243e-01 (-8.9996e-01)\n",
            "Epoch: [430][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0601e-01 (-9.0005e-01)\n",
            "Epoch: [430][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0324e-01 (-9.0017e-01)\n",
            "Validating...\n",
            "Top1: 0.8435444078947368\n",
            "Training...\n",
            "Epoch: [431][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.9526e-01 (-8.9526e-01)\n",
            "Epoch: [431][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9154e-01 (-8.9983e-01)\n",
            "Epoch: [431][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0099e-01 (-8.9997e-01)\n",
            "Epoch: [431][30/97]\tTime  0.176 ( 0.186)\tLoss -8.9767e-01 (-8.9968e-01)\n",
            "Epoch: [431][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0382e-01 (-8.9883e-01)\n",
            "Epoch: [431][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0204e-01 (-8.9949e-01)\n",
            "Epoch: [431][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9281e-01 (-8.9952e-01)\n",
            "Epoch: [431][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9849e-01 (-8.9966e-01)\n",
            "Epoch: [431][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0257e-01 (-8.9996e-01)\n",
            "Epoch: [431][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9636e-01 (-8.9980e-01)\n",
            "Training...\n",
            "Epoch: [432][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9136e-01 (-8.9136e-01)\n",
            "Epoch: [432][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0163e-01 (-8.9909e-01)\n",
            "Epoch: [432][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0243e-01 (-9.0090e-01)\n",
            "Epoch: [432][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9872e-01 (-9.0059e-01)\n",
            "Epoch: [432][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0610e-01 (-9.0123e-01)\n",
            "Epoch: [432][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0224e-01 (-9.0131e-01)\n",
            "Epoch: [432][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9461e-01 (-9.0112e-01)\n",
            "Epoch: [432][70/97]\tTime  0.177 ( 0.181)\tLoss -8.8888e-01 (-9.0121e-01)\n",
            "Epoch: [432][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0393e-01 (-9.0112e-01)\n",
            "Epoch: [432][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0691e-01 (-9.0127e-01)\n",
            "Training...\n",
            "Epoch: [433][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.0158e-01 (-9.0158e-01)\n",
            "Epoch: [433][10/97]\tTime  0.178 ( 0.203)\tLoss -9.0564e-01 (-9.0274e-01)\n",
            "Epoch: [433][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0265e-01 (-9.0333e-01)\n",
            "Epoch: [433][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0798e-01 (-9.0275e-01)\n",
            "Epoch: [433][40/97]\tTime  0.177 ( 0.184)\tLoss -8.8873e-01 (-9.0207e-01)\n",
            "Epoch: [433][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9485e-01 (-9.0139e-01)\n",
            "Epoch: [433][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0305e-01 (-9.0170e-01)\n",
            "Epoch: [433][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9595e-01 (-9.0130e-01)\n",
            "Epoch: [433][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9862e-01 (-9.0128e-01)\n",
            "Epoch: [433][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9601e-01 (-9.0125e-01)\n",
            "Training...\n",
            "Epoch: [434][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0125e-01 (-9.0125e-01)\n",
            "Epoch: [434][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0272e-01 (-9.0266e-01)\n",
            "Epoch: [434][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1405e-01 (-9.0310e-01)\n",
            "Epoch: [434][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0184e-01 (-9.0403e-01)\n",
            "Epoch: [434][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9802e-01 (-9.0386e-01)\n",
            "Epoch: [434][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0349e-01 (-9.0310e-01)\n",
            "Epoch: [434][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9714e-01 (-9.0268e-01)\n",
            "Epoch: [434][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1018e-01 (-9.0254e-01)\n",
            "Epoch: [434][80/97]\tTime  0.177 ( 0.180)\tLoss -8.8753e-01 (-9.0197e-01)\n",
            "Epoch: [434][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0797e-01 (-9.0170e-01)\n",
            "Training...\n",
            "Epoch: [435][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.0019e-01 (-9.0019e-01)\n",
            "Epoch: [435][10/97]\tTime  0.178 ( 0.201)\tLoss -9.1290e-01 (-9.0395e-01)\n",
            "Epoch: [435][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1142e-01 (-9.0364e-01)\n",
            "Epoch: [435][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0376e-01 (-9.0293e-01)\n",
            "Epoch: [435][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9171e-01 (-9.0212e-01)\n",
            "Epoch: [435][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9914e-01 (-9.0212e-01)\n",
            "Epoch: [435][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9638e-01 (-9.0129e-01)\n",
            "Epoch: [435][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9416e-01 (-9.0135e-01)\n",
            "Epoch: [435][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9269e-01 (-9.0092e-01)\n",
            "Epoch: [435][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0400e-01 (-9.0059e-01)\n",
            "Validating...\n",
            "Top1: 0.8445723684210527\n",
            "Training...\n",
            "Epoch: [436][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.0317e-01 (-9.0317e-01)\n",
            "Epoch: [436][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9967e-01 (-9.0224e-01)\n",
            "Epoch: [436][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0567e-01 (-9.0195e-01)\n",
            "Epoch: [436][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2168e-01 (-9.0240e-01)\n",
            "Epoch: [436][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0679e-01 (-9.0329e-01)\n",
            "Epoch: [436][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0453e-01 (-9.0291e-01)\n",
            "Epoch: [436][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9420e-01 (-9.0257e-01)\n",
            "Epoch: [436][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0309e-01 (-9.0245e-01)\n",
            "Epoch: [436][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9878e-01 (-9.0239e-01)\n",
            "Epoch: [436][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9753e-01 (-9.0199e-01)\n",
            "Training...\n",
            "Epoch: [437][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.0874e-01 (-9.0874e-01)\n",
            "Epoch: [437][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0973e-01 (-9.0371e-01)\n",
            "Epoch: [437][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1567e-01 (-9.0270e-01)\n",
            "Epoch: [437][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1417e-01 (-9.0328e-01)\n",
            "Epoch: [437][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9515e-01 (-9.0232e-01)\n",
            "Epoch: [437][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9981e-01 (-9.0255e-01)\n",
            "Epoch: [437][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0368e-01 (-9.0203e-01)\n",
            "Epoch: [437][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0037e-01 (-9.0254e-01)\n",
            "Epoch: [437][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9783e-01 (-9.0252e-01)\n",
            "Epoch: [437][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0903e-01 (-9.0261e-01)\n",
            "Training...\n",
            "Epoch: [438][ 0/97]\tTime  0.464 ( 0.464)\tLoss -9.0485e-01 (-9.0485e-01)\n",
            "Epoch: [438][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0146e-01 (-8.9813e-01)\n",
            "Epoch: [438][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9796e-01 (-8.9967e-01)\n",
            "Epoch: [438][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9441e-01 (-9.0027e-01)\n",
            "Epoch: [438][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1259e-01 (-9.0090e-01)\n",
            "Epoch: [438][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0110e-01 (-9.0070e-01)\n",
            "Epoch: [438][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0560e-01 (-9.0154e-01)\n",
            "Epoch: [438][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0226e-01 (-9.0185e-01)\n",
            "Epoch: [438][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1408e-01 (-9.0205e-01)\n",
            "Epoch: [438][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9696e-01 (-9.0180e-01)\n",
            "Training...\n",
            "Epoch: [439][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0557e-01 (-9.0557e-01)\n",
            "Epoch: [439][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1429e-01 (-9.0613e-01)\n",
            "Epoch: [439][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9520e-01 (-9.0426e-01)\n",
            "Epoch: [439][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9774e-01 (-9.0406e-01)\n",
            "Epoch: [439][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9539e-01 (-9.0310e-01)\n",
            "Epoch: [439][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0207e-01 (-9.0261e-01)\n",
            "Epoch: [439][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0712e-01 (-9.0253e-01)\n",
            "Epoch: [439][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0131e-01 (-9.0239e-01)\n",
            "Epoch: [439][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9327e-01 (-9.0238e-01)\n",
            "Epoch: [439][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0233e-01 (-9.0224e-01)\n",
            "Training...\n",
            "Epoch: [440][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.0008e-01 (-9.0008e-01)\n",
            "Epoch: [440][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0280e-01 (-9.0292e-01)\n",
            "Epoch: [440][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0997e-01 (-9.0364e-01)\n",
            "Epoch: [440][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9701e-01 (-9.0316e-01)\n",
            "Epoch: [440][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9690e-01 (-9.0212e-01)\n",
            "Epoch: [440][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9689e-01 (-9.0165e-01)\n",
            "Epoch: [440][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0812e-01 (-9.0240e-01)\n",
            "Epoch: [440][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9537e-01 (-9.0195e-01)\n",
            "Epoch: [440][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0515e-01 (-9.0190e-01)\n",
            "Epoch: [440][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0077e-01 (-9.0189e-01)\n",
            "Validating...\n",
            "Top1: 0.8406661184210527\n",
            "Training...\n",
            "Epoch: [441][ 0/97]\tTime  0.442 ( 0.442)\tLoss -9.0038e-01 (-9.0038e-01)\n",
            "Epoch: [441][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1284e-01 (-9.0345e-01)\n",
            "Epoch: [441][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0675e-01 (-9.0270e-01)\n",
            "Epoch: [441][30/97]\tTime  0.177 ( 0.185)\tLoss -8.9939e-01 (-9.0270e-01)\n",
            "Epoch: [441][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0262e-01 (-9.0139e-01)\n",
            "Epoch: [441][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9554e-01 (-9.0207e-01)\n",
            "Epoch: [441][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9679e-01 (-9.0200e-01)\n",
            "Epoch: [441][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0106e-01 (-9.0167e-01)\n",
            "Epoch: [441][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0255e-01 (-9.0188e-01)\n",
            "Epoch: [441][90/97]\tTime  0.177 ( 0.180)\tLoss -8.8655e-01 (-9.0149e-01)\n",
            "Training...\n",
            "Epoch: [442][ 0/97]\tTime  0.448 ( 0.448)\tLoss -8.8615e-01 (-8.8615e-01)\n",
            "Epoch: [442][10/97]\tTime  0.178 ( 0.201)\tLoss -8.9733e-01 (-8.9712e-01)\n",
            "Epoch: [442][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0361e-01 (-9.0070e-01)\n",
            "Epoch: [442][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1022e-01 (-9.0081e-01)\n",
            "Epoch: [442][40/97]\tTime  0.177 ( 0.183)\tLoss -8.9986e-01 (-9.0069e-01)\n",
            "Epoch: [442][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0037e-01 (-9.0062e-01)\n",
            "Epoch: [442][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0164e-01 (-9.0021e-01)\n",
            "Epoch: [442][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9378e-01 (-9.0050e-01)\n",
            "Epoch: [442][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0525e-01 (-9.0043e-01)\n",
            "Epoch: [442][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0423e-01 (-9.0074e-01)\n",
            "Training...\n",
            "Epoch: [443][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.0270e-01 (-9.0270e-01)\n",
            "Epoch: [443][10/97]\tTime  0.176 ( 0.202)\tLoss -8.9353e-01 (-9.0059e-01)\n",
            "Epoch: [443][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9725e-01 (-9.0026e-01)\n",
            "Epoch: [443][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0623e-01 (-9.0141e-01)\n",
            "Epoch: [443][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9048e-01 (-9.0103e-01)\n",
            "Epoch: [443][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0665e-01 (-9.0048e-01)\n",
            "Epoch: [443][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0984e-01 (-9.0097e-01)\n",
            "Epoch: [443][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0620e-01 (-9.0127e-01)\n",
            "Epoch: [443][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0861e-01 (-9.0162e-01)\n",
            "Epoch: [443][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1239e-01 (-9.0168e-01)\n",
            "Training...\n",
            "Epoch: [444][ 0/97]\tTime  0.449 ( 0.449)\tLoss -8.8948e-01 (-8.8948e-01)\n",
            "Epoch: [444][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9860e-01 (-9.0464e-01)\n",
            "Epoch: [444][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9931e-01 (-9.0204e-01)\n",
            "Epoch: [444][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9258e-01 (-9.0017e-01)\n",
            "Epoch: [444][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0753e-01 (-9.0070e-01)\n",
            "Epoch: [444][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9760e-01 (-9.0078e-01)\n",
            "Epoch: [444][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9988e-01 (-9.0074e-01)\n",
            "Epoch: [444][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9116e-01 (-9.0074e-01)\n",
            "Epoch: [444][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0541e-01 (-9.0114e-01)\n",
            "Epoch: [444][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0142e-01 (-9.0123e-01)\n",
            "Training...\n",
            "Epoch: [445][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0779e-01 (-9.0779e-01)\n",
            "Epoch: [445][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9420e-01 (-9.0186e-01)\n",
            "Epoch: [445][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9789e-01 (-9.0211e-01)\n",
            "Epoch: [445][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9524e-01 (-9.0158e-01)\n",
            "Epoch: [445][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9776e-01 (-9.0123e-01)\n",
            "Epoch: [445][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0801e-01 (-9.0132e-01)\n",
            "Epoch: [445][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1266e-01 (-9.0211e-01)\n",
            "Epoch: [445][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0156e-01 (-9.0198e-01)\n",
            "Epoch: [445][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9928e-01 (-9.0234e-01)\n",
            "Epoch: [445][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9742e-01 (-9.0177e-01)\n",
            "Validating...\n",
            "Top1: 0.8434416118421053\n",
            "Training...\n",
            "Epoch: [446][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.1065e-01 (-9.1065e-01)\n",
            "Epoch: [446][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9309e-01 (-9.0115e-01)\n",
            "Epoch: [446][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0260e-01 (-9.0193e-01)\n",
            "Epoch: [446][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0391e-01 (-9.0328e-01)\n",
            "Epoch: [446][40/97]\tTime  0.176 ( 0.183)\tLoss -9.0908e-01 (-9.0293e-01)\n",
            "Epoch: [446][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0383e-01 (-9.0276e-01)\n",
            "Epoch: [446][60/97]\tTime  0.177 ( 0.181)\tLoss -8.8533e-01 (-9.0237e-01)\n",
            "Epoch: [446][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0834e-01 (-9.0209e-01)\n",
            "Epoch: [446][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1059e-01 (-9.0221e-01)\n",
            "Epoch: [446][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0724e-01 (-9.0193e-01)\n",
            "Training...\n",
            "Epoch: [447][ 0/97]\tTime  0.461 ( 0.461)\tLoss -8.9377e-01 (-8.9377e-01)\n",
            "Epoch: [447][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0591e-01 (-9.0609e-01)\n",
            "Epoch: [447][20/97]\tTime  0.178 ( 0.193)\tLoss -9.1241e-01 (-9.0600e-01)\n",
            "Epoch: [447][30/97]\tTime  0.177 ( 0.188)\tLoss -9.0780e-01 (-9.0668e-01)\n",
            "Epoch: [447][40/97]\tTime  0.177 ( 0.185)\tLoss -9.1967e-01 (-9.0649e-01)\n",
            "Epoch: [447][50/97]\tTime  0.177 ( 0.184)\tLoss -9.0636e-01 (-9.0638e-01)\n",
            "Epoch: [447][60/97]\tTime  0.178 ( 0.183)\tLoss -8.8573e-01 (-9.0539e-01)\n",
            "Epoch: [447][70/97]\tTime  0.182 ( 0.184)\tLoss -8.8702e-01 (-9.0415e-01)\n",
            "Epoch: [447][80/97]\tTime  0.190 ( 0.185)\tLoss -9.0694e-01 (-9.0386e-01)\n",
            "Epoch: [447][90/97]\tTime  0.189 ( 0.186)\tLoss -9.0191e-01 (-9.0352e-01)\n",
            "Training...\n",
            "Epoch: [448][ 0/97]\tTime  0.462 ( 0.462)\tLoss -9.0118e-01 (-9.0118e-01)\n",
            "Epoch: [448][10/97]\tTime  0.191 ( 0.215)\tLoss -9.0258e-01 (-8.9941e-01)\n",
            "Epoch: [448][20/97]\tTime  0.189 ( 0.203)\tLoss -9.0426e-01 (-9.0048e-01)\n",
            "Epoch: [448][30/97]\tTime  0.188 ( 0.198)\tLoss -8.9741e-01 (-9.0202e-01)\n",
            "Epoch: [448][40/97]\tTime  0.189 ( 0.196)\tLoss -9.0013e-01 (-9.0176e-01)\n",
            "Epoch: [448][50/97]\tTime  0.178 ( 0.194)\tLoss -8.9589e-01 (-9.0145e-01)\n",
            "Epoch: [448][60/97]\tTime  0.177 ( 0.192)\tLoss -9.0185e-01 (-9.0152e-01)\n",
            "Epoch: [448][70/97]\tTime  0.178 ( 0.190)\tLoss -9.0810e-01 (-9.0204e-01)\n",
            "Epoch: [448][80/97]\tTime  0.177 ( 0.188)\tLoss -9.0673e-01 (-9.0211e-01)\n",
            "Epoch: [448][90/97]\tTime  0.177 ( 0.187)\tLoss -9.0248e-01 (-9.0177e-01)\n",
            "Training...\n",
            "Epoch: [449][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0523e-01 (-9.0523e-01)\n",
            "Epoch: [449][10/97]\tTime  0.176 ( 0.201)\tLoss -9.0590e-01 (-9.0115e-01)\n",
            "Epoch: [449][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9587e-01 (-9.0332e-01)\n",
            "Epoch: [449][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0835e-01 (-9.0433e-01)\n",
            "Epoch: [449][40/97]\tTime  0.176 ( 0.183)\tLoss -9.0054e-01 (-9.0392e-01)\n",
            "Epoch: [449][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0885e-01 (-9.0370e-01)\n",
            "Epoch: [449][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9824e-01 (-9.0351e-01)\n",
            "Epoch: [449][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0677e-01 (-9.0363e-01)\n",
            "Epoch: [449][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9458e-01 (-9.0287e-01)\n",
            "Epoch: [449][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9512e-01 (-9.0242e-01)\n",
            "Training...\n",
            "Epoch: [450][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.0611e-01 (-9.0611e-01)\n",
            "Epoch: [450][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1206e-01 (-9.0729e-01)\n",
            "Epoch: [450][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0021e-01 (-9.0479e-01)\n",
            "Epoch: [450][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9906e-01 (-9.0356e-01)\n",
            "Epoch: [450][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9812e-01 (-9.0306e-01)\n",
            "Epoch: [450][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1561e-01 (-9.0347e-01)\n",
            "Epoch: [450][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9831e-01 (-9.0287e-01)\n",
            "Epoch: [450][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0837e-01 (-9.0311e-01)\n",
            "Epoch: [450][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0154e-01 (-9.0320e-01)\n",
            "Epoch: [450][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9607e-01 (-9.0314e-01)\n",
            "Validating...\n",
            "Top1: 0.8431332236842105\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [451][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.0802e-01 (-9.0802e-01)\n",
            "Epoch: [451][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1433e-01 (-9.0544e-01)\n",
            "Epoch: [451][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9940e-01 (-9.0417e-01)\n",
            "Epoch: [451][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1302e-01 (-9.0418e-01)\n",
            "Epoch: [451][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0474e-01 (-9.0489e-01)\n",
            "Epoch: [451][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0385e-01 (-9.0471e-01)\n",
            "Epoch: [451][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0029e-01 (-9.0450e-01)\n",
            "Epoch: [451][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0817e-01 (-9.0437e-01)\n",
            "Epoch: [451][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9560e-01 (-9.0402e-01)\n",
            "Epoch: [451][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9928e-01 (-9.0398e-01)\n",
            "Training...\n",
            "Epoch: [452][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0065e-01 (-9.0065e-01)\n",
            "Epoch: [452][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0731e-01 (-9.0538e-01)\n",
            "Epoch: [452][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0089e-01 (-9.0477e-01)\n",
            "Epoch: [452][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0564e-01 (-9.0363e-01)\n",
            "Epoch: [452][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0029e-01 (-9.0400e-01)\n",
            "Epoch: [452][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0982e-01 (-9.0437e-01)\n",
            "Epoch: [452][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0412e-01 (-9.0407e-01)\n",
            "Epoch: [452][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9495e-01 (-9.0379e-01)\n",
            "Epoch: [452][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0601e-01 (-9.0403e-01)\n",
            "Epoch: [452][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0008e-01 (-9.0378e-01)\n",
            "Training...\n",
            "Epoch: [453][ 0/97]\tTime  0.454 ( 0.454)\tLoss -8.9398e-01 (-8.9398e-01)\n",
            "Epoch: [453][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1115e-01 (-9.0292e-01)\n",
            "Epoch: [453][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0773e-01 (-9.0329e-01)\n",
            "Epoch: [453][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9887e-01 (-9.0300e-01)\n",
            "Epoch: [453][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0276e-01 (-9.0314e-01)\n",
            "Epoch: [453][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9393e-01 (-9.0290e-01)\n",
            "Epoch: [453][60/97]\tTime  0.178 ( 0.181)\tLoss -9.0580e-01 (-9.0283e-01)\n",
            "Epoch: [453][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0674e-01 (-9.0291e-01)\n",
            "Epoch: [453][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9495e-01 (-9.0326e-01)\n",
            "Epoch: [453][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1139e-01 (-9.0356e-01)\n",
            "Training...\n",
            "Epoch: [454][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.0142e-01 (-9.0142e-01)\n",
            "Epoch: [454][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0212e-01 (-9.0375e-01)\n",
            "Epoch: [454][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0179e-01 (-9.0395e-01)\n",
            "Epoch: [454][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0559e-01 (-9.0527e-01)\n",
            "Epoch: [454][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0328e-01 (-9.0577e-01)\n",
            "Epoch: [454][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9585e-01 (-9.0467e-01)\n",
            "Epoch: [454][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9758e-01 (-9.0374e-01)\n",
            "Epoch: [454][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0843e-01 (-9.0352e-01)\n",
            "Epoch: [454][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0079e-01 (-9.0334e-01)\n",
            "Epoch: [454][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1378e-01 (-9.0362e-01)\n",
            "Training...\n",
            "Epoch: [455][ 0/97]\tTime  0.458 ( 0.458)\tLoss -8.9979e-01 (-8.9979e-01)\n",
            "Epoch: [455][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0289e-01 (-9.0358e-01)\n",
            "Epoch: [455][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0087e-01 (-9.0463e-01)\n",
            "Epoch: [455][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9581e-01 (-9.0350e-01)\n",
            "Epoch: [455][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0457e-01 (-9.0301e-01)\n",
            "Epoch: [455][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0330e-01 (-9.0294e-01)\n",
            "Epoch: [455][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9246e-01 (-9.0306e-01)\n",
            "Epoch: [455][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0842e-01 (-9.0348e-01)\n",
            "Epoch: [455][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0666e-01 (-9.0352e-01)\n",
            "Epoch: [455][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9889e-01 (-9.0378e-01)\n",
            "Validating...\n",
            "Top1: 0.8447779605263158\n",
            "Training...\n",
            "Epoch: [456][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.0004e-01 (-9.0004e-01)\n",
            "Epoch: [456][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1304e-01 (-9.0517e-01)\n",
            "Epoch: [456][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0579e-01 (-9.0299e-01)\n",
            "Epoch: [456][30/97]\tTime  0.176 ( 0.186)\tLoss -9.0668e-01 (-9.0268e-01)\n",
            "Epoch: [456][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1133e-01 (-9.0273e-01)\n",
            "Epoch: [456][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0625e-01 (-9.0338e-01)\n",
            "Epoch: [456][60/97]\tTime  0.178 ( 0.182)\tLoss -8.9370e-01 (-9.0298e-01)\n",
            "Epoch: [456][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0662e-01 (-9.0239e-01)\n",
            "Epoch: [456][80/97]\tTime  0.176 ( 0.180)\tLoss -9.0187e-01 (-9.0237e-01)\n",
            "Epoch: [456][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1037e-01 (-9.0253e-01)\n",
            "Training...\n",
            "Epoch: [457][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0576e-01 (-9.0576e-01)\n",
            "Epoch: [457][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9587e-01 (-8.9993e-01)\n",
            "Epoch: [457][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0683e-01 (-9.0163e-01)\n",
            "Epoch: [457][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9098e-01 (-9.0228e-01)\n",
            "Epoch: [457][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0900e-01 (-9.0227e-01)\n",
            "Epoch: [457][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0037e-01 (-9.0226e-01)\n",
            "Epoch: [457][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9840e-01 (-9.0161e-01)\n",
            "Epoch: [457][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9699e-01 (-9.0214e-01)\n",
            "Epoch: [457][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0464e-01 (-9.0243e-01)\n",
            "Epoch: [457][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0478e-01 (-9.0230e-01)\n",
            "Training...\n",
            "Epoch: [458][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.0102e-01 (-9.0102e-01)\n",
            "Epoch: [458][10/97]\tTime  0.178 ( 0.202)\tLoss -8.9869e-01 (-9.0478e-01)\n",
            "Epoch: [458][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0288e-01 (-9.0294e-01)\n",
            "Epoch: [458][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9092e-01 (-9.0272e-01)\n",
            "Epoch: [458][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9573e-01 (-9.0295e-01)\n",
            "Epoch: [458][50/97]\tTime  0.178 ( 0.182)\tLoss -9.1178e-01 (-9.0284e-01)\n",
            "Epoch: [458][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9740e-01 (-9.0232e-01)\n",
            "Epoch: [458][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0307e-01 (-9.0251e-01)\n",
            "Epoch: [458][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0058e-01 (-9.0214e-01)\n",
            "Epoch: [458][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9840e-01 (-9.0192e-01)\n",
            "Training...\n",
            "Epoch: [459][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.9653e-01 (-8.9653e-01)\n",
            "Epoch: [459][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0847e-01 (-9.0621e-01)\n",
            "Epoch: [459][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0270e-01 (-9.0394e-01)\n",
            "Epoch: [459][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0362e-01 (-9.0372e-01)\n",
            "Epoch: [459][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0517e-01 (-9.0399e-01)\n",
            "Epoch: [459][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9625e-01 (-9.0449e-01)\n",
            "Epoch: [459][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0371e-01 (-9.0434e-01)\n",
            "Epoch: [459][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0002e-01 (-9.0386e-01)\n",
            "Epoch: [459][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0151e-01 (-9.0369e-01)\n",
            "Epoch: [459][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0464e-01 (-9.0358e-01)\n",
            "Training...\n",
            "Epoch: [460][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9681e-01 (-8.9681e-01)\n",
            "Epoch: [460][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0497e-01 (-9.0448e-01)\n",
            "Epoch: [460][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0752e-01 (-9.0448e-01)\n",
            "Epoch: [460][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0889e-01 (-9.0416e-01)\n",
            "Epoch: [460][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0183e-01 (-9.0406e-01)\n",
            "Epoch: [460][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0545e-01 (-9.0327e-01)\n",
            "Epoch: [460][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9421e-01 (-9.0254e-01)\n",
            "Epoch: [460][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1175e-01 (-9.0265e-01)\n",
            "Epoch: [460][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0509e-01 (-9.0234e-01)\n",
            "Epoch: [460][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0348e-01 (-9.0217e-01)\n",
            "Validating...\n",
            "Top1: 0.8505345394736842\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [461][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0279e-01 (-9.0279e-01)\n",
            "Epoch: [461][10/97]\tTime  0.178 ( 0.201)\tLoss -9.0024e-01 (-9.0250e-01)\n",
            "Epoch: [461][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9729e-01 (-9.0252e-01)\n",
            "Epoch: [461][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0715e-01 (-9.0191e-01)\n",
            "Epoch: [461][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0144e-01 (-9.0221e-01)\n",
            "Epoch: [461][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0580e-01 (-9.0239e-01)\n",
            "Epoch: [461][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0308e-01 (-9.0242e-01)\n",
            "Epoch: [461][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0118e-01 (-9.0224e-01)\n",
            "Epoch: [461][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1490e-01 (-9.0287e-01)\n",
            "Epoch: [461][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9326e-01 (-9.0288e-01)\n",
            "Training...\n",
            "Epoch: [462][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.0899e-01 (-9.0899e-01)\n",
            "Epoch: [462][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0550e-01 (-9.0344e-01)\n",
            "Epoch: [462][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9699e-01 (-9.0433e-01)\n",
            "Epoch: [462][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0729e-01 (-9.0348e-01)\n",
            "Epoch: [462][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0518e-01 (-9.0340e-01)\n",
            "Epoch: [462][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0086e-01 (-9.0298e-01)\n",
            "Epoch: [462][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0869e-01 (-9.0319e-01)\n",
            "Epoch: [462][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0216e-01 (-9.0295e-01)\n",
            "Epoch: [462][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0983e-01 (-9.0269e-01)\n",
            "Epoch: [462][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1122e-01 (-9.0258e-01)\n",
            "Training...\n",
            "Epoch: [463][ 0/97]\tTime  0.451 ( 0.451)\tLoss -8.9259e-01 (-8.9259e-01)\n",
            "Epoch: [463][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0605e-01 (-9.0550e-01)\n",
            "Epoch: [463][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9878e-01 (-9.0488e-01)\n",
            "Epoch: [463][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1154e-01 (-9.0451e-01)\n",
            "Epoch: [463][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0304e-01 (-9.0445e-01)\n",
            "Epoch: [463][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0893e-01 (-9.0411e-01)\n",
            "Epoch: [463][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0134e-01 (-9.0324e-01)\n",
            "Epoch: [463][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0491e-01 (-9.0314e-01)\n",
            "Epoch: [463][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9159e-01 (-9.0302e-01)\n",
            "Epoch: [463][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0512e-01 (-9.0306e-01)\n",
            "Training...\n",
            "Epoch: [464][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0300e-01 (-9.0300e-01)\n",
            "Epoch: [464][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9588e-01 (-9.0558e-01)\n",
            "Epoch: [464][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0902e-01 (-9.0514e-01)\n",
            "Epoch: [464][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9816e-01 (-9.0434e-01)\n",
            "Epoch: [464][40/97]\tTime  0.176 ( 0.183)\tLoss -8.9779e-01 (-9.0295e-01)\n",
            "Epoch: [464][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0399e-01 (-9.0290e-01)\n",
            "Epoch: [464][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9164e-01 (-9.0241e-01)\n",
            "Epoch: [464][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0263e-01 (-9.0193e-01)\n",
            "Epoch: [464][80/97]\tTime  0.176 ( 0.180)\tLoss -8.9822e-01 (-9.0172e-01)\n",
            "Epoch: [464][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9983e-01 (-9.0159e-01)\n",
            "Training...\n",
            "Epoch: [465][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.1047e-01 (-9.1047e-01)\n",
            "Epoch: [465][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9865e-01 (-9.0244e-01)\n",
            "Epoch: [465][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9475e-01 (-9.0297e-01)\n",
            "Epoch: [465][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0126e-01 (-9.0290e-01)\n",
            "Epoch: [465][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1849e-01 (-9.0286e-01)\n",
            "Epoch: [465][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0539e-01 (-9.0352e-01)\n",
            "Epoch: [465][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0718e-01 (-9.0357e-01)\n",
            "Epoch: [465][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0085e-01 (-9.0352e-01)\n",
            "Epoch: [465][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1069e-01 (-9.0362e-01)\n",
            "Epoch: [465][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0807e-01 (-9.0379e-01)\n",
            "Validating...\n",
            "Top1: 0.8404605263157895\n",
            "Training...\n",
            "Epoch: [466][ 0/97]\tTime  0.470 ( 0.470)\tLoss -8.9900e-01 (-8.9900e-01)\n",
            "Epoch: [466][10/97]\tTime  0.177 ( 0.204)\tLoss -8.9937e-01 (-9.0373e-01)\n",
            "Epoch: [466][20/97]\tTime  0.177 ( 0.191)\tLoss -9.0759e-01 (-9.0401e-01)\n",
            "Epoch: [466][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0148e-01 (-9.0197e-01)\n",
            "Epoch: [466][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1051e-01 (-9.0253e-01)\n",
            "Epoch: [466][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0831e-01 (-9.0318e-01)\n",
            "Epoch: [466][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9841e-01 (-9.0319e-01)\n",
            "Epoch: [466][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1077e-01 (-9.0345e-01)\n",
            "Epoch: [466][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1024e-01 (-9.0349e-01)\n",
            "Epoch: [466][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0023e-01 (-9.0315e-01)\n",
            "Training...\n",
            "Epoch: [467][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.0751e-01 (-9.0751e-01)\n",
            "Epoch: [467][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9308e-01 (-9.0325e-01)\n",
            "Epoch: [467][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0484e-01 (-9.0414e-01)\n",
            "Epoch: [467][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0571e-01 (-9.0464e-01)\n",
            "Epoch: [467][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1018e-01 (-9.0489e-01)\n",
            "Epoch: [467][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0413e-01 (-9.0482e-01)\n",
            "Epoch: [467][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9972e-01 (-9.0407e-01)\n",
            "Epoch: [467][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9802e-01 (-9.0418e-01)\n",
            "Epoch: [467][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1080e-01 (-9.0409e-01)\n",
            "Epoch: [467][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9867e-01 (-9.0383e-01)\n",
            "Training...\n",
            "Epoch: [468][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.0469e-01 (-9.0469e-01)\n",
            "Epoch: [468][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1441e-01 (-9.0676e-01)\n",
            "Epoch: [468][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9741e-01 (-9.0562e-01)\n",
            "Epoch: [468][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9068e-01 (-9.0423e-01)\n",
            "Epoch: [468][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9184e-01 (-9.0379e-01)\n",
            "Epoch: [468][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0186e-01 (-9.0400e-01)\n",
            "Epoch: [468][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0564e-01 (-9.0419e-01)\n",
            "Epoch: [468][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1178e-01 (-9.0414e-01)\n",
            "Epoch: [468][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9970e-01 (-9.0378e-01)\n",
            "Epoch: [468][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0887e-01 (-9.0391e-01)\n",
            "Training...\n",
            "Epoch: [469][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.0999e-01 (-9.0999e-01)\n",
            "Epoch: [469][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9590e-01 (-9.0240e-01)\n",
            "Epoch: [469][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0098e-01 (-9.0382e-01)\n",
            "Epoch: [469][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9920e-01 (-9.0436e-01)\n",
            "Epoch: [469][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1264e-01 (-9.0456e-01)\n",
            "Epoch: [469][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0211e-01 (-9.0424e-01)\n",
            "Epoch: [469][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1133e-01 (-9.0441e-01)\n",
            "Epoch: [469][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0981e-01 (-9.0422e-01)\n",
            "Epoch: [469][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0616e-01 (-9.0449e-01)\n",
            "Epoch: [469][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0168e-01 (-9.0461e-01)\n",
            "Training...\n",
            "Epoch: [470][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.1492e-01 (-9.1492e-01)\n",
            "Epoch: [470][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0628e-01 (-9.0360e-01)\n",
            "Epoch: [470][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0731e-01 (-9.0303e-01)\n",
            "Epoch: [470][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0204e-01 (-9.0257e-01)\n",
            "Epoch: [470][40/97]\tTime  0.176 ( 0.184)\tLoss -8.9845e-01 (-9.0287e-01)\n",
            "Epoch: [470][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0120e-01 (-9.0282e-01)\n",
            "Epoch: [470][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1033e-01 (-9.0369e-01)\n",
            "Epoch: [470][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0513e-01 (-9.0350e-01)\n",
            "Epoch: [470][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0903e-01 (-9.0383e-01)\n",
            "Epoch: [470][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9613e-01 (-9.0390e-01)\n",
            "Validating...\n",
            "Top1: 0.8452919407894737\n",
            "Training...\n",
            "Epoch: [471][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0177e-01 (-9.0177e-01)\n",
            "Epoch: [471][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9965e-01 (-8.9987e-01)\n",
            "Epoch: [471][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0507e-01 (-9.0215e-01)\n",
            "Epoch: [471][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0360e-01 (-9.0333e-01)\n",
            "Epoch: [471][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0811e-01 (-9.0435e-01)\n",
            "Epoch: [471][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9967e-01 (-9.0447e-01)\n",
            "Epoch: [471][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9611e-01 (-9.0456e-01)\n",
            "Epoch: [471][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0739e-01 (-9.0438e-01)\n",
            "Epoch: [471][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9568e-01 (-9.0426e-01)\n",
            "Epoch: [471][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0200e-01 (-9.0422e-01)\n",
            "Training...\n",
            "Epoch: [472][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.0842e-01 (-9.0842e-01)\n",
            "Epoch: [472][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0315e-01 (-9.0708e-01)\n",
            "Epoch: [472][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9463e-01 (-9.0528e-01)\n",
            "Epoch: [472][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9416e-01 (-9.0454e-01)\n",
            "Epoch: [472][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0043e-01 (-9.0386e-01)\n",
            "Epoch: [472][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9270e-01 (-9.0378e-01)\n",
            "Epoch: [472][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0064e-01 (-9.0431e-01)\n",
            "Epoch: [472][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0051e-01 (-9.0474e-01)\n",
            "Epoch: [472][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0195e-01 (-9.0492e-01)\n",
            "Epoch: [472][90/97]\tTime  0.178 ( 0.180)\tLoss -9.0241e-01 (-9.0484e-01)\n",
            "Training...\n",
            "Epoch: [473][ 0/97]\tTime  0.455 ( 0.455)\tLoss -8.8658e-01 (-8.8658e-01)\n",
            "Epoch: [473][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0424e-01 (-9.0291e-01)\n",
            "Epoch: [473][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1345e-01 (-9.0450e-01)\n",
            "Epoch: [473][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0656e-01 (-9.0424e-01)\n",
            "Epoch: [473][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9914e-01 (-9.0312e-01)\n",
            "Epoch: [473][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0248e-01 (-9.0356e-01)\n",
            "Epoch: [473][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9987e-01 (-9.0351e-01)\n",
            "Epoch: [473][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9780e-01 (-9.0286e-01)\n",
            "Epoch: [473][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0055e-01 (-9.0291e-01)\n",
            "Epoch: [473][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0243e-01 (-9.0299e-01)\n",
            "Training...\n",
            "Epoch: [474][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0979e-01 (-9.0979e-01)\n",
            "Epoch: [474][10/97]\tTime  0.176 ( 0.202)\tLoss -9.1402e-01 (-9.1044e-01)\n",
            "Epoch: [474][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0684e-01 (-9.0681e-01)\n",
            "Epoch: [474][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0092e-01 (-9.0604e-01)\n",
            "Epoch: [474][40/97]\tTime  0.176 ( 0.184)\tLoss -9.0357e-01 (-9.0584e-01)\n",
            "Epoch: [474][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9812e-01 (-9.0527e-01)\n",
            "Epoch: [474][60/97]\tTime  0.176 ( 0.181)\tLoss -9.0132e-01 (-9.0538e-01)\n",
            "Epoch: [474][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9521e-01 (-9.0528e-01)\n",
            "Epoch: [474][80/97]\tTime  0.176 ( 0.180)\tLoss -9.0795e-01 (-9.0471e-01)\n",
            "Epoch: [474][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0457e-01 (-9.0519e-01)\n",
            "Training...\n",
            "Epoch: [475][ 0/97]\tTime  0.452 ( 0.452)\tLoss -8.9925e-01 (-8.9925e-01)\n",
            "Epoch: [475][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0470e-01 (-9.0687e-01)\n",
            "Epoch: [475][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0240e-01 (-9.0659e-01)\n",
            "Epoch: [475][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0844e-01 (-9.0617e-01)\n",
            "Epoch: [475][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0620e-01 (-9.0545e-01)\n",
            "Epoch: [475][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9944e-01 (-9.0462e-01)\n",
            "Epoch: [475][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0284e-01 (-9.0484e-01)\n",
            "Epoch: [475][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0445e-01 (-9.0438e-01)\n",
            "Epoch: [475][80/97]\tTime  0.176 ( 0.180)\tLoss -9.0291e-01 (-9.0467e-01)\n",
            "Epoch: [475][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9897e-01 (-9.0470e-01)\n",
            "Validating...\n",
            "Top1: 0.8530016447368421\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [476][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.0423e-01 (-9.0423e-01)\n",
            "Epoch: [476][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0196e-01 (-9.0176e-01)\n",
            "Epoch: [476][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0786e-01 (-9.0215e-01)\n",
            "Epoch: [476][30/97]\tTime  0.177 ( 0.185)\tLoss -9.0633e-01 (-9.0380e-01)\n",
            "Epoch: [476][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0538e-01 (-9.0441e-01)\n",
            "Epoch: [476][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9863e-01 (-9.0434e-01)\n",
            "Epoch: [476][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0816e-01 (-9.0434e-01)\n",
            "Epoch: [476][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1441e-01 (-9.0439e-01)\n",
            "Epoch: [476][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0844e-01 (-9.0472e-01)\n",
            "Epoch: [476][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9341e-01 (-9.0419e-01)\n",
            "Training...\n",
            "Epoch: [477][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.0434e-01 (-9.0434e-01)\n",
            "Epoch: [477][10/97]\tTime  0.177 ( 0.202)\tLoss -8.9675e-01 (-9.0725e-01)\n",
            "Epoch: [477][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0979e-01 (-9.0665e-01)\n",
            "Epoch: [477][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9502e-01 (-9.0601e-01)\n",
            "Epoch: [477][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1359e-01 (-9.0680e-01)\n",
            "Epoch: [477][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9823e-01 (-9.0661e-01)\n",
            "Epoch: [477][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1289e-01 (-9.0695e-01)\n",
            "Epoch: [477][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9764e-01 (-9.0617e-01)\n",
            "Epoch: [477][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0319e-01 (-9.0585e-01)\n",
            "Epoch: [477][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9441e-01 (-9.0543e-01)\n",
            "Training...\n",
            "Epoch: [478][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0779e-01 (-9.0779e-01)\n",
            "Epoch: [478][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0665e-01 (-9.0544e-01)\n",
            "Epoch: [478][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1430e-01 (-9.0547e-01)\n",
            "Epoch: [478][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1196e-01 (-9.0630e-01)\n",
            "Epoch: [478][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9536e-01 (-9.0611e-01)\n",
            "Epoch: [478][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0188e-01 (-9.0544e-01)\n",
            "Epoch: [478][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0310e-01 (-9.0527e-01)\n",
            "Epoch: [478][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1118e-01 (-9.0544e-01)\n",
            "Epoch: [478][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0757e-01 (-9.0511e-01)\n",
            "Epoch: [478][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9805e-01 (-9.0519e-01)\n",
            "Training...\n",
            "Epoch: [479][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0628e-01 (-9.0628e-01)\n",
            "Epoch: [479][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0865e-01 (-9.0516e-01)\n",
            "Epoch: [479][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0012e-01 (-9.0403e-01)\n",
            "Epoch: [479][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9418e-01 (-9.0484e-01)\n",
            "Epoch: [479][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1175e-01 (-9.0531e-01)\n",
            "Epoch: [479][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9420e-01 (-9.0533e-01)\n",
            "Epoch: [479][60/97]\tTime  0.176 ( 0.181)\tLoss -9.0900e-01 (-9.0545e-01)\n",
            "Epoch: [479][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9890e-01 (-9.0526e-01)\n",
            "Epoch: [479][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1173e-01 (-9.0526e-01)\n",
            "Epoch: [479][90/97]\tTime  0.176 ( 0.180)\tLoss -9.0300e-01 (-9.0489e-01)\n",
            "Training...\n",
            "Epoch: [480][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.0415e-01 (-9.0415e-01)\n",
            "Epoch: [480][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1647e-01 (-9.0470e-01)\n",
            "Epoch: [480][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0525e-01 (-9.0679e-01)\n",
            "Epoch: [480][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1108e-01 (-9.0683e-01)\n",
            "Epoch: [480][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9811e-01 (-9.0578e-01)\n",
            "Epoch: [480][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0847e-01 (-9.0550e-01)\n",
            "Epoch: [480][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1242e-01 (-9.0559e-01)\n",
            "Epoch: [480][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1630e-01 (-9.0589e-01)\n",
            "Epoch: [480][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0623e-01 (-9.0604e-01)\n",
            "Epoch: [480][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1395e-01 (-9.0601e-01)\n",
            "Validating...\n",
            "Top1: 0.8490953947368421\n",
            "Training...\n",
            "Epoch: [481][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.0175e-01 (-9.0175e-01)\n",
            "Epoch: [481][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0719e-01 (-9.0859e-01)\n",
            "Epoch: [481][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0851e-01 (-9.0772e-01)\n",
            "Epoch: [481][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1006e-01 (-9.0730e-01)\n",
            "Epoch: [481][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9824e-01 (-9.0659e-01)\n",
            "Epoch: [481][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0597e-01 (-9.0661e-01)\n",
            "Epoch: [481][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0301e-01 (-9.0624e-01)\n",
            "Epoch: [481][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0876e-01 (-9.0619e-01)\n",
            "Epoch: [481][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9631e-01 (-9.0569e-01)\n",
            "Epoch: [481][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0846e-01 (-9.0544e-01)\n",
            "Training...\n",
            "Epoch: [482][ 0/97]\tTime  0.459 ( 0.459)\tLoss -9.0596e-01 (-9.0596e-01)\n",
            "Epoch: [482][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1492e-01 (-9.0794e-01)\n",
            "Epoch: [482][20/97]\tTime  0.177 ( 0.191)\tLoss -9.0936e-01 (-9.0699e-01)\n",
            "Epoch: [482][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0559e-01 (-9.0576e-01)\n",
            "Epoch: [482][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1424e-01 (-9.0563e-01)\n",
            "Epoch: [482][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0842e-01 (-9.0542e-01)\n",
            "Epoch: [482][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1693e-01 (-9.0572e-01)\n",
            "Epoch: [482][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0613e-01 (-9.0562e-01)\n",
            "Epoch: [482][80/97]\tTime  0.178 ( 0.181)\tLoss -8.9632e-01 (-9.0502e-01)\n",
            "Epoch: [482][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0504e-01 (-9.0463e-01)\n",
            "Training...\n",
            "Epoch: [483][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.0386e-01 (-9.0386e-01)\n",
            "Epoch: [483][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0425e-01 (-9.0604e-01)\n",
            "Epoch: [483][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1010e-01 (-9.0539e-01)\n",
            "Epoch: [483][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9733e-01 (-9.0470e-01)\n",
            "Epoch: [483][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0920e-01 (-9.0503e-01)\n",
            "Epoch: [483][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0674e-01 (-9.0519e-01)\n",
            "Epoch: [483][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0422e-01 (-9.0502e-01)\n",
            "Epoch: [483][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9559e-01 (-9.0555e-01)\n",
            "Epoch: [483][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0366e-01 (-9.0518e-01)\n",
            "Epoch: [483][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1797e-01 (-9.0550e-01)\n",
            "Training...\n",
            "Epoch: [484][ 0/97]\tTime  0.458 ( 0.458)\tLoss -9.0320e-01 (-9.0320e-01)\n",
            "Epoch: [484][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0990e-01 (-9.0572e-01)\n",
            "Epoch: [484][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0143e-01 (-9.0555e-01)\n",
            "Epoch: [484][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1270e-01 (-9.0571e-01)\n",
            "Epoch: [484][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9895e-01 (-9.0599e-01)\n",
            "Epoch: [484][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0715e-01 (-9.0583e-01)\n",
            "Epoch: [484][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0512e-01 (-9.0594e-01)\n",
            "Epoch: [484][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0964e-01 (-9.0583e-01)\n",
            "Epoch: [484][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0505e-01 (-9.0509e-01)\n",
            "Epoch: [484][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1022e-01 (-9.0497e-01)\n",
            "Training...\n",
            "Epoch: [485][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.1720e-01 (-9.1720e-01)\n",
            "Epoch: [485][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0904e-01 (-9.0878e-01)\n",
            "Epoch: [485][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0023e-01 (-9.0784e-01)\n",
            "Epoch: [485][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0395e-01 (-9.0763e-01)\n",
            "Epoch: [485][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1353e-01 (-9.0684e-01)\n",
            "Epoch: [485][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2168e-01 (-9.0687e-01)\n",
            "Epoch: [485][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9878e-01 (-9.0704e-01)\n",
            "Epoch: [485][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9298e-01 (-9.0694e-01)\n",
            "Epoch: [485][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1297e-01 (-9.0748e-01)\n",
            "Epoch: [485][90/97]\tTime  0.178 ( 0.180)\tLoss -8.9453e-01 (-9.0708e-01)\n",
            "Validating...\n",
            "Top1: 0.8484786184210527\n",
            "Training...\n",
            "Epoch: [486][ 0/97]\tTime  0.445 ( 0.445)\tLoss -8.9981e-01 (-8.9981e-01)\n",
            "Epoch: [486][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1287e-01 (-9.0473e-01)\n",
            "Epoch: [486][20/97]\tTime  0.176 ( 0.190)\tLoss -9.1704e-01 (-9.0575e-01)\n",
            "Epoch: [486][30/97]\tTime  0.176 ( 0.186)\tLoss -9.1295e-01 (-9.0615e-01)\n",
            "Epoch: [486][40/97]\tTime  0.177 ( 0.183)\tLoss -9.1212e-01 (-9.0586e-01)\n",
            "Epoch: [486][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0598e-01 (-9.0596e-01)\n",
            "Epoch: [486][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0876e-01 (-9.0561e-01)\n",
            "Epoch: [486][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1347e-01 (-9.0546e-01)\n",
            "Epoch: [486][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1366e-01 (-9.0531e-01)\n",
            "Epoch: [486][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1435e-01 (-9.0551e-01)\n",
            "Training...\n",
            "Epoch: [487][ 0/97]\tTime  0.483 ( 0.483)\tLoss -9.1844e-01 (-9.1844e-01)\n",
            "Epoch: [487][10/97]\tTime  0.177 ( 0.205)\tLoss -9.1755e-01 (-9.1214e-01)\n",
            "Epoch: [487][20/97]\tTime  0.177 ( 0.191)\tLoss -8.9813e-01 (-9.1032e-01)\n",
            "Epoch: [487][30/97]\tTime  0.177 ( 0.187)\tLoss -9.0697e-01 (-9.0966e-01)\n",
            "Epoch: [487][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0675e-01 (-9.0863e-01)\n",
            "Epoch: [487][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1196e-01 (-9.0813e-01)\n",
            "Epoch: [487][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9859e-01 (-9.0742e-01)\n",
            "Epoch: [487][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1122e-01 (-9.0692e-01)\n",
            "Epoch: [487][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0757e-01 (-9.0735e-01)\n",
            "Epoch: [487][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1236e-01 (-9.0719e-01)\n",
            "Training...\n",
            "Epoch: [488][ 0/97]\tTime  0.460 ( 0.460)\tLoss -9.1360e-01 (-9.1360e-01)\n",
            "Epoch: [488][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1100e-01 (-9.0401e-01)\n",
            "Epoch: [488][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1587e-01 (-9.0677e-01)\n",
            "Epoch: [488][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9639e-01 (-9.0659e-01)\n",
            "Epoch: [488][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9957e-01 (-9.0518e-01)\n",
            "Epoch: [488][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0725e-01 (-9.0509e-01)\n",
            "Epoch: [488][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1198e-01 (-9.0550e-01)\n",
            "Epoch: [488][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9680e-01 (-9.0525e-01)\n",
            "Epoch: [488][80/97]\tTime  0.176 ( 0.180)\tLoss -9.0887e-01 (-9.0521e-01)\n",
            "Epoch: [488][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0923e-01 (-9.0547e-01)\n",
            "Training...\n",
            "Epoch: [489][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.0690e-01 (-9.0690e-01)\n",
            "Epoch: [489][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1639e-01 (-9.0674e-01)\n",
            "Epoch: [489][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0851e-01 (-9.0764e-01)\n",
            "Epoch: [489][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0213e-01 (-9.0795e-01)\n",
            "Epoch: [489][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1059e-01 (-9.0770e-01)\n",
            "Epoch: [489][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0204e-01 (-9.0729e-01)\n",
            "Epoch: [489][60/97]\tTime  0.178 ( 0.182)\tLoss -9.0201e-01 (-9.0722e-01)\n",
            "Epoch: [489][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1612e-01 (-9.0776e-01)\n",
            "Epoch: [489][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1343e-01 (-9.0806e-01)\n",
            "Epoch: [489][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0498e-01 (-9.0775e-01)\n",
            "Training...\n",
            "Epoch: [490][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.0015e-01 (-9.0015e-01)\n",
            "Epoch: [490][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1543e-01 (-9.0776e-01)\n",
            "Epoch: [490][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9849e-01 (-9.0541e-01)\n",
            "Epoch: [490][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0521e-01 (-9.0667e-01)\n",
            "Epoch: [490][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0505e-01 (-9.0628e-01)\n",
            "Epoch: [490][50/97]\tTime  0.176 ( 0.182)\tLoss -8.9939e-01 (-9.0604e-01)\n",
            "Epoch: [490][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9869e-01 (-9.0599e-01)\n",
            "Epoch: [490][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0132e-01 (-9.0532e-01)\n",
            "Epoch: [490][80/97]\tTime  0.178 ( 0.180)\tLoss -8.9531e-01 (-9.0534e-01)\n",
            "Epoch: [490][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0450e-01 (-9.0547e-01)\n",
            "Validating...\n",
            "Top1: 0.8495065789473685\n",
            "Training...\n",
            "Epoch: [491][ 0/97]\tTime  0.453 ( 0.453)\tLoss -8.9978e-01 (-8.9978e-01)\n",
            "Epoch: [491][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1807e-01 (-9.0924e-01)\n",
            "Epoch: [491][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1271e-01 (-9.0924e-01)\n",
            "Epoch: [491][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1148e-01 (-9.0953e-01)\n",
            "Epoch: [491][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1659e-01 (-9.0850e-01)\n",
            "Epoch: [491][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2269e-01 (-9.0865e-01)\n",
            "Epoch: [491][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0416e-01 (-9.0855e-01)\n",
            "Epoch: [491][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1105e-01 (-9.0885e-01)\n",
            "Epoch: [491][80/97]\tTime  0.177 ( 0.181)\tLoss -8.9879e-01 (-9.0864e-01)\n",
            "Epoch: [491][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9002e-01 (-9.0803e-01)\n",
            "Training...\n",
            "Epoch: [492][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.0431e-01 (-9.0431e-01)\n",
            "Epoch: [492][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1369e-01 (-9.0790e-01)\n",
            "Epoch: [492][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0492e-01 (-9.0699e-01)\n",
            "Epoch: [492][30/97]\tTime  0.176 ( 0.186)\tLoss -9.0466e-01 (-9.0721e-01)\n",
            "Epoch: [492][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1716e-01 (-9.0735e-01)\n",
            "Epoch: [492][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1115e-01 (-9.0788e-01)\n",
            "Epoch: [492][60/97]\tTime  0.178 ( 0.181)\tLoss -9.0090e-01 (-9.0740e-01)\n",
            "Epoch: [492][70/97]\tTime  0.176 ( 0.181)\tLoss -9.0051e-01 (-9.0760e-01)\n",
            "Epoch: [492][80/97]\tTime  0.176 ( 0.180)\tLoss -9.1346e-01 (-9.0718e-01)\n",
            "Epoch: [492][90/97]\tTime  0.176 ( 0.180)\tLoss -8.8850e-01 (-9.0714e-01)\n",
            "Training...\n",
            "Epoch: [493][ 0/97]\tTime  0.463 ( 0.463)\tLoss -9.1429e-01 (-9.1429e-01)\n",
            "Epoch: [493][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1114e-01 (-9.1144e-01)\n",
            "Epoch: [493][20/97]\tTime  0.177 ( 0.191)\tLoss -9.0874e-01 (-9.0942e-01)\n",
            "Epoch: [493][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0323e-01 (-9.0741e-01)\n",
            "Epoch: [493][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0312e-01 (-9.0729e-01)\n",
            "Epoch: [493][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0849e-01 (-9.0725e-01)\n",
            "Epoch: [493][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0758e-01 (-9.0685e-01)\n",
            "Epoch: [493][70/97]\tTime  0.176 ( 0.181)\tLoss -8.9479e-01 (-9.0653e-01)\n",
            "Epoch: [493][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0097e-01 (-9.0664e-01)\n",
            "Epoch: [493][90/97]\tTime  0.176 ( 0.180)\tLoss -9.0196e-01 (-9.0662e-01)\n",
            "Training...\n",
            "Epoch: [494][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.1204e-01 (-9.1204e-01)\n",
            "Epoch: [494][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0427e-01 (-9.0608e-01)\n",
            "Epoch: [494][20/97]\tTime  0.176 ( 0.190)\tLoss -9.0626e-01 (-9.0688e-01)\n",
            "Epoch: [494][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1225e-01 (-9.0624e-01)\n",
            "Epoch: [494][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0789e-01 (-9.0668e-01)\n",
            "Epoch: [494][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9863e-01 (-9.0644e-01)\n",
            "Epoch: [494][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0880e-01 (-9.0632e-01)\n",
            "Epoch: [494][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0674e-01 (-9.0640e-01)\n",
            "Epoch: [494][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0002e-01 (-9.0637e-01)\n",
            "Epoch: [494][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0492e-01 (-9.0627e-01)\n",
            "Training...\n",
            "Epoch: [495][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.0934e-01 (-9.0934e-01)\n",
            "Epoch: [495][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1524e-01 (-9.0893e-01)\n",
            "Epoch: [495][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9619e-01 (-9.0724e-01)\n",
            "Epoch: [495][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0889e-01 (-9.0689e-01)\n",
            "Epoch: [495][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9910e-01 (-9.0702e-01)\n",
            "Epoch: [495][50/97]\tTime  0.176 ( 0.182)\tLoss -9.1095e-01 (-9.0729e-01)\n",
            "Epoch: [495][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1389e-01 (-9.0760e-01)\n",
            "Epoch: [495][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0210e-01 (-9.0739e-01)\n",
            "Epoch: [495][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9644e-01 (-9.0724e-01)\n",
            "Epoch: [495][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0992e-01 (-9.0731e-01)\n",
            "Validating...\n",
            "Top1: 0.8487870065789473\n",
            "Training...\n",
            "Epoch: [496][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.0644e-01 (-9.0644e-01)\n",
            "Epoch: [496][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1593e-01 (-9.0855e-01)\n",
            "Epoch: [496][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0831e-01 (-9.0761e-01)\n",
            "Epoch: [496][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1156e-01 (-9.0805e-01)\n",
            "Epoch: [496][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0699e-01 (-9.0750e-01)\n",
            "Epoch: [496][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1400e-01 (-9.0775e-01)\n",
            "Epoch: [496][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0487e-01 (-9.0739e-01)\n",
            "Epoch: [496][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2105e-01 (-9.0759e-01)\n",
            "Epoch: [496][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9757e-01 (-9.0788e-01)\n",
            "Epoch: [496][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0807e-01 (-9.0789e-01)\n",
            "Training...\n",
            "Epoch: [497][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.1601e-01 (-9.1601e-01)\n",
            "Epoch: [497][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0830e-01 (-9.0533e-01)\n",
            "Epoch: [497][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0612e-01 (-9.0613e-01)\n",
            "Epoch: [497][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1131e-01 (-9.0595e-01)\n",
            "Epoch: [497][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1223e-01 (-9.0646e-01)\n",
            "Epoch: [497][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1506e-01 (-9.0744e-01)\n",
            "Epoch: [497][60/97]\tTime  0.176 ( 0.181)\tLoss -9.0970e-01 (-9.0816e-01)\n",
            "Epoch: [497][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0293e-01 (-9.0782e-01)\n",
            "Epoch: [497][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1108e-01 (-9.0819e-01)\n",
            "Epoch: [497][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1585e-01 (-9.0815e-01)\n",
            "Training...\n",
            "Epoch: [498][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.1750e-01 (-9.1750e-01)\n",
            "Epoch: [498][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1609e-01 (-9.1307e-01)\n",
            "Epoch: [498][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1290e-01 (-9.1076e-01)\n",
            "Epoch: [498][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1479e-01 (-9.0847e-01)\n",
            "Epoch: [498][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1598e-01 (-9.0948e-01)\n",
            "Epoch: [498][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0662e-01 (-9.0978e-01)\n",
            "Epoch: [498][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9828e-01 (-9.0917e-01)\n",
            "Epoch: [498][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0634e-01 (-9.0915e-01)\n",
            "Epoch: [498][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0425e-01 (-9.0869e-01)\n",
            "Epoch: [498][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0842e-01 (-9.0868e-01)\n",
            "Training...\n",
            "Epoch: [499][ 0/97]\tTime  0.459 ( 0.459)\tLoss -9.1570e-01 (-9.1570e-01)\n",
            "Epoch: [499][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1504e-01 (-9.1059e-01)\n",
            "Epoch: [499][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1362e-01 (-9.1086e-01)\n",
            "Epoch: [499][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0105e-01 (-9.0940e-01)\n",
            "Epoch: [499][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0490e-01 (-9.0993e-01)\n",
            "Epoch: [499][50/97]\tTime  0.177 ( 0.182)\tLoss -8.9389e-01 (-9.0855e-01)\n",
            "Epoch: [499][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0688e-01 (-9.0792e-01)\n",
            "Epoch: [499][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0385e-01 (-9.0752e-01)\n",
            "Epoch: [499][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9780e-01 (-9.0757e-01)\n",
            "Epoch: [499][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1010e-01 (-9.0787e-01)\n",
            "Training...\n",
            "Epoch: [500][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.1832e-01 (-9.1832e-01)\n",
            "Epoch: [500][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1550e-01 (-9.0958e-01)\n",
            "Epoch: [500][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2285e-01 (-9.0937e-01)\n",
            "Epoch: [500][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1120e-01 (-9.0984e-01)\n",
            "Epoch: [500][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0314e-01 (-9.0882e-01)\n",
            "Epoch: [500][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0583e-01 (-9.0895e-01)\n",
            "Epoch: [500][60/97]\tTime  0.178 ( 0.181)\tLoss -9.1190e-01 (-9.0866e-01)\n",
            "Epoch: [500][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2028e-01 (-9.0924e-01)\n",
            "Epoch: [500][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1616e-01 (-9.0909e-01)\n",
            "Epoch: [500][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9416e-01 (-9.0866e-01)\n",
            "Validating...\n",
            "Top1: 0.8525904605263158\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [501][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.1752e-01 (-9.1752e-01)\n",
            "Epoch: [501][10/97]\tTime  0.176 ( 0.201)\tLoss -9.1817e-01 (-9.1246e-01)\n",
            "Epoch: [501][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1354e-01 (-9.0960e-01)\n",
            "Epoch: [501][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0843e-01 (-9.0922e-01)\n",
            "Epoch: [501][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0942e-01 (-9.0930e-01)\n",
            "Epoch: [501][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1398e-01 (-9.0947e-01)\n",
            "Epoch: [501][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1191e-01 (-9.0948e-01)\n",
            "Epoch: [501][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0867e-01 (-9.0903e-01)\n",
            "Epoch: [501][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0245e-01 (-9.0852e-01)\n",
            "Epoch: [501][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0533e-01 (-9.0889e-01)\n",
            "Training...\n",
            "Epoch: [502][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1543e-01 (-9.1543e-01)\n",
            "Epoch: [502][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0672e-01 (-9.0927e-01)\n",
            "Epoch: [502][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9805e-01 (-9.0765e-01)\n",
            "Epoch: [502][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0245e-01 (-9.0656e-01)\n",
            "Epoch: [502][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1367e-01 (-9.0666e-01)\n",
            "Epoch: [502][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1395e-01 (-9.0773e-01)\n",
            "Epoch: [502][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1622e-01 (-9.0784e-01)\n",
            "Epoch: [502][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0760e-01 (-9.0785e-01)\n",
            "Epoch: [502][80/97]\tTime  0.178 ( 0.180)\tLoss -9.1055e-01 (-9.0818e-01)\n",
            "Epoch: [502][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0902e-01 (-9.0839e-01)\n",
            "Training...\n",
            "Epoch: [503][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1143e-01 (-9.1143e-01)\n",
            "Epoch: [503][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1548e-01 (-9.1171e-01)\n",
            "Epoch: [503][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0776e-01 (-9.0989e-01)\n",
            "Epoch: [503][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1340e-01 (-9.1000e-01)\n",
            "Epoch: [503][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0712e-01 (-9.0904e-01)\n",
            "Epoch: [503][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0947e-01 (-9.0926e-01)\n",
            "Epoch: [503][60/97]\tTime  0.178 ( 0.181)\tLoss -9.1328e-01 (-9.0933e-01)\n",
            "Epoch: [503][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0605e-01 (-9.0928e-01)\n",
            "Epoch: [503][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0654e-01 (-9.0938e-01)\n",
            "Epoch: [503][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0915e-01 (-9.0946e-01)\n",
            "Training...\n",
            "Epoch: [504][ 0/97]\tTime  0.447 ( 0.447)\tLoss -8.9392e-01 (-8.9392e-01)\n",
            "Epoch: [504][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0103e-01 (-9.0761e-01)\n",
            "Epoch: [504][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0865e-01 (-9.0615e-01)\n",
            "Epoch: [504][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0918e-01 (-9.0720e-01)\n",
            "Epoch: [504][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0936e-01 (-9.0689e-01)\n",
            "Epoch: [504][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1870e-01 (-9.0674e-01)\n",
            "Epoch: [504][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0249e-01 (-9.0640e-01)\n",
            "Epoch: [504][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0978e-01 (-9.0680e-01)\n",
            "Epoch: [504][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0997e-01 (-9.0716e-01)\n",
            "Epoch: [504][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1362e-01 (-9.0728e-01)\n",
            "Training...\n",
            "Epoch: [505][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.0609e-01 (-9.0609e-01)\n",
            "Epoch: [505][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0927e-01 (-9.0824e-01)\n",
            "Epoch: [505][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0794e-01 (-9.1006e-01)\n",
            "Epoch: [505][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0753e-01 (-9.0947e-01)\n",
            "Epoch: [505][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1670e-01 (-9.0910e-01)\n",
            "Epoch: [505][50/97]\tTime  0.178 ( 0.182)\tLoss -9.1152e-01 (-9.0895e-01)\n",
            "Epoch: [505][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1067e-01 (-9.0887e-01)\n",
            "Epoch: [505][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1560e-01 (-9.0876e-01)\n",
            "Epoch: [505][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1339e-01 (-9.0845e-01)\n",
            "Epoch: [505][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0611e-01 (-9.0828e-01)\n",
            "Validating...\n",
            "Top1: 0.8538240131578947\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [506][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1199e-01 (-9.1199e-01)\n",
            "Epoch: [506][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1090e-01 (-9.1034e-01)\n",
            "Epoch: [506][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1336e-01 (-9.1049e-01)\n",
            "Epoch: [506][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0244e-01 (-9.0964e-01)\n",
            "Epoch: [506][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0817e-01 (-9.0940e-01)\n",
            "Epoch: [506][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1394e-01 (-9.0985e-01)\n",
            "Epoch: [506][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1269e-01 (-9.0928e-01)\n",
            "Epoch: [506][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0623e-01 (-9.0906e-01)\n",
            "Epoch: [506][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0117e-01 (-9.0910e-01)\n",
            "Epoch: [506][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0545e-01 (-9.0877e-01)\n",
            "Training...\n",
            "Epoch: [507][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.0651e-01 (-9.0651e-01)\n",
            "Epoch: [507][10/97]\tTime  0.177 ( 0.203)\tLoss -9.0565e-01 (-9.1180e-01)\n",
            "Epoch: [507][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1078e-01 (-9.0980e-01)\n",
            "Epoch: [507][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0985e-01 (-9.0950e-01)\n",
            "Epoch: [507][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1192e-01 (-9.0939e-01)\n",
            "Epoch: [507][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1390e-01 (-9.0974e-01)\n",
            "Epoch: [507][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0916e-01 (-9.1005e-01)\n",
            "Epoch: [507][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0523e-01 (-9.0979e-01)\n",
            "Epoch: [507][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0848e-01 (-9.0955e-01)\n",
            "Epoch: [507][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0559e-01 (-9.0960e-01)\n",
            "Training...\n",
            "Epoch: [508][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.1374e-01 (-9.1374e-01)\n",
            "Epoch: [508][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1073e-01 (-9.0805e-01)\n",
            "Epoch: [508][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1478e-01 (-9.0925e-01)\n",
            "Epoch: [508][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0238e-01 (-9.0707e-01)\n",
            "Epoch: [508][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0112e-01 (-9.0716e-01)\n",
            "Epoch: [508][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0977e-01 (-9.0707e-01)\n",
            "Epoch: [508][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1846e-01 (-9.0764e-01)\n",
            "Epoch: [508][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0286e-01 (-9.0785e-01)\n",
            "Epoch: [508][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0849e-01 (-9.0800e-01)\n",
            "Epoch: [508][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0534e-01 (-9.0800e-01)\n",
            "Training...\n",
            "Epoch: [509][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0454e-01 (-9.0454e-01)\n",
            "Epoch: [509][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1198e-01 (-9.1004e-01)\n",
            "Epoch: [509][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2304e-01 (-9.1121e-01)\n",
            "Epoch: [509][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0824e-01 (-9.1061e-01)\n",
            "Epoch: [509][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0706e-01 (-9.1064e-01)\n",
            "Epoch: [509][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2274e-01 (-9.1063e-01)\n",
            "Epoch: [509][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0161e-01 (-9.0971e-01)\n",
            "Epoch: [509][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0663e-01 (-9.0922e-01)\n",
            "Epoch: [509][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0457e-01 (-9.0873e-01)\n",
            "Epoch: [509][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1176e-01 (-9.0845e-01)\n",
            "Training...\n",
            "Epoch: [510][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1551e-01 (-9.1551e-01)\n",
            "Epoch: [510][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1334e-01 (-9.0902e-01)\n",
            "Epoch: [510][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9988e-01 (-9.0713e-01)\n",
            "Epoch: [510][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1916e-01 (-9.0703e-01)\n",
            "Epoch: [510][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1134e-01 (-9.0743e-01)\n",
            "Epoch: [510][50/97]\tTime  0.178 ( 0.182)\tLoss -9.0774e-01 (-9.0737e-01)\n",
            "Epoch: [510][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0456e-01 (-9.0777e-01)\n",
            "Epoch: [510][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0570e-01 (-9.0784e-01)\n",
            "Epoch: [510][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1386e-01 (-9.0832e-01)\n",
            "Epoch: [510][90/97]\tTime  0.178 ( 0.180)\tLoss -9.0851e-01 (-9.0811e-01)\n",
            "Validating...\n",
            "Top1: 0.8526932565789473\n",
            "Training...\n",
            "Epoch: [511][ 0/97]\tTime  0.457 ( 0.457)\tLoss -8.9500e-01 (-8.9500e-01)\n",
            "Epoch: [511][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1776e-01 (-9.0883e-01)\n",
            "Epoch: [511][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0996e-01 (-9.0816e-01)\n",
            "Epoch: [511][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0330e-01 (-9.0817e-01)\n",
            "Epoch: [511][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0016e-01 (-9.0797e-01)\n",
            "Epoch: [511][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0874e-01 (-9.0830e-01)\n",
            "Epoch: [511][60/97]\tTime  0.177 ( 0.182)\tLoss -8.9719e-01 (-9.0757e-01)\n",
            "Epoch: [511][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0863e-01 (-9.0745e-01)\n",
            "Epoch: [511][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0440e-01 (-9.0718e-01)\n",
            "Epoch: [511][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1536e-01 (-9.0725e-01)\n",
            "Training...\n",
            "Epoch: [512][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.0899e-01 (-9.0899e-01)\n",
            "Epoch: [512][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0622e-01 (-9.1364e-01)\n",
            "Epoch: [512][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9492e-01 (-9.1026e-01)\n",
            "Epoch: [512][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0694e-01 (-9.0967e-01)\n",
            "Epoch: [512][40/97]\tTime  0.177 ( 0.183)\tLoss -9.1565e-01 (-9.0969e-01)\n",
            "Epoch: [512][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1562e-01 (-9.0995e-01)\n",
            "Epoch: [512][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9654e-01 (-9.0933e-01)\n",
            "Epoch: [512][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0433e-01 (-9.0908e-01)\n",
            "Epoch: [512][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1709e-01 (-9.0941e-01)\n",
            "Epoch: [512][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1016e-01 (-9.0915e-01)\n",
            "Training...\n",
            "Epoch: [513][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.1106e-01 (-9.1106e-01)\n",
            "Epoch: [513][10/97]\tTime  0.176 ( 0.202)\tLoss -9.1536e-01 (-9.0995e-01)\n",
            "Epoch: [513][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1626e-01 (-9.0957e-01)\n",
            "Epoch: [513][30/97]\tTime  0.176 ( 0.186)\tLoss -9.1336e-01 (-9.0982e-01)\n",
            "Epoch: [513][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0946e-01 (-9.0998e-01)\n",
            "Epoch: [513][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0389e-01 (-9.0943e-01)\n",
            "Epoch: [513][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0425e-01 (-9.0909e-01)\n",
            "Epoch: [513][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9653e-01 (-9.0861e-01)\n",
            "Epoch: [513][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0417e-01 (-9.0907e-01)\n",
            "Epoch: [513][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1085e-01 (-9.0913e-01)\n",
            "Training...\n",
            "Epoch: [514][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.1190e-01 (-9.1190e-01)\n",
            "Epoch: [514][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0106e-01 (-9.0838e-01)\n",
            "Epoch: [514][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1132e-01 (-9.0887e-01)\n",
            "Epoch: [514][30/97]\tTime  0.178 ( 0.186)\tLoss -9.1362e-01 (-9.0823e-01)\n",
            "Epoch: [514][40/97]\tTime  0.177 ( 0.184)\tLoss -8.9918e-01 (-9.0840e-01)\n",
            "Epoch: [514][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0422e-01 (-9.0894e-01)\n",
            "Epoch: [514][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0916e-01 (-9.0960e-01)\n",
            "Epoch: [514][70/97]\tTime  0.176 ( 0.181)\tLoss -9.2013e-01 (-9.0998e-01)\n",
            "Epoch: [514][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9783e-01 (-9.0967e-01)\n",
            "Epoch: [514][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1122e-01 (-9.0980e-01)\n",
            "Training...\n",
            "Epoch: [515][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.0374e-01 (-9.0374e-01)\n",
            "Epoch: [515][10/97]\tTime  0.176 ( 0.201)\tLoss -9.0672e-01 (-9.0971e-01)\n",
            "Epoch: [515][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9885e-01 (-9.0994e-01)\n",
            "Epoch: [515][30/97]\tTime  0.177 ( 0.185)\tLoss -9.0309e-01 (-9.1089e-01)\n",
            "Epoch: [515][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0619e-01 (-9.1066e-01)\n",
            "Epoch: [515][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0455e-01 (-9.0971e-01)\n",
            "Epoch: [515][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1674e-01 (-9.0958e-01)\n",
            "Epoch: [515][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0231e-01 (-9.0933e-01)\n",
            "Epoch: [515][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0848e-01 (-9.0932e-01)\n",
            "Epoch: [515][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0501e-01 (-9.0968e-01)\n",
            "Validating...\n",
            "Top1: 0.8563939144736842\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [516][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1431e-01 (-9.1431e-01)\n",
            "Epoch: [516][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1015e-01 (-9.1103e-01)\n",
            "Epoch: [516][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1565e-01 (-9.1154e-01)\n",
            "Epoch: [516][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0363e-01 (-9.1008e-01)\n",
            "Epoch: [516][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0104e-01 (-9.0977e-01)\n",
            "Epoch: [516][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0857e-01 (-9.0968e-01)\n",
            "Epoch: [516][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1165e-01 (-9.0953e-01)\n",
            "Epoch: [516][70/97]\tTime  0.176 ( 0.181)\tLoss -9.0847e-01 (-9.0964e-01)\n",
            "Epoch: [516][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0591e-01 (-9.0940e-01)\n",
            "Epoch: [516][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0964e-01 (-9.0913e-01)\n",
            "Training...\n",
            "Epoch: [517][ 0/97]\tTime  0.459 ( 0.459)\tLoss -9.0367e-01 (-9.0367e-01)\n",
            "Epoch: [517][10/97]\tTime  0.176 ( 0.203)\tLoss -9.1780e-01 (-9.1078e-01)\n",
            "Epoch: [517][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1863e-01 (-9.1070e-01)\n",
            "Epoch: [517][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2303e-01 (-9.1054e-01)\n",
            "Epoch: [517][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0761e-01 (-9.1026e-01)\n",
            "Epoch: [517][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1431e-01 (-9.1043e-01)\n",
            "Epoch: [517][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1889e-01 (-9.1030e-01)\n",
            "Epoch: [517][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0721e-01 (-9.1009e-01)\n",
            "Epoch: [517][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1669e-01 (-9.1005e-01)\n",
            "Epoch: [517][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0021e-01 (-9.1003e-01)\n",
            "Training...\n",
            "Epoch: [518][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.1409e-01 (-9.1409e-01)\n",
            "Epoch: [518][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1751e-01 (-9.1046e-01)\n",
            "Epoch: [518][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1284e-01 (-9.1083e-01)\n",
            "Epoch: [518][30/97]\tTime  0.178 ( 0.186)\tLoss -8.9613e-01 (-9.0974e-01)\n",
            "Epoch: [518][40/97]\tTime  0.177 ( 0.183)\tLoss -9.1114e-01 (-9.1050e-01)\n",
            "Epoch: [518][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2177e-01 (-9.1070e-01)\n",
            "Epoch: [518][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1408e-01 (-9.1063e-01)\n",
            "Epoch: [518][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2020e-01 (-9.1051e-01)\n",
            "Epoch: [518][80/97]\tTime  0.178 ( 0.180)\tLoss -9.1478e-01 (-9.1088e-01)\n",
            "Epoch: [518][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1608e-01 (-9.1103e-01)\n",
            "Training...\n",
            "Epoch: [519][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1880e-01 (-9.1880e-01)\n",
            "Epoch: [519][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1621e-01 (-9.1198e-01)\n",
            "Epoch: [519][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1460e-01 (-9.1214e-01)\n",
            "Epoch: [519][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0226e-01 (-9.1211e-01)\n",
            "Epoch: [519][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0750e-01 (-9.1093e-01)\n",
            "Epoch: [519][50/97]\tTime  0.178 ( 0.182)\tLoss -8.9988e-01 (-9.1013e-01)\n",
            "Epoch: [519][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1452e-01 (-9.1063e-01)\n",
            "Epoch: [519][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1850e-01 (-9.1074e-01)\n",
            "Epoch: [519][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0618e-01 (-9.1030e-01)\n",
            "Epoch: [519][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1288e-01 (-9.1014e-01)\n",
            "Training...\n",
            "Epoch: [520][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1132e-01 (-9.1132e-01)\n",
            "Epoch: [520][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2042e-01 (-9.1169e-01)\n",
            "Epoch: [520][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0714e-01 (-9.1145e-01)\n",
            "Epoch: [520][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1788e-01 (-9.1189e-01)\n",
            "Epoch: [520][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0837e-01 (-9.1093e-01)\n",
            "Epoch: [520][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0671e-01 (-9.1067e-01)\n",
            "Epoch: [520][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1222e-01 (-9.1032e-01)\n",
            "Epoch: [520][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1084e-01 (-9.1025e-01)\n",
            "Epoch: [520][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0779e-01 (-9.1019e-01)\n",
            "Epoch: [520][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2132e-01 (-9.1049e-01)\n",
            "Validating...\n",
            "Top1: 0.8601973684210527\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [521][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.0975e-01 (-9.0975e-01)\n",
            "Epoch: [521][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1442e-01 (-9.1624e-01)\n",
            "Epoch: [521][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0797e-01 (-9.1332e-01)\n",
            "Epoch: [521][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1562e-01 (-9.1269e-01)\n",
            "Epoch: [521][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0928e-01 (-9.1213e-01)\n",
            "Epoch: [521][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0282e-01 (-9.1157e-01)\n",
            "Epoch: [521][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0794e-01 (-9.1079e-01)\n",
            "Epoch: [521][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0298e-01 (-9.1057e-01)\n",
            "Epoch: [521][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0992e-01 (-9.1043e-01)\n",
            "Epoch: [521][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1945e-01 (-9.1066e-01)\n",
            "Training...\n",
            "Epoch: [522][ 0/97]\tTime  0.458 ( 0.458)\tLoss -9.0151e-01 (-9.0151e-01)\n",
            "Epoch: [522][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2233e-01 (-9.1365e-01)\n",
            "Epoch: [522][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2047e-01 (-9.1308e-01)\n",
            "Epoch: [522][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0336e-01 (-9.1279e-01)\n",
            "Epoch: [522][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1410e-01 (-9.1270e-01)\n",
            "Epoch: [522][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2259e-01 (-9.1231e-01)\n",
            "Epoch: [522][60/97]\tTime  0.176 ( 0.181)\tLoss -9.0116e-01 (-9.1139e-01)\n",
            "Epoch: [522][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1030e-01 (-9.1129e-01)\n",
            "Epoch: [522][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1439e-01 (-9.1110e-01)\n",
            "Epoch: [522][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1984e-01 (-9.1120e-01)\n",
            "Training...\n",
            "Epoch: [523][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.1212e-01 (-9.1212e-01)\n",
            "Epoch: [523][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1139e-01 (-9.1374e-01)\n",
            "Epoch: [523][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0651e-01 (-9.1228e-01)\n",
            "Epoch: [523][30/97]\tTime  0.177 ( 0.186)\tLoss -8.9931e-01 (-9.1312e-01)\n",
            "Epoch: [523][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1466e-01 (-9.1217e-01)\n",
            "Epoch: [523][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0572e-01 (-9.1169e-01)\n",
            "Epoch: [523][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1416e-01 (-9.1169e-01)\n",
            "Epoch: [523][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1577e-01 (-9.1191e-01)\n",
            "Epoch: [523][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1158e-01 (-9.1149e-01)\n",
            "Epoch: [523][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1377e-01 (-9.1096e-01)\n",
            "Training...\n",
            "Epoch: [524][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.1687e-01 (-9.1687e-01)\n",
            "Epoch: [524][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1131e-01 (-9.1190e-01)\n",
            "Epoch: [524][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1215e-01 (-9.1289e-01)\n",
            "Epoch: [524][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0090e-01 (-9.1286e-01)\n",
            "Epoch: [524][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0900e-01 (-9.1320e-01)\n",
            "Epoch: [524][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1451e-01 (-9.1336e-01)\n",
            "Epoch: [524][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0627e-01 (-9.1267e-01)\n",
            "Epoch: [524][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1347e-01 (-9.1196e-01)\n",
            "Epoch: [524][80/97]\tTime  0.177 ( 0.180)\tLoss -8.9512e-01 (-9.1092e-01)\n",
            "Epoch: [524][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1317e-01 (-9.1079e-01)\n",
            "Training...\n",
            "Epoch: [525][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.1233e-01 (-9.1233e-01)\n",
            "Epoch: [525][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0692e-01 (-9.0894e-01)\n",
            "Epoch: [525][20/97]\tTime  0.176 ( 0.190)\tLoss -9.1160e-01 (-9.1065e-01)\n",
            "Epoch: [525][30/97]\tTime  0.176 ( 0.186)\tLoss -9.1252e-01 (-9.1068e-01)\n",
            "Epoch: [525][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1929e-01 (-9.1065e-01)\n",
            "Epoch: [525][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0696e-01 (-9.1121e-01)\n",
            "Epoch: [525][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1451e-01 (-9.1105e-01)\n",
            "Epoch: [525][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1639e-01 (-9.1093e-01)\n",
            "Epoch: [525][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0906e-01 (-9.1065e-01)\n",
            "Epoch: [525][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1506e-01 (-9.1079e-01)\n",
            "Validating...\n",
            "Top1: 0.8539268092105263\n",
            "Training...\n",
            "Epoch: [526][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0648e-01 (-9.0648e-01)\n",
            "Epoch: [526][10/97]\tTime  0.177 ( 0.201)\tLoss -8.9973e-01 (-9.0998e-01)\n",
            "Epoch: [526][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0583e-01 (-9.0912e-01)\n",
            "Epoch: [526][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1009e-01 (-9.1024e-01)\n",
            "Epoch: [526][40/97]\tTime  0.177 ( 0.183)\tLoss -9.1654e-01 (-9.1026e-01)\n",
            "Epoch: [526][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1221e-01 (-9.1064e-01)\n",
            "Epoch: [526][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1882e-01 (-9.1084e-01)\n",
            "Epoch: [526][70/97]\tTime  0.176 ( 0.181)\tLoss -9.1756e-01 (-9.1067e-01)\n",
            "Epoch: [526][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1514e-01 (-9.1041e-01)\n",
            "Epoch: [526][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1276e-01 (-9.1012e-01)\n",
            "Training...\n",
            "Epoch: [527][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.1410e-01 (-9.1410e-01)\n",
            "Epoch: [527][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2510e-01 (-9.1581e-01)\n",
            "Epoch: [527][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0588e-01 (-9.1324e-01)\n",
            "Epoch: [527][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2270e-01 (-9.1312e-01)\n",
            "Epoch: [527][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1577e-01 (-9.1251e-01)\n",
            "Epoch: [527][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1091e-01 (-9.1267e-01)\n",
            "Epoch: [527][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2065e-01 (-9.1308e-01)\n",
            "Epoch: [527][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0665e-01 (-9.1267e-01)\n",
            "Epoch: [527][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0987e-01 (-9.1198e-01)\n",
            "Epoch: [527][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1327e-01 (-9.1158e-01)\n",
            "Training...\n",
            "Epoch: [528][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.2027e-01 (-9.2027e-01)\n",
            "Epoch: [528][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0494e-01 (-9.1243e-01)\n",
            "Epoch: [528][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0045e-01 (-9.1182e-01)\n",
            "Epoch: [528][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1851e-01 (-9.1218e-01)\n",
            "Epoch: [528][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0779e-01 (-9.1201e-01)\n",
            "Epoch: [528][50/97]\tTime  0.177 ( 0.183)\tLoss -8.9852e-01 (-9.1205e-01)\n",
            "Epoch: [528][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0292e-01 (-9.1166e-01)\n",
            "Epoch: [528][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0224e-01 (-9.1182e-01)\n",
            "Epoch: [528][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1024e-01 (-9.1185e-01)\n",
            "Epoch: [528][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1863e-01 (-9.1192e-01)\n",
            "Training...\n",
            "Epoch: [529][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.0804e-01 (-9.0804e-01)\n",
            "Epoch: [529][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2222e-01 (-9.1221e-01)\n",
            "Epoch: [529][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0836e-01 (-9.1259e-01)\n",
            "Epoch: [529][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0812e-01 (-9.1156e-01)\n",
            "Epoch: [529][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0122e-01 (-9.1190e-01)\n",
            "Epoch: [529][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0810e-01 (-9.1269e-01)\n",
            "Epoch: [529][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2531e-01 (-9.1335e-01)\n",
            "Epoch: [529][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0837e-01 (-9.1307e-01)\n",
            "Epoch: [529][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0432e-01 (-9.1232e-01)\n",
            "Epoch: [529][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1824e-01 (-9.1191e-01)\n",
            "Training...\n",
            "Epoch: [530][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.0245e-01 (-9.0245e-01)\n",
            "Epoch: [530][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1233e-01 (-9.1110e-01)\n",
            "Epoch: [530][20/97]\tTime  0.176 ( 0.190)\tLoss -9.0641e-01 (-9.1158e-01)\n",
            "Epoch: [530][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2164e-01 (-9.1168e-01)\n",
            "Epoch: [530][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0816e-01 (-9.1164e-01)\n",
            "Epoch: [530][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0356e-01 (-9.1237e-01)\n",
            "Epoch: [530][60/97]\tTime  0.176 ( 0.181)\tLoss -9.1573e-01 (-9.1220e-01)\n",
            "Epoch: [530][70/97]\tTime  0.176 ( 0.181)\tLoss -9.0581e-01 (-9.1202e-01)\n",
            "Epoch: [530][80/97]\tTime  0.176 ( 0.180)\tLoss -9.2032e-01 (-9.1233e-01)\n",
            "Epoch: [530][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0593e-01 (-9.1234e-01)\n",
            "Validating...\n",
            "Top1: 0.8572162828947368\n",
            "Training...\n",
            "Epoch: [531][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.1656e-01 (-9.1656e-01)\n",
            "Epoch: [531][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1741e-01 (-9.1294e-01)\n",
            "Epoch: [531][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1608e-01 (-9.1318e-01)\n",
            "Epoch: [531][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2345e-01 (-9.1432e-01)\n",
            "Epoch: [531][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2388e-01 (-9.1389e-01)\n",
            "Epoch: [531][50/97]\tTime  0.178 ( 0.182)\tLoss -9.1485e-01 (-9.1357e-01)\n",
            "Epoch: [531][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1350e-01 (-9.1359e-01)\n",
            "Epoch: [531][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2106e-01 (-9.1352e-01)\n",
            "Epoch: [531][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0142e-01 (-9.1318e-01)\n",
            "Epoch: [531][90/97]\tTime  0.177 ( 0.180)\tLoss -8.9775e-01 (-9.1251e-01)\n",
            "Training...\n",
            "Epoch: [532][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.1044e-01 (-9.1044e-01)\n",
            "Epoch: [532][10/97]\tTime  0.176 ( 0.201)\tLoss -9.0427e-01 (-9.1244e-01)\n",
            "Epoch: [532][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0640e-01 (-9.1078e-01)\n",
            "Epoch: [532][30/97]\tTime  0.177 ( 0.185)\tLoss -9.1452e-01 (-9.1141e-01)\n",
            "Epoch: [532][40/97]\tTime  0.177 ( 0.183)\tLoss -9.2329e-01 (-9.1169e-01)\n",
            "Epoch: [532][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0060e-01 (-9.1174e-01)\n",
            "Epoch: [532][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1288e-01 (-9.1157e-01)\n",
            "Epoch: [532][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1353e-01 (-9.1187e-01)\n",
            "Epoch: [532][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1209e-01 (-9.1230e-01)\n",
            "Epoch: [532][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1583e-01 (-9.1257e-01)\n",
            "Training...\n",
            "Epoch: [533][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1695e-01 (-9.1695e-01)\n",
            "Epoch: [533][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1855e-01 (-9.1489e-01)\n",
            "Epoch: [533][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0876e-01 (-9.1300e-01)\n",
            "Epoch: [533][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0977e-01 (-9.1262e-01)\n",
            "Epoch: [533][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0356e-01 (-9.1110e-01)\n",
            "Epoch: [533][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1499e-01 (-9.1135e-01)\n",
            "Epoch: [533][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0327e-01 (-9.1111e-01)\n",
            "Epoch: [533][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0998e-01 (-9.1103e-01)\n",
            "Epoch: [533][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2129e-01 (-9.1137e-01)\n",
            "Epoch: [533][90/97]\tTime  0.178 ( 0.180)\tLoss -9.3359e-01 (-9.1149e-01)\n",
            "Training...\n",
            "Epoch: [534][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.0506e-01 (-9.0506e-01)\n",
            "Epoch: [534][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1083e-01 (-9.1158e-01)\n",
            "Epoch: [534][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1283e-01 (-9.1207e-01)\n",
            "Epoch: [534][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0325e-01 (-9.1138e-01)\n",
            "Epoch: [534][40/97]\tTime  0.177 ( 0.183)\tLoss -9.1002e-01 (-9.1174e-01)\n",
            "Epoch: [534][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1687e-01 (-9.1175e-01)\n",
            "Epoch: [534][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0834e-01 (-9.1211e-01)\n",
            "Epoch: [534][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1117e-01 (-9.1220e-01)\n",
            "Epoch: [534][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1083e-01 (-9.1224e-01)\n",
            "Epoch: [534][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1292e-01 (-9.1227e-01)\n",
            "Training...\n",
            "Epoch: [535][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.1959e-01 (-9.1959e-01)\n",
            "Epoch: [535][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1161e-01 (-9.1404e-01)\n",
            "Epoch: [535][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1266e-01 (-9.1123e-01)\n",
            "Epoch: [535][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1517e-01 (-9.1106e-01)\n",
            "Epoch: [535][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1573e-01 (-9.1204e-01)\n",
            "Epoch: [535][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0362e-01 (-9.1164e-01)\n",
            "Epoch: [535][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0986e-01 (-9.1166e-01)\n",
            "Epoch: [535][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1027e-01 (-9.1179e-01)\n",
            "Epoch: [535][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0442e-01 (-9.1195e-01)\n",
            "Epoch: [535][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1202e-01 (-9.1209e-01)\n",
            "Validating...\n",
            "Top1: 0.8556743421052632\n",
            "Training...\n",
            "Epoch: [536][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.1430e-01 (-9.1430e-01)\n",
            "Epoch: [536][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1546e-01 (-9.1243e-01)\n",
            "Epoch: [536][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1331e-01 (-9.1228e-01)\n",
            "Epoch: [536][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1189e-01 (-9.1260e-01)\n",
            "Epoch: [536][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1331e-01 (-9.1218e-01)\n",
            "Epoch: [536][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1995e-01 (-9.1289e-01)\n",
            "Epoch: [536][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1578e-01 (-9.1296e-01)\n",
            "Epoch: [536][70/97]\tTime  0.177 ( 0.181)\tLoss -8.9749e-01 (-9.1275e-01)\n",
            "Epoch: [536][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1838e-01 (-9.1269e-01)\n",
            "Epoch: [536][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1678e-01 (-9.1253e-01)\n",
            "Training...\n",
            "Epoch: [537][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.1759e-01 (-9.1759e-01)\n",
            "Epoch: [537][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0512e-01 (-9.1251e-01)\n",
            "Epoch: [537][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0311e-01 (-9.1330e-01)\n",
            "Epoch: [537][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1959e-01 (-9.1213e-01)\n",
            "Epoch: [537][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1560e-01 (-9.1247e-01)\n",
            "Epoch: [537][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1552e-01 (-9.1293e-01)\n",
            "Epoch: [537][60/97]\tTime  0.178 ( 0.182)\tLoss -9.0718e-01 (-9.1273e-01)\n",
            "Epoch: [537][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1623e-01 (-9.1301e-01)\n",
            "Epoch: [537][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0405e-01 (-9.1278e-01)\n",
            "Epoch: [537][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0535e-01 (-9.1236e-01)\n",
            "Training...\n",
            "Epoch: [538][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.1264e-01 (-9.1264e-01)\n",
            "Epoch: [538][10/97]\tTime  0.176 ( 0.202)\tLoss -9.1284e-01 (-9.1229e-01)\n",
            "Epoch: [538][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2345e-01 (-9.1373e-01)\n",
            "Epoch: [538][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1734e-01 (-9.1390e-01)\n",
            "Epoch: [538][40/97]\tTime  0.178 ( 0.184)\tLoss -9.0789e-01 (-9.1382e-01)\n",
            "Epoch: [538][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2098e-01 (-9.1382e-01)\n",
            "Epoch: [538][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1230e-01 (-9.1379e-01)\n",
            "Epoch: [538][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0830e-01 (-9.1377e-01)\n",
            "Epoch: [538][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0894e-01 (-9.1321e-01)\n",
            "Epoch: [538][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1102e-01 (-9.1347e-01)\n",
            "Training...\n",
            "Epoch: [539][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.1594e-01 (-9.1594e-01)\n",
            "Epoch: [539][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1338e-01 (-9.1630e-01)\n",
            "Epoch: [539][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1646e-01 (-9.1534e-01)\n",
            "Epoch: [539][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0404e-01 (-9.1435e-01)\n",
            "Epoch: [539][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1519e-01 (-9.1409e-01)\n",
            "Epoch: [539][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2172e-01 (-9.1423e-01)\n",
            "Epoch: [539][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1065e-01 (-9.1393e-01)\n",
            "Epoch: [539][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0745e-01 (-9.1383e-01)\n",
            "Epoch: [539][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2056e-01 (-9.1329e-01)\n",
            "Epoch: [539][90/97]\tTime  0.176 ( 0.180)\tLoss -9.1266e-01 (-9.1327e-01)\n",
            "Training...\n",
            "Epoch: [540][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.0397e-01 (-9.0397e-01)\n",
            "Epoch: [540][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2420e-01 (-9.1331e-01)\n",
            "Epoch: [540][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1061e-01 (-9.1326e-01)\n",
            "Epoch: [540][30/97]\tTime  0.178 ( 0.186)\tLoss -9.1785e-01 (-9.1351e-01)\n",
            "Epoch: [540][40/97]\tTime  0.176 ( 0.184)\tLoss -9.2413e-01 (-9.1291e-01)\n",
            "Epoch: [540][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0223e-01 (-9.1230e-01)\n",
            "Epoch: [540][60/97]\tTime  0.177 ( 0.181)\tLoss -8.9926e-01 (-9.1206e-01)\n",
            "Epoch: [540][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1734e-01 (-9.1215e-01)\n",
            "Epoch: [540][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0741e-01 (-9.1259e-01)\n",
            "Epoch: [540][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1190e-01 (-9.1296e-01)\n",
            "Validating...\n",
            "Top1: 0.8526932565789473\n",
            "Training...\n",
            "Epoch: [541][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.0795e-01 (-9.0795e-01)\n",
            "Epoch: [541][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2156e-01 (-9.1518e-01)\n",
            "Epoch: [541][20/97]\tTime  0.176 ( 0.190)\tLoss -9.1020e-01 (-9.1497e-01)\n",
            "Epoch: [541][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2012e-01 (-9.1426e-01)\n",
            "Epoch: [541][40/97]\tTime  0.177 ( 0.183)\tLoss -9.2088e-01 (-9.1400e-01)\n",
            "Epoch: [541][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2154e-01 (-9.1394e-01)\n",
            "Epoch: [541][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0293e-01 (-9.1388e-01)\n",
            "Epoch: [541][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1447e-01 (-9.1396e-01)\n",
            "Epoch: [541][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1196e-01 (-9.1349e-01)\n",
            "Epoch: [541][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1508e-01 (-9.1361e-01)\n",
            "Training...\n",
            "Epoch: [542][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.1666e-01 (-9.1666e-01)\n",
            "Epoch: [542][10/97]\tTime  0.176 ( 0.201)\tLoss -9.1870e-01 (-9.1976e-01)\n",
            "Epoch: [542][20/97]\tTime  0.176 ( 0.190)\tLoss -9.0424e-01 (-9.1851e-01)\n",
            "Epoch: [542][30/97]\tTime  0.177 ( 0.185)\tLoss -9.1660e-01 (-9.1868e-01)\n",
            "Epoch: [542][40/97]\tTime  0.177 ( 0.183)\tLoss -9.0967e-01 (-9.1803e-01)\n",
            "Epoch: [542][50/97]\tTime  0.178 ( 0.182)\tLoss -9.1575e-01 (-9.1705e-01)\n",
            "Epoch: [542][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1827e-01 (-9.1632e-01)\n",
            "Epoch: [542][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2024e-01 (-9.1557e-01)\n",
            "Epoch: [542][80/97]\tTime  0.176 ( 0.180)\tLoss -9.1526e-01 (-9.1543e-01)\n",
            "Epoch: [542][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1968e-01 (-9.1518e-01)\n",
            "Training...\n",
            "Epoch: [543][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.0916e-01 (-9.0916e-01)\n",
            "Epoch: [543][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1400e-01 (-9.1934e-01)\n",
            "Epoch: [543][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0729e-01 (-9.1842e-01)\n",
            "Epoch: [543][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0823e-01 (-9.1584e-01)\n",
            "Epoch: [543][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0893e-01 (-9.1570e-01)\n",
            "Epoch: [543][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1166e-01 (-9.1487e-01)\n",
            "Epoch: [543][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0589e-01 (-9.1429e-01)\n",
            "Epoch: [543][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1726e-01 (-9.1392e-01)\n",
            "Epoch: [543][80/97]\tTime  0.176 ( 0.180)\tLoss -9.1642e-01 (-9.1370e-01)\n",
            "Epoch: [543][90/97]\tTime  0.176 ( 0.180)\tLoss -9.1314e-01 (-9.1343e-01)\n",
            "Training...\n",
            "Epoch: [544][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.1588e-01 (-9.1588e-01)\n",
            "Epoch: [544][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2196e-01 (-9.1390e-01)\n",
            "Epoch: [544][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1377e-01 (-9.1495e-01)\n",
            "Epoch: [544][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1211e-01 (-9.1477e-01)\n",
            "Epoch: [544][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1860e-01 (-9.1363e-01)\n",
            "Epoch: [544][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2178e-01 (-9.1357e-01)\n",
            "Epoch: [544][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1522e-01 (-9.1426e-01)\n",
            "Epoch: [544][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1970e-01 (-9.1418e-01)\n",
            "Epoch: [544][80/97]\tTime  0.176 ( 0.180)\tLoss -9.0543e-01 (-9.1406e-01)\n",
            "Epoch: [544][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1684e-01 (-9.1433e-01)\n",
            "Training...\n",
            "Epoch: [545][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.1652e-01 (-9.1652e-01)\n",
            "Epoch: [545][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1830e-01 (-9.1629e-01)\n",
            "Epoch: [545][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1898e-01 (-9.1510e-01)\n",
            "Epoch: [545][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2418e-01 (-9.1441e-01)\n",
            "Epoch: [545][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1661e-01 (-9.1427e-01)\n",
            "Epoch: [545][50/97]\tTime  0.176 ( 0.182)\tLoss -9.1416e-01 (-9.1430e-01)\n",
            "Epoch: [545][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1252e-01 (-9.1406e-01)\n",
            "Epoch: [545][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0419e-01 (-9.1405e-01)\n",
            "Epoch: [545][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0853e-01 (-9.1402e-01)\n",
            "Epoch: [545][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1580e-01 (-9.1377e-01)\n",
            "Validating...\n",
            "Top1: 0.8597861842105263\n",
            "Training...\n",
            "Epoch: [546][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.2124e-01 (-9.2124e-01)\n",
            "Epoch: [546][10/97]\tTime  0.176 ( 0.201)\tLoss -9.2096e-01 (-9.1373e-01)\n",
            "Epoch: [546][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0898e-01 (-9.1308e-01)\n",
            "Epoch: [546][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1303e-01 (-9.1447e-01)\n",
            "Epoch: [546][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1599e-01 (-9.1512e-01)\n",
            "Epoch: [546][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1181e-01 (-9.1476e-01)\n",
            "Epoch: [546][60/97]\tTime  0.178 ( 0.181)\tLoss -9.2174e-01 (-9.1523e-01)\n",
            "Epoch: [546][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1983e-01 (-9.1514e-01)\n",
            "Epoch: [546][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1017e-01 (-9.1475e-01)\n",
            "Epoch: [546][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1879e-01 (-9.1480e-01)\n",
            "Training...\n",
            "Epoch: [547][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1169e-01 (-9.1169e-01)\n",
            "Epoch: [547][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1194e-01 (-9.1334e-01)\n",
            "Epoch: [547][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1356e-01 (-9.1357e-01)\n",
            "Epoch: [547][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1255e-01 (-9.1398e-01)\n",
            "Epoch: [547][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1843e-01 (-9.1403e-01)\n",
            "Epoch: [547][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0818e-01 (-9.1361e-01)\n",
            "Epoch: [547][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0726e-01 (-9.1367e-01)\n",
            "Epoch: [547][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2130e-01 (-9.1416e-01)\n",
            "Epoch: [547][80/97]\tTime  0.178 ( 0.180)\tLoss -9.1821e-01 (-9.1393e-01)\n",
            "Epoch: [547][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1348e-01 (-9.1375e-01)\n",
            "Training...\n",
            "Epoch: [548][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.2134e-01 (-9.2134e-01)\n",
            "Epoch: [548][10/97]\tTime  0.178 ( 0.202)\tLoss -9.0774e-01 (-9.1505e-01)\n",
            "Epoch: [548][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0913e-01 (-9.1440e-01)\n",
            "Epoch: [548][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0937e-01 (-9.1364e-01)\n",
            "Epoch: [548][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0219e-01 (-9.1320e-01)\n",
            "Epoch: [548][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0100e-01 (-9.1296e-01)\n",
            "Epoch: [548][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2122e-01 (-9.1336e-01)\n",
            "Epoch: [548][70/97]\tTime  0.176 ( 0.181)\tLoss -9.1256e-01 (-9.1368e-01)\n",
            "Epoch: [548][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1525e-01 (-9.1374e-01)\n",
            "Epoch: [548][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2406e-01 (-9.1374e-01)\n",
            "Training...\n",
            "Epoch: [549][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.1212e-01 (-9.1212e-01)\n",
            "Epoch: [549][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0995e-01 (-9.1463e-01)\n",
            "Epoch: [549][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1515e-01 (-9.1528e-01)\n",
            "Epoch: [549][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1583e-01 (-9.1527e-01)\n",
            "Epoch: [549][40/97]\tTime  0.176 ( 0.183)\tLoss -9.1700e-01 (-9.1471e-01)\n",
            "Epoch: [549][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2156e-01 (-9.1446e-01)\n",
            "Epoch: [549][60/97]\tTime  0.177 ( 0.181)\tLoss -9.0104e-01 (-9.1443e-01)\n",
            "Epoch: [549][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1666e-01 (-9.1439e-01)\n",
            "Epoch: [549][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0221e-01 (-9.1462e-01)\n",
            "Epoch: [549][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1443e-01 (-9.1487e-01)\n",
            "Training...\n",
            "Epoch: [550][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.2404e-01 (-9.2404e-01)\n",
            "Epoch: [550][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1925e-01 (-9.1696e-01)\n",
            "Epoch: [550][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2280e-01 (-9.1466e-01)\n",
            "Epoch: [550][30/97]\tTime  0.177 ( 0.185)\tLoss -9.1331e-01 (-9.1479e-01)\n",
            "Epoch: [550][40/97]\tTime  0.176 ( 0.183)\tLoss -9.2268e-01 (-9.1517e-01)\n",
            "Epoch: [550][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1523e-01 (-9.1491e-01)\n",
            "Epoch: [550][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1879e-01 (-9.1421e-01)\n",
            "Epoch: [550][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0715e-01 (-9.1394e-01)\n",
            "Epoch: [550][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2323e-01 (-9.1432e-01)\n",
            "Epoch: [550][90/97]\tTime  0.178 ( 0.180)\tLoss -9.0833e-01 (-9.1384e-01)\n",
            "Validating...\n",
            "Top1: 0.8541324013157895\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [551][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.2750e-01 (-9.2750e-01)\n",
            "Epoch: [551][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1675e-01 (-9.1861e-01)\n",
            "Epoch: [551][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1701e-01 (-9.1572e-01)\n",
            "Epoch: [551][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2221e-01 (-9.1503e-01)\n",
            "Epoch: [551][40/97]\tTime  0.176 ( 0.184)\tLoss -9.1261e-01 (-9.1484e-01)\n",
            "Epoch: [551][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1787e-01 (-9.1429e-01)\n",
            "Epoch: [551][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1278e-01 (-9.1469e-01)\n",
            "Epoch: [551][70/97]\tTime  0.176 ( 0.181)\tLoss -9.1888e-01 (-9.1495e-01)\n",
            "Epoch: [551][80/97]\tTime  0.177 ( 0.180)\tLoss -9.0326e-01 (-9.1462e-01)\n",
            "Epoch: [551][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2319e-01 (-9.1525e-01)\n",
            "Training...\n",
            "Epoch: [552][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.1936e-01 (-9.1936e-01)\n",
            "Epoch: [552][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1328e-01 (-9.1608e-01)\n",
            "Epoch: [552][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1974e-01 (-9.1708e-01)\n",
            "Epoch: [552][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0404e-01 (-9.1695e-01)\n",
            "Epoch: [552][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1504e-01 (-9.1644e-01)\n",
            "Epoch: [552][50/97]\tTime  0.176 ( 0.182)\tLoss -9.0429e-01 (-9.1558e-01)\n",
            "Epoch: [552][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2080e-01 (-9.1556e-01)\n",
            "Epoch: [552][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1052e-01 (-9.1498e-01)\n",
            "Epoch: [552][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1438e-01 (-9.1494e-01)\n",
            "Epoch: [552][90/97]\tTime  0.176 ( 0.180)\tLoss -9.1275e-01 (-9.1510e-01)\n",
            "Training...\n",
            "Epoch: [553][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.2028e-01 (-9.2028e-01)\n",
            "Epoch: [553][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2862e-01 (-9.1732e-01)\n",
            "Epoch: [553][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0779e-01 (-9.1705e-01)\n",
            "Epoch: [553][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2352e-01 (-9.1679e-01)\n",
            "Epoch: [553][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1741e-01 (-9.1676e-01)\n",
            "Epoch: [553][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2076e-01 (-9.1667e-01)\n",
            "Epoch: [553][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1688e-01 (-9.1602e-01)\n",
            "Epoch: [553][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1850e-01 (-9.1614e-01)\n",
            "Epoch: [553][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2018e-01 (-9.1616e-01)\n",
            "Epoch: [553][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1084e-01 (-9.1603e-01)\n",
            "Training...\n",
            "Epoch: [554][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.0947e-01 (-9.0947e-01)\n",
            "Epoch: [554][10/97]\tTime  0.177 ( 0.201)\tLoss -9.0797e-01 (-9.1332e-01)\n",
            "Epoch: [554][20/97]\tTime  0.178 ( 0.190)\tLoss -8.9820e-01 (-9.1479e-01)\n",
            "Epoch: [554][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1101e-01 (-9.1481e-01)\n",
            "Epoch: [554][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1911e-01 (-9.1462e-01)\n",
            "Epoch: [554][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1078e-01 (-9.1416e-01)\n",
            "Epoch: [554][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2043e-01 (-9.1469e-01)\n",
            "Epoch: [554][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2023e-01 (-9.1512e-01)\n",
            "Epoch: [554][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2417e-01 (-9.1501e-01)\n",
            "Epoch: [554][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1670e-01 (-9.1477e-01)\n",
            "Training...\n",
            "Epoch: [555][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.2245e-01 (-9.2245e-01)\n",
            "Epoch: [555][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1406e-01 (-9.1761e-01)\n",
            "Epoch: [555][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1597e-01 (-9.1564e-01)\n",
            "Epoch: [555][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1835e-01 (-9.1506e-01)\n",
            "Epoch: [555][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1251e-01 (-9.1489e-01)\n",
            "Epoch: [555][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2166e-01 (-9.1498e-01)\n",
            "Epoch: [555][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0647e-01 (-9.1500e-01)\n",
            "Epoch: [555][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2021e-01 (-9.1543e-01)\n",
            "Epoch: [555][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1862e-01 (-9.1546e-01)\n",
            "Epoch: [555][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1276e-01 (-9.1553e-01)\n",
            "Validating...\n",
            "Top1: 0.8667763157894737\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [556][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.2024e-01 (-9.2024e-01)\n",
            "Epoch: [556][10/97]\tTime  0.177 ( 0.201)\tLoss -9.2203e-01 (-9.1454e-01)\n",
            "Epoch: [556][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1608e-01 (-9.1605e-01)\n",
            "Epoch: [556][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1782e-01 (-9.1625e-01)\n",
            "Epoch: [556][40/97]\tTime  0.177 ( 0.183)\tLoss -9.1963e-01 (-9.1619e-01)\n",
            "Epoch: [556][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1978e-01 (-9.1645e-01)\n",
            "Epoch: [556][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1703e-01 (-9.1629e-01)\n",
            "Epoch: [556][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1032e-01 (-9.1622e-01)\n",
            "Epoch: [556][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2088e-01 (-9.1602e-01)\n",
            "Epoch: [556][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1653e-01 (-9.1572e-01)\n",
            "Training...\n",
            "Epoch: [557][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.2328e-01 (-9.2328e-01)\n",
            "Epoch: [557][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1259e-01 (-9.1626e-01)\n",
            "Epoch: [557][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1411e-01 (-9.1576e-01)\n",
            "Epoch: [557][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1044e-01 (-9.1565e-01)\n",
            "Epoch: [557][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2025e-01 (-9.1574e-01)\n",
            "Epoch: [557][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2300e-01 (-9.1610e-01)\n",
            "Epoch: [557][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0886e-01 (-9.1596e-01)\n",
            "Epoch: [557][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1630e-01 (-9.1597e-01)\n",
            "Epoch: [557][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1892e-01 (-9.1591e-01)\n",
            "Epoch: [557][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0850e-01 (-9.1600e-01)\n",
            "Training...\n",
            "Epoch: [558][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.2676e-01 (-9.2676e-01)\n",
            "Epoch: [558][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1798e-01 (-9.1970e-01)\n",
            "Epoch: [558][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1652e-01 (-9.1653e-01)\n",
            "Epoch: [558][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2083e-01 (-9.1643e-01)\n",
            "Epoch: [558][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0727e-01 (-9.1599e-01)\n",
            "Epoch: [558][50/97]\tTime  0.176 ( 0.182)\tLoss -9.1063e-01 (-9.1530e-01)\n",
            "Epoch: [558][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2348e-01 (-9.1517e-01)\n",
            "Epoch: [558][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1162e-01 (-9.1509e-01)\n",
            "Epoch: [558][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2734e-01 (-9.1473e-01)\n",
            "Epoch: [558][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0683e-01 (-9.1477e-01)\n",
            "Training...\n",
            "Epoch: [559][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.1905e-01 (-9.1905e-01)\n",
            "Epoch: [559][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0858e-01 (-9.1573e-01)\n",
            "Epoch: [559][20/97]\tTime  0.178 ( 0.190)\tLoss -9.0791e-01 (-9.1513e-01)\n",
            "Epoch: [559][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1288e-01 (-9.1558e-01)\n",
            "Epoch: [559][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2111e-01 (-9.1531e-01)\n",
            "Epoch: [559][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0942e-01 (-9.1515e-01)\n",
            "Epoch: [559][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1253e-01 (-9.1567e-01)\n",
            "Epoch: [559][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0694e-01 (-9.1567e-01)\n",
            "Epoch: [559][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2115e-01 (-9.1566e-01)\n",
            "Epoch: [559][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2372e-01 (-9.1559e-01)\n",
            "Training...\n",
            "Epoch: [560][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1365e-01 (-9.1365e-01)\n",
            "Epoch: [560][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1567e-01 (-9.1394e-01)\n",
            "Epoch: [560][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1570e-01 (-9.1515e-01)\n",
            "Epoch: [560][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2179e-01 (-9.1728e-01)\n",
            "Epoch: [560][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2014e-01 (-9.1754e-01)\n",
            "Epoch: [560][50/97]\tTime  0.176 ( 0.182)\tLoss -9.1860e-01 (-9.1689e-01)\n",
            "Epoch: [560][60/97]\tTime  0.177 ( 0.182)\tLoss -9.0485e-01 (-9.1600e-01)\n",
            "Epoch: [560][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1039e-01 (-9.1588e-01)\n",
            "Epoch: [560][80/97]\tTime  0.178 ( 0.180)\tLoss -9.0380e-01 (-9.1610e-01)\n",
            "Epoch: [560][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1720e-01 (-9.1609e-01)\n",
            "Validating...\n",
            "Top1: 0.8601973684210527\n",
            "Training...\n",
            "Epoch: [561][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.0901e-01 (-9.0901e-01)\n",
            "Epoch: [561][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1906e-01 (-9.1876e-01)\n",
            "Epoch: [561][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1190e-01 (-9.1715e-01)\n",
            "Epoch: [561][30/97]\tTime  0.178 ( 0.186)\tLoss -9.1220e-01 (-9.1619e-01)\n",
            "Epoch: [561][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0877e-01 (-9.1639e-01)\n",
            "Epoch: [561][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2199e-01 (-9.1624e-01)\n",
            "Epoch: [561][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1972e-01 (-9.1611e-01)\n",
            "Epoch: [561][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1507e-01 (-9.1627e-01)\n",
            "Epoch: [561][80/97]\tTime  0.178 ( 0.180)\tLoss -9.2446e-01 (-9.1710e-01)\n",
            "Epoch: [561][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1419e-01 (-9.1651e-01)\n",
            "Training...\n",
            "Epoch: [562][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.2483e-01 (-9.2483e-01)\n",
            "Epoch: [562][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1524e-01 (-9.1588e-01)\n",
            "Epoch: [562][20/97]\tTime  0.177 ( 0.190)\tLoss -8.9783e-01 (-9.1544e-01)\n",
            "Epoch: [562][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1770e-01 (-9.1431e-01)\n",
            "Epoch: [562][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1838e-01 (-9.1435e-01)\n",
            "Epoch: [562][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1316e-01 (-9.1479e-01)\n",
            "Epoch: [562][60/97]\tTime  0.176 ( 0.181)\tLoss -9.1597e-01 (-9.1468e-01)\n",
            "Epoch: [562][70/97]\tTime  0.176 ( 0.181)\tLoss -9.1129e-01 (-9.1488e-01)\n",
            "Epoch: [562][80/97]\tTime  0.176 ( 0.180)\tLoss -9.1904e-01 (-9.1464e-01)\n",
            "Epoch: [562][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2690e-01 (-9.1478e-01)\n",
            "Training...\n",
            "Epoch: [563][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1891e-01 (-9.1891e-01)\n",
            "Epoch: [563][10/97]\tTime  0.176 ( 0.202)\tLoss -9.2397e-01 (-9.1766e-01)\n",
            "Epoch: [563][20/97]\tTime  0.176 ( 0.190)\tLoss -9.0989e-01 (-9.1776e-01)\n",
            "Epoch: [563][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2078e-01 (-9.1794e-01)\n",
            "Epoch: [563][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2131e-01 (-9.1741e-01)\n",
            "Epoch: [563][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2105e-01 (-9.1764e-01)\n",
            "Epoch: [563][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1395e-01 (-9.1752e-01)\n",
            "Epoch: [563][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1100e-01 (-9.1698e-01)\n",
            "Epoch: [563][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2212e-01 (-9.1670e-01)\n",
            "Epoch: [563][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1527e-01 (-9.1668e-01)\n",
            "Training...\n",
            "Epoch: [564][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.0606e-01 (-9.0606e-01)\n",
            "Epoch: [564][10/97]\tTime  0.176 ( 0.201)\tLoss -9.2154e-01 (-9.1968e-01)\n",
            "Epoch: [564][20/97]\tTime  0.177 ( 0.189)\tLoss -9.1675e-01 (-9.1830e-01)\n",
            "Epoch: [564][30/97]\tTime  0.177 ( 0.185)\tLoss -9.2072e-01 (-9.1824e-01)\n",
            "Epoch: [564][40/97]\tTime  0.177 ( 0.183)\tLoss -9.1925e-01 (-9.1864e-01)\n",
            "Epoch: [564][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2828e-01 (-9.1886e-01)\n",
            "Epoch: [564][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1846e-01 (-9.1839e-01)\n",
            "Epoch: [564][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1724e-01 (-9.1869e-01)\n",
            "Epoch: [564][80/97]\tTime  0.176 ( 0.180)\tLoss -9.2247e-01 (-9.1853e-01)\n",
            "Epoch: [564][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1150e-01 (-9.1813e-01)\n",
            "Training...\n",
            "Epoch: [565][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.2507e-01 (-9.2507e-01)\n",
            "Epoch: [565][10/97]\tTime  0.176 ( 0.201)\tLoss -9.2385e-01 (-9.2022e-01)\n",
            "Epoch: [565][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1944e-01 (-9.1914e-01)\n",
            "Epoch: [565][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1728e-01 (-9.1943e-01)\n",
            "Epoch: [565][40/97]\tTime  0.177 ( 0.183)\tLoss -9.2062e-01 (-9.1863e-01)\n",
            "Epoch: [565][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2323e-01 (-9.1865e-01)\n",
            "Epoch: [565][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1757e-01 (-9.1862e-01)\n",
            "Epoch: [565][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2545e-01 (-9.1806e-01)\n",
            "Epoch: [565][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2189e-01 (-9.1795e-01)\n",
            "Epoch: [565][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1249e-01 (-9.1803e-01)\n",
            "Validating...\n",
            "Top1: 0.8555715460526315\n",
            "Training...\n",
            "Epoch: [566][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.1708e-01 (-9.1708e-01)\n",
            "Epoch: [566][10/97]\tTime  0.177 ( 0.201)\tLoss -9.1977e-01 (-9.1852e-01)\n",
            "Epoch: [566][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3141e-01 (-9.1933e-01)\n",
            "Epoch: [566][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2074e-01 (-9.1853e-01)\n",
            "Epoch: [566][40/97]\tTime  0.177 ( 0.183)\tLoss -9.2003e-01 (-9.1851e-01)\n",
            "Epoch: [566][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1781e-01 (-9.1811e-01)\n",
            "Epoch: [566][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2406e-01 (-9.1839e-01)\n",
            "Epoch: [566][70/97]\tTime  0.176 ( 0.181)\tLoss -9.1382e-01 (-9.1844e-01)\n",
            "Epoch: [566][80/97]\tTime  0.176 ( 0.180)\tLoss -9.1201e-01 (-9.1822e-01)\n",
            "Epoch: [566][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1582e-01 (-9.1820e-01)\n",
            "Training...\n",
            "Epoch: [567][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.2121e-01 (-9.2121e-01)\n",
            "Epoch: [567][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2115e-01 (-9.1682e-01)\n",
            "Epoch: [567][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2212e-01 (-9.1817e-01)\n",
            "Epoch: [567][30/97]\tTime  0.176 ( 0.186)\tLoss -9.2238e-01 (-9.1853e-01)\n",
            "Epoch: [567][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1979e-01 (-9.1847e-01)\n",
            "Epoch: [567][50/97]\tTime  0.177 ( 0.182)\tLoss -9.0902e-01 (-9.1831e-01)\n",
            "Epoch: [567][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1858e-01 (-9.1818e-01)\n",
            "Epoch: [567][70/97]\tTime  0.178 ( 0.182)\tLoss -9.1861e-01 (-9.1797e-01)\n",
            "Epoch: [567][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1474e-01 (-9.1767e-01)\n",
            "Epoch: [567][90/97]\tTime  0.177 ( 0.181)\tLoss -9.1249e-01 (-9.1739e-01)\n",
            "Training...\n",
            "Epoch: [568][ 0/97]\tTime  0.474 ( 0.474)\tLoss -9.1141e-01 (-9.1141e-01)\n",
            "Epoch: [568][10/97]\tTime  0.190 ( 0.215)\tLoss -9.2581e-01 (-9.2018e-01)\n",
            "Epoch: [568][20/97]\tTime  0.193 ( 0.204)\tLoss -9.1635e-01 (-9.1955e-01)\n",
            "Epoch: [568][30/97]\tTime  0.192 ( 0.200)\tLoss -9.2346e-01 (-9.1985e-01)\n",
            "Epoch: [568][40/97]\tTime  0.192 ( 0.198)\tLoss -9.1550e-01 (-9.1979e-01)\n",
            "Epoch: [568][50/97]\tTime  0.191 ( 0.197)\tLoss -9.1736e-01 (-9.1984e-01)\n",
            "Epoch: [568][60/97]\tTime  0.193 ( 0.196)\tLoss -9.0543e-01 (-9.1895e-01)\n",
            "Epoch: [568][70/97]\tTime  0.178 ( 0.194)\tLoss -9.2064e-01 (-9.1892e-01)\n",
            "Epoch: [568][80/97]\tTime  0.177 ( 0.192)\tLoss -9.0406e-01 (-9.1876e-01)\n",
            "Epoch: [568][90/97]\tTime  0.178 ( 0.190)\tLoss -9.1521e-01 (-9.1840e-01)\n",
            "Training...\n",
            "Epoch: [569][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.1091e-01 (-9.1091e-01)\n",
            "Epoch: [569][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2865e-01 (-9.1624e-01)\n",
            "Epoch: [569][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1385e-01 (-9.1644e-01)\n",
            "Epoch: [569][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2235e-01 (-9.1726e-01)\n",
            "Epoch: [569][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1998e-01 (-9.1737e-01)\n",
            "Epoch: [569][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1515e-01 (-9.1703e-01)\n",
            "Epoch: [569][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2712e-01 (-9.1749e-01)\n",
            "Epoch: [569][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0694e-01 (-9.1729e-01)\n",
            "Epoch: [569][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1864e-01 (-9.1750e-01)\n",
            "Epoch: [569][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2205e-01 (-9.1764e-01)\n",
            "Training...\n",
            "Epoch: [570][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.1232e-01 (-9.1232e-01)\n",
            "Epoch: [570][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1606e-01 (-9.1809e-01)\n",
            "Epoch: [570][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1599e-01 (-9.1699e-01)\n",
            "Epoch: [570][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1324e-01 (-9.1835e-01)\n",
            "Epoch: [570][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1685e-01 (-9.1896e-01)\n",
            "Epoch: [570][50/97]\tTime  0.178 ( 0.183)\tLoss -9.1713e-01 (-9.1901e-01)\n",
            "Epoch: [570][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2382e-01 (-9.1879e-01)\n",
            "Epoch: [570][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1781e-01 (-9.1857e-01)\n",
            "Epoch: [570][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1278e-01 (-9.1812e-01)\n",
            "Epoch: [570][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1384e-01 (-9.1790e-01)\n",
            "Validating...\n",
            "Top1: 0.8530016447368421\n",
            "Training...\n",
            "Epoch: [571][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.1185e-01 (-9.1185e-01)\n",
            "Epoch: [571][10/97]\tTime  0.177 ( 0.201)\tLoss -9.2621e-01 (-9.2169e-01)\n",
            "Epoch: [571][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2095e-01 (-9.2076e-01)\n",
            "Epoch: [571][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2662e-01 (-9.2177e-01)\n",
            "Epoch: [571][40/97]\tTime  0.177 ( 0.184)\tLoss -9.0944e-01 (-9.2115e-01)\n",
            "Epoch: [571][50/97]\tTime  0.178 ( 0.182)\tLoss -9.2359e-01 (-9.2079e-01)\n",
            "Epoch: [571][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1512e-01 (-9.2037e-01)\n",
            "Epoch: [571][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1799e-01 (-9.1995e-01)\n",
            "Epoch: [571][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2414e-01 (-9.1976e-01)\n",
            "Epoch: [571][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1942e-01 (-9.1942e-01)\n",
            "Training...\n",
            "Epoch: [572][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1635e-01 (-9.1635e-01)\n",
            "Epoch: [572][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1366e-01 (-9.1953e-01)\n",
            "Epoch: [572][20/97]\tTime  0.178 ( 0.190)\tLoss -9.2262e-01 (-9.1917e-01)\n",
            "Epoch: [572][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2901e-01 (-9.1934e-01)\n",
            "Epoch: [572][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2151e-01 (-9.1902e-01)\n",
            "Epoch: [572][50/97]\tTime  0.177 ( 0.183)\tLoss -9.0769e-01 (-9.1832e-01)\n",
            "Epoch: [572][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2626e-01 (-9.1839e-01)\n",
            "Epoch: [572][70/97]\tTime  0.178 ( 0.181)\tLoss -9.0989e-01 (-9.1769e-01)\n",
            "Epoch: [572][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2358e-01 (-9.1737e-01)\n",
            "Epoch: [572][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1888e-01 (-9.1754e-01)\n",
            "Training...\n",
            "Epoch: [573][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.0894e-01 (-9.0894e-01)\n",
            "Epoch: [573][10/97]\tTime  0.177 ( 0.203)\tLoss -9.2197e-01 (-9.2039e-01)\n",
            "Epoch: [573][20/97]\tTime  0.178 ( 0.191)\tLoss -9.1674e-01 (-9.2013e-01)\n",
            "Epoch: [573][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2036e-01 (-9.1925e-01)\n",
            "Epoch: [573][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1021e-01 (-9.1840e-01)\n",
            "Epoch: [573][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1885e-01 (-9.1783e-01)\n",
            "Epoch: [573][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1237e-01 (-9.1767e-01)\n",
            "Epoch: [573][70/97]\tTime  0.179 ( 0.181)\tLoss -9.1948e-01 (-9.1781e-01)\n",
            "Epoch: [573][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2952e-01 (-9.1809e-01)\n",
            "Epoch: [573][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1237e-01 (-9.1831e-01)\n",
            "Training...\n",
            "Epoch: [574][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.1034e-01 (-9.1034e-01)\n",
            "Epoch: [574][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2485e-01 (-9.1973e-01)\n",
            "Epoch: [574][20/97]\tTime  0.180 ( 0.191)\tLoss -9.2048e-01 (-9.1951e-01)\n",
            "Epoch: [574][30/97]\tTime  0.179 ( 0.187)\tLoss -9.1699e-01 (-9.1871e-01)\n",
            "Epoch: [574][40/97]\tTime  0.178 ( 0.186)\tLoss -9.2269e-01 (-9.1889e-01)\n",
            "Epoch: [574][50/97]\tTime  0.179 ( 0.184)\tLoss -9.1771e-01 (-9.1876e-01)\n",
            "Epoch: [574][60/97]\tTime  0.177 ( 0.183)\tLoss -9.1801e-01 (-9.1837e-01)\n",
            "Epoch: [574][70/97]\tTime  0.178 ( 0.183)\tLoss -9.2145e-01 (-9.1875e-01)\n",
            "Epoch: [574][80/97]\tTime  0.177 ( 0.182)\tLoss -9.0921e-01 (-9.1871e-01)\n",
            "Epoch: [574][90/97]\tTime  0.178 ( 0.182)\tLoss -9.1448e-01 (-9.1860e-01)\n",
            "Training...\n",
            "Epoch: [575][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.2760e-01 (-9.2760e-01)\n",
            "Epoch: [575][10/97]\tTime  0.180 ( 0.203)\tLoss -9.1119e-01 (-9.1867e-01)\n",
            "Epoch: [575][20/97]\tTime  0.177 ( 0.191)\tLoss -9.1763e-01 (-9.1818e-01)\n",
            "Epoch: [575][30/97]\tTime  0.178 ( 0.188)\tLoss -9.2038e-01 (-9.1925e-01)\n",
            "Epoch: [575][40/97]\tTime  0.177 ( 0.185)\tLoss -9.1811e-01 (-9.1934e-01)\n",
            "Epoch: [575][50/97]\tTime  0.178 ( 0.184)\tLoss -9.1129e-01 (-9.1908e-01)\n",
            "Epoch: [575][60/97]\tTime  0.177 ( 0.183)\tLoss -9.2517e-01 (-9.1948e-01)\n",
            "Epoch: [575][70/97]\tTime  0.178 ( 0.182)\tLoss -9.1982e-01 (-9.1958e-01)\n",
            "Epoch: [575][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1708e-01 (-9.1921e-01)\n",
            "Epoch: [575][90/97]\tTime  0.177 ( 0.181)\tLoss -9.2874e-01 (-9.1929e-01)\n",
            "Validating...\n",
            "Top1: 0.8604029605263158\n",
            "Training...\n",
            "Epoch: [576][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.2504e-01 (-9.2504e-01)\n",
            "Epoch: [576][10/97]\tTime  0.179 ( 0.203)\tLoss -9.1978e-01 (-9.2110e-01)\n",
            "Epoch: [576][20/97]\tTime  0.177 ( 0.191)\tLoss -9.2518e-01 (-9.1965e-01)\n",
            "Epoch: [576][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2590e-01 (-9.2049e-01)\n",
            "Epoch: [576][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1426e-01 (-9.2065e-01)\n",
            "Epoch: [576][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2085e-01 (-9.2054e-01)\n",
            "Epoch: [576][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2120e-01 (-9.2002e-01)\n",
            "Epoch: [576][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1496e-01 (-9.1954e-01)\n",
            "Epoch: [576][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2242e-01 (-9.1919e-01)\n",
            "Epoch: [576][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2262e-01 (-9.1929e-01)\n",
            "Training...\n",
            "Epoch: [577][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.2184e-01 (-9.2184e-01)\n",
            "Epoch: [577][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1514e-01 (-9.2328e-01)\n",
            "Epoch: [577][20/97]\tTime  0.177 ( 0.190)\tLoss -9.0786e-01 (-9.2155e-01)\n",
            "Epoch: [577][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1905e-01 (-9.2062e-01)\n",
            "Epoch: [577][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1843e-01 (-9.2057e-01)\n",
            "Epoch: [577][50/97]\tTime  0.178 ( 0.183)\tLoss -9.1843e-01 (-9.2017e-01)\n",
            "Epoch: [577][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1173e-01 (-9.1914e-01)\n",
            "Epoch: [577][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1071e-01 (-9.1869e-01)\n",
            "Epoch: [577][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2834e-01 (-9.1908e-01)\n",
            "Epoch: [577][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2351e-01 (-9.1919e-01)\n",
            "Training...\n",
            "Epoch: [578][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.1020e-01 (-9.1020e-01)\n",
            "Epoch: [578][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2452e-01 (-9.1960e-01)\n",
            "Epoch: [578][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2666e-01 (-9.2043e-01)\n",
            "Epoch: [578][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2188e-01 (-9.2121e-01)\n",
            "Epoch: [578][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2103e-01 (-9.2067e-01)\n",
            "Epoch: [578][50/97]\tTime  0.178 ( 0.182)\tLoss -9.1981e-01 (-9.1979e-01)\n",
            "Epoch: [578][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1989e-01 (-9.1959e-01)\n",
            "Epoch: [578][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1342e-01 (-9.1939e-01)\n",
            "Epoch: [578][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2142e-01 (-9.1920e-01)\n",
            "Epoch: [578][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1649e-01 (-9.1915e-01)\n",
            "Training...\n",
            "Epoch: [579][ 0/97]\tTime  0.458 ( 0.458)\tLoss -9.2255e-01 (-9.2255e-01)\n",
            "Epoch: [579][10/97]\tTime  0.177 ( 0.203)\tLoss -9.2161e-01 (-9.2112e-01)\n",
            "Epoch: [579][20/97]\tTime  0.177 ( 0.191)\tLoss -9.1893e-01 (-9.2086e-01)\n",
            "Epoch: [579][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2467e-01 (-9.2059e-01)\n",
            "Epoch: [579][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1625e-01 (-9.1981e-01)\n",
            "Epoch: [579][50/97]\tTime  0.178 ( 0.183)\tLoss -9.1911e-01 (-9.1978e-01)\n",
            "Epoch: [579][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1867e-01 (-9.1992e-01)\n",
            "Epoch: [579][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1964e-01 (-9.1966e-01)\n",
            "Epoch: [579][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1953e-01 (-9.1981e-01)\n",
            "Epoch: [579][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2140e-01 (-9.1953e-01)\n",
            "Training...\n",
            "Epoch: [580][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1110e-01 (-9.1110e-01)\n",
            "Epoch: [580][10/97]\tTime  0.176 ( 0.202)\tLoss -9.2316e-01 (-9.1767e-01)\n",
            "Epoch: [580][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1559e-01 (-9.1805e-01)\n",
            "Epoch: [580][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2467e-01 (-9.1880e-01)\n",
            "Epoch: [580][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2737e-01 (-9.1945e-01)\n",
            "Epoch: [580][50/97]\tTime  0.178 ( 0.183)\tLoss -9.1564e-01 (-9.1879e-01)\n",
            "Epoch: [580][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1170e-01 (-9.1919e-01)\n",
            "Epoch: [580][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2653e-01 (-9.1958e-01)\n",
            "Epoch: [580][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2828e-01 (-9.2002e-01)\n",
            "Epoch: [580][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2372e-01 (-9.1989e-01)\n",
            "Validating...\n",
            "Top1: 0.8607113486842105\n",
            "Training...\n",
            "Epoch: [581][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.3226e-01 (-9.3226e-01)\n",
            "Epoch: [581][10/97]\tTime  0.177 ( 0.202)\tLoss -9.0945e-01 (-9.2090e-01)\n",
            "Epoch: [581][20/97]\tTime  0.177 ( 0.191)\tLoss -9.2191e-01 (-9.2211e-01)\n",
            "Epoch: [581][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2271e-01 (-9.2160e-01)\n",
            "Epoch: [581][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2135e-01 (-9.2154e-01)\n",
            "Epoch: [581][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2162e-01 (-9.2199e-01)\n",
            "Epoch: [581][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1738e-01 (-9.2168e-01)\n",
            "Epoch: [581][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2668e-01 (-9.2202e-01)\n",
            "Epoch: [581][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1466e-01 (-9.2186e-01)\n",
            "Epoch: [581][90/97]\tTime  0.178 ( 0.180)\tLoss -9.3166e-01 (-9.2182e-01)\n",
            "Training...\n",
            "Epoch: [582][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.1870e-01 (-9.1870e-01)\n",
            "Epoch: [582][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1736e-01 (-9.1916e-01)\n",
            "Epoch: [582][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1344e-01 (-9.1977e-01)\n",
            "Epoch: [582][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2460e-01 (-9.1984e-01)\n",
            "Epoch: [582][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1276e-01 (-9.1970e-01)\n",
            "Epoch: [582][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2619e-01 (-9.1999e-01)\n",
            "Epoch: [582][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1462e-01 (-9.1997e-01)\n",
            "Epoch: [582][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2318e-01 (-9.2011e-01)\n",
            "Epoch: [582][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2067e-01 (-9.2035e-01)\n",
            "Epoch: [582][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1662e-01 (-9.2054e-01)\n",
            "Training...\n",
            "Epoch: [583][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.1812e-01 (-9.1812e-01)\n",
            "Epoch: [583][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1526e-01 (-9.2457e-01)\n",
            "Epoch: [583][20/97]\tTime  0.178 ( 0.190)\tLoss -9.2654e-01 (-9.2499e-01)\n",
            "Epoch: [583][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3695e-01 (-9.2344e-01)\n",
            "Epoch: [583][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2149e-01 (-9.2264e-01)\n",
            "Epoch: [583][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1943e-01 (-9.2267e-01)\n",
            "Epoch: [583][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1441e-01 (-9.2202e-01)\n",
            "Epoch: [583][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1897e-01 (-9.2159e-01)\n",
            "Epoch: [583][80/97]\tTime  0.177 ( 0.181)\tLoss -9.0977e-01 (-9.2174e-01)\n",
            "Epoch: [583][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1363e-01 (-9.2128e-01)\n",
            "Training...\n",
            "Epoch: [584][ 0/97]\tTime  0.463 ( 0.463)\tLoss -9.2409e-01 (-9.2409e-01)\n",
            "Epoch: [584][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1593e-01 (-9.2297e-01)\n",
            "Epoch: [584][20/97]\tTime  0.178 ( 0.191)\tLoss -9.2036e-01 (-9.2025e-01)\n",
            "Epoch: [584][30/97]\tTime  0.177 ( 0.187)\tLoss -9.1013e-01 (-9.2051e-01)\n",
            "Epoch: [584][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1617e-01 (-9.2027e-01)\n",
            "Epoch: [584][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1551e-01 (-9.1963e-01)\n",
            "Epoch: [584][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1484e-01 (-9.1977e-01)\n",
            "Epoch: [584][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2014e-01 (-9.1992e-01)\n",
            "Epoch: [584][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1573e-01 (-9.2008e-01)\n",
            "Epoch: [584][90/97]\tTime  0.178 ( 0.180)\tLoss -9.1942e-01 (-9.2052e-01)\n",
            "Training...\n",
            "Epoch: [585][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1810e-01 (-9.1810e-01)\n",
            "Epoch: [585][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1192e-01 (-9.2346e-01)\n",
            "Epoch: [585][20/97]\tTime  0.178 ( 0.190)\tLoss -9.2158e-01 (-9.2343e-01)\n",
            "Epoch: [585][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1796e-01 (-9.2197e-01)\n",
            "Epoch: [585][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1705e-01 (-9.2159e-01)\n",
            "Epoch: [585][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1861e-01 (-9.2159e-01)\n",
            "Epoch: [585][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1509e-01 (-9.2089e-01)\n",
            "Epoch: [585][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1639e-01 (-9.2088e-01)\n",
            "Epoch: [585][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2666e-01 (-9.2118e-01)\n",
            "Epoch: [585][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2044e-01 (-9.2144e-01)\n",
            "Validating...\n",
            "Top1: 0.8587582236842105\n",
            "Training...\n",
            "Epoch: [586][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.2130e-01 (-9.2130e-01)\n",
            "Epoch: [586][10/97]\tTime  0.177 ( 0.203)\tLoss -9.1704e-01 (-9.1942e-01)\n",
            "Epoch: [586][20/97]\tTime  0.178 ( 0.191)\tLoss -9.2134e-01 (-9.1808e-01)\n",
            "Epoch: [586][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1766e-01 (-9.1909e-01)\n",
            "Epoch: [586][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2536e-01 (-9.2009e-01)\n",
            "Epoch: [586][50/97]\tTime  0.176 ( 0.183)\tLoss -9.2052e-01 (-9.2063e-01)\n",
            "Epoch: [586][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2492e-01 (-9.2105e-01)\n",
            "Epoch: [586][70/97]\tTime  0.178 ( 0.181)\tLoss -9.1341e-01 (-9.2055e-01)\n",
            "Epoch: [586][80/97]\tTime  0.176 ( 0.181)\tLoss -9.1484e-01 (-9.2030e-01)\n",
            "Epoch: [586][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2600e-01 (-9.2052e-01)\n",
            "Training...\n",
            "Epoch: [587][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1894e-01 (-9.1894e-01)\n",
            "Epoch: [587][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3132e-01 (-9.2217e-01)\n",
            "Epoch: [587][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1768e-01 (-9.2264e-01)\n",
            "Epoch: [587][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2598e-01 (-9.2259e-01)\n",
            "Epoch: [587][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2623e-01 (-9.2158e-01)\n",
            "Epoch: [587][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1719e-01 (-9.2117e-01)\n",
            "Epoch: [587][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2014e-01 (-9.2087e-01)\n",
            "Epoch: [587][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2500e-01 (-9.2098e-01)\n",
            "Epoch: [587][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2552e-01 (-9.2129e-01)\n",
            "Epoch: [587][90/97]\tTime  0.178 ( 0.180)\tLoss -9.3016e-01 (-9.2140e-01)\n",
            "Training...\n",
            "Epoch: [588][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.1392e-01 (-9.1392e-01)\n",
            "Epoch: [588][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2482e-01 (-9.2021e-01)\n",
            "Epoch: [588][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1693e-01 (-9.2071e-01)\n",
            "Epoch: [588][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1930e-01 (-9.2117e-01)\n",
            "Epoch: [588][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2611e-01 (-9.2165e-01)\n",
            "Epoch: [588][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2811e-01 (-9.2154e-01)\n",
            "Epoch: [588][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1209e-01 (-9.2068e-01)\n",
            "Epoch: [588][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1534e-01 (-9.2066e-01)\n",
            "Epoch: [588][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2002e-01 (-9.2062e-01)\n",
            "Epoch: [588][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3169e-01 (-9.2117e-01)\n",
            "Training...\n",
            "Epoch: [589][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.2490e-01 (-9.2490e-01)\n",
            "Epoch: [589][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1747e-01 (-9.2201e-01)\n",
            "Epoch: [589][20/97]\tTime  0.178 ( 0.190)\tLoss -9.2122e-01 (-9.2174e-01)\n",
            "Epoch: [589][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1641e-01 (-9.2177e-01)\n",
            "Epoch: [589][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1709e-01 (-9.2162e-01)\n",
            "Epoch: [589][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1900e-01 (-9.2138e-01)\n",
            "Epoch: [589][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1925e-01 (-9.2131e-01)\n",
            "Epoch: [589][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2624e-01 (-9.2136e-01)\n",
            "Epoch: [589][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2332e-01 (-9.2140e-01)\n",
            "Epoch: [589][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2067e-01 (-9.2140e-01)\n",
            "Training...\n",
            "Epoch: [590][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.2327e-01 (-9.2327e-01)\n",
            "Epoch: [590][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1998e-01 (-9.2476e-01)\n",
            "Epoch: [590][20/97]\tTime  0.178 ( 0.190)\tLoss -9.2348e-01 (-9.2369e-01)\n",
            "Epoch: [590][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2900e-01 (-9.2381e-01)\n",
            "Epoch: [590][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2260e-01 (-9.2283e-01)\n",
            "Epoch: [590][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2749e-01 (-9.2264e-01)\n",
            "Epoch: [590][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1705e-01 (-9.2185e-01)\n",
            "Epoch: [590][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2096e-01 (-9.2210e-01)\n",
            "Epoch: [590][80/97]\tTime  0.177 ( 0.181)\tLoss -9.3047e-01 (-9.2194e-01)\n",
            "Epoch: [590][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2544e-01 (-9.2176e-01)\n",
            "Validating...\n",
            "Top1: 0.8649259868421053\n",
            "Training...\n",
            "Epoch: [591][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.2277e-01 (-9.2277e-01)\n",
            "Epoch: [591][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2222e-01 (-9.2224e-01)\n",
            "Epoch: [591][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1274e-01 (-9.2135e-01)\n",
            "Epoch: [591][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2556e-01 (-9.2142e-01)\n",
            "Epoch: [591][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2688e-01 (-9.2177e-01)\n",
            "Epoch: [591][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2137e-01 (-9.2148e-01)\n",
            "Epoch: [591][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2174e-01 (-9.2149e-01)\n",
            "Epoch: [591][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3205e-01 (-9.2170e-01)\n",
            "Epoch: [591][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2547e-01 (-9.2197e-01)\n",
            "Epoch: [591][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2736e-01 (-9.2239e-01)\n",
            "Training...\n",
            "Epoch: [592][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.2056e-01 (-9.2056e-01)\n",
            "Epoch: [592][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3150e-01 (-9.2250e-01)\n",
            "Epoch: [592][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3361e-01 (-9.2373e-01)\n",
            "Epoch: [592][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0632e-01 (-9.2257e-01)\n",
            "Epoch: [592][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1166e-01 (-9.2189e-01)\n",
            "Epoch: [592][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2714e-01 (-9.2179e-01)\n",
            "Epoch: [592][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1813e-01 (-9.2178e-01)\n",
            "Epoch: [592][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2476e-01 (-9.2185e-01)\n",
            "Epoch: [592][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1906e-01 (-9.2165e-01)\n",
            "Epoch: [592][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2233e-01 (-9.2176e-01)\n",
            "Training...\n",
            "Epoch: [593][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.2264e-01 (-9.2264e-01)\n",
            "Epoch: [593][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2755e-01 (-9.2159e-01)\n",
            "Epoch: [593][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1543e-01 (-9.1961e-01)\n",
            "Epoch: [593][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2481e-01 (-9.2068e-01)\n",
            "Epoch: [593][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2698e-01 (-9.2123e-01)\n",
            "Epoch: [593][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1673e-01 (-9.2116e-01)\n",
            "Epoch: [593][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1904e-01 (-9.2083e-01)\n",
            "Epoch: [593][70/97]\tTime  0.177 ( 0.181)\tLoss -9.0605e-01 (-9.2093e-01)\n",
            "Epoch: [593][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1739e-01 (-9.2117e-01)\n",
            "Epoch: [593][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1747e-01 (-9.2128e-01)\n",
            "Training...\n",
            "Epoch: [594][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.2923e-01 (-9.2923e-01)\n",
            "Epoch: [594][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1614e-01 (-9.2343e-01)\n",
            "Epoch: [594][20/97]\tTime  0.178 ( 0.190)\tLoss -9.1228e-01 (-9.2161e-01)\n",
            "Epoch: [594][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2531e-01 (-9.2124e-01)\n",
            "Epoch: [594][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2008e-01 (-9.2184e-01)\n",
            "Epoch: [594][50/97]\tTime  0.177 ( 0.183)\tLoss -9.3118e-01 (-9.2249e-01)\n",
            "Epoch: [594][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2354e-01 (-9.2241e-01)\n",
            "Epoch: [594][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2044e-01 (-9.2261e-01)\n",
            "Epoch: [594][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2318e-01 (-9.2270e-01)\n",
            "Epoch: [594][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3611e-01 (-9.2290e-01)\n",
            "Training...\n",
            "Epoch: [595][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.3173e-01 (-9.3173e-01)\n",
            "Epoch: [595][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2456e-01 (-9.2479e-01)\n",
            "Epoch: [595][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2477e-01 (-9.2375e-01)\n",
            "Epoch: [595][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3163e-01 (-9.2274e-01)\n",
            "Epoch: [595][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1852e-01 (-9.2270e-01)\n",
            "Epoch: [595][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2167e-01 (-9.2227e-01)\n",
            "Epoch: [595][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2013e-01 (-9.2245e-01)\n",
            "Epoch: [595][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3369e-01 (-9.2236e-01)\n",
            "Epoch: [595][80/97]\tTime  0.178 ( 0.181)\tLoss -9.3068e-01 (-9.2290e-01)\n",
            "Epoch: [595][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2075e-01 (-9.2288e-01)\n",
            "Validating...\n",
            "Top1: 0.8657483552631579\n",
            "Training...\n",
            "Epoch: [596][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.2402e-01 (-9.2402e-01)\n",
            "Epoch: [596][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3107e-01 (-9.2649e-01)\n",
            "Epoch: [596][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2953e-01 (-9.2539e-01)\n",
            "Epoch: [596][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3547e-01 (-9.2461e-01)\n",
            "Epoch: [596][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2140e-01 (-9.2401e-01)\n",
            "Epoch: [596][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2370e-01 (-9.2332e-01)\n",
            "Epoch: [596][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2386e-01 (-9.2308e-01)\n",
            "Epoch: [596][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1588e-01 (-9.2321e-01)\n",
            "Epoch: [596][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2523e-01 (-9.2305e-01)\n",
            "Epoch: [596][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2465e-01 (-9.2311e-01)\n",
            "Training...\n",
            "Epoch: [597][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.1185e-01 (-9.1185e-01)\n",
            "Epoch: [597][10/97]\tTime  0.178 ( 0.203)\tLoss -9.2110e-01 (-9.2183e-01)\n",
            "Epoch: [597][20/97]\tTime  0.177 ( 0.191)\tLoss -9.2014e-01 (-9.2214e-01)\n",
            "Epoch: [597][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2699e-01 (-9.2280e-01)\n",
            "Epoch: [597][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3292e-01 (-9.2219e-01)\n",
            "Epoch: [597][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1173e-01 (-9.2199e-01)\n",
            "Epoch: [597][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2052e-01 (-9.2185e-01)\n",
            "Epoch: [597][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1951e-01 (-9.2176e-01)\n",
            "Epoch: [597][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2316e-01 (-9.2150e-01)\n",
            "Epoch: [597][90/97]\tTime  0.177 ( 0.181)\tLoss -9.2572e-01 (-9.2165e-01)\n",
            "Training...\n",
            "Epoch: [598][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.2075e-01 (-9.2075e-01)\n",
            "Epoch: [598][10/97]\tTime  0.178 ( 0.203)\tLoss -9.1286e-01 (-9.2251e-01)\n",
            "Epoch: [598][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2672e-01 (-9.2420e-01)\n",
            "Epoch: [598][30/97]\tTime  0.177 ( 0.186)\tLoss -9.0535e-01 (-9.2387e-01)\n",
            "Epoch: [598][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1792e-01 (-9.2334e-01)\n",
            "Epoch: [598][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1471e-01 (-9.2317e-01)\n",
            "Epoch: [598][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2717e-01 (-9.2361e-01)\n",
            "Epoch: [598][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2377e-01 (-9.2365e-01)\n",
            "Epoch: [598][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2969e-01 (-9.2360e-01)\n",
            "Epoch: [598][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0563e-01 (-9.2329e-01)\n",
            "Training...\n",
            "Epoch: [599][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.1193e-01 (-9.1193e-01)\n",
            "Epoch: [599][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2478e-01 (-9.2253e-01)\n",
            "Epoch: [599][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1021e-01 (-9.2186e-01)\n",
            "Epoch: [599][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2302e-01 (-9.2194e-01)\n",
            "Epoch: [599][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2270e-01 (-9.2242e-01)\n",
            "Epoch: [599][50/97]\tTime  0.176 ( 0.183)\tLoss -9.2036e-01 (-9.2247e-01)\n",
            "Epoch: [599][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2599e-01 (-9.2230e-01)\n",
            "Epoch: [599][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3894e-01 (-9.2281e-01)\n",
            "Epoch: [599][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2230e-01 (-9.2266e-01)\n",
            "Epoch: [599][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2498e-01 (-9.2266e-01)\n",
            "Training...\n",
            "Epoch: [600][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.2803e-01 (-9.2803e-01)\n",
            "Epoch: [600][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2265e-01 (-9.2829e-01)\n",
            "Epoch: [600][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1463e-01 (-9.2614e-01)\n",
            "Epoch: [600][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2557e-01 (-9.2652e-01)\n",
            "Epoch: [600][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3050e-01 (-9.2610e-01)\n",
            "Epoch: [600][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1964e-01 (-9.2507e-01)\n",
            "Epoch: [600][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2462e-01 (-9.2446e-01)\n",
            "Epoch: [600][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2860e-01 (-9.2451e-01)\n",
            "Epoch: [600][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2896e-01 (-9.2442e-01)\n",
            "Epoch: [600][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2661e-01 (-9.2433e-01)\n",
            "Validating...\n",
            "Top1: 0.8665707236842105\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [601][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.3486e-01 (-9.3486e-01)\n",
            "Epoch: [601][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2513e-01 (-9.2512e-01)\n",
            "Epoch: [601][20/97]\tTime  0.178 ( 0.190)\tLoss -9.2213e-01 (-9.2378e-01)\n",
            "Epoch: [601][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1452e-01 (-9.2361e-01)\n",
            "Epoch: [601][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2239e-01 (-9.2405e-01)\n",
            "Epoch: [601][50/97]\tTime  0.177 ( 0.183)\tLoss -9.1256e-01 (-9.2340e-01)\n",
            "Epoch: [601][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2165e-01 (-9.2278e-01)\n",
            "Epoch: [601][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2796e-01 (-9.2256e-01)\n",
            "Epoch: [601][80/97]\tTime  0.178 ( 0.181)\tLoss -9.3240e-01 (-9.2269e-01)\n",
            "Epoch: [601][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2351e-01 (-9.2223e-01)\n",
            "Training...\n",
            "Epoch: [602][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.2414e-01 (-9.2414e-01)\n",
            "Epoch: [602][10/97]\tTime  0.177 ( 0.202)\tLoss -9.1931e-01 (-9.2655e-01)\n",
            "Epoch: [602][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2778e-01 (-9.2614e-01)\n",
            "Epoch: [602][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2316e-01 (-9.2540e-01)\n",
            "Epoch: [602][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2583e-01 (-9.2566e-01)\n",
            "Epoch: [602][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2392e-01 (-9.2515e-01)\n",
            "Epoch: [602][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2871e-01 (-9.2503e-01)\n",
            "Epoch: [602][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2212e-01 (-9.2470e-01)\n",
            "Epoch: [602][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2599e-01 (-9.2445e-01)\n",
            "Epoch: [602][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3006e-01 (-9.2434e-01)\n",
            "Training...\n",
            "Epoch: [603][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.1955e-01 (-9.1955e-01)\n",
            "Epoch: [603][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2004e-01 (-9.2281e-01)\n",
            "Epoch: [603][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2731e-01 (-9.2381e-01)\n",
            "Epoch: [603][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2163e-01 (-9.2435e-01)\n",
            "Epoch: [603][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1999e-01 (-9.2444e-01)\n",
            "Epoch: [603][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2907e-01 (-9.2383e-01)\n",
            "Epoch: [603][60/97]\tTime  0.178 ( 0.182)\tLoss -9.1999e-01 (-9.2319e-01)\n",
            "Epoch: [603][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2174e-01 (-9.2315e-01)\n",
            "Epoch: [603][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2611e-01 (-9.2326e-01)\n",
            "Epoch: [603][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2070e-01 (-9.2299e-01)\n",
            "Training...\n",
            "Epoch: [604][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.3292e-01 (-9.3292e-01)\n",
            "Epoch: [604][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1938e-01 (-9.2654e-01)\n",
            "Epoch: [604][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2925e-01 (-9.2512e-01)\n",
            "Epoch: [604][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2206e-01 (-9.2455e-01)\n",
            "Epoch: [604][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1143e-01 (-9.2465e-01)\n",
            "Epoch: [604][50/97]\tTime  0.177 ( 0.183)\tLoss -9.3403e-01 (-9.2495e-01)\n",
            "Epoch: [604][60/97]\tTime  0.178 ( 0.182)\tLoss -9.3385e-01 (-9.2489e-01)\n",
            "Epoch: [604][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3358e-01 (-9.2479e-01)\n",
            "Epoch: [604][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2493e-01 (-9.2478e-01)\n",
            "Epoch: [604][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2138e-01 (-9.2466e-01)\n",
            "Training...\n",
            "Epoch: [605][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.3040e-01 (-9.3040e-01)\n",
            "Epoch: [605][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2876e-01 (-9.2544e-01)\n",
            "Epoch: [605][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3329e-01 (-9.2618e-01)\n",
            "Epoch: [605][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3128e-01 (-9.2515e-01)\n",
            "Epoch: [605][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3174e-01 (-9.2549e-01)\n",
            "Epoch: [605][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1907e-01 (-9.2459e-01)\n",
            "Epoch: [605][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2537e-01 (-9.2511e-01)\n",
            "Epoch: [605][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1711e-01 (-9.2501e-01)\n",
            "Epoch: [605][80/97]\tTime  0.178 ( 0.181)\tLoss -9.3425e-01 (-9.2509e-01)\n",
            "Epoch: [605][90/97]\tTime  0.177 ( 0.180)\tLoss -9.0845e-01 (-9.2484e-01)\n",
            "Validating...\n",
            "Top1: 0.8629728618421053\n",
            "Training...\n",
            "Epoch: [606][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.2574e-01 (-9.2574e-01)\n",
            "Epoch: [606][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3552e-01 (-9.2796e-01)\n",
            "Epoch: [606][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2911e-01 (-9.2705e-01)\n",
            "Epoch: [606][30/97]\tTime  0.178 ( 0.186)\tLoss -9.1012e-01 (-9.2548e-01)\n",
            "Epoch: [606][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2231e-01 (-9.2496e-01)\n",
            "Epoch: [606][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2327e-01 (-9.2461e-01)\n",
            "Epoch: [606][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2272e-01 (-9.2450e-01)\n",
            "Epoch: [606][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1951e-01 (-9.2458e-01)\n",
            "Epoch: [606][80/97]\tTime  0.178 ( 0.181)\tLoss -9.3003e-01 (-9.2490e-01)\n",
            "Epoch: [606][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2447e-01 (-9.2489e-01)\n",
            "Training...\n",
            "Epoch: [607][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.2841e-01 (-9.2841e-01)\n",
            "Epoch: [607][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2960e-01 (-9.2420e-01)\n",
            "Epoch: [607][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2649e-01 (-9.2501e-01)\n",
            "Epoch: [607][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2159e-01 (-9.2493e-01)\n",
            "Epoch: [607][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2439e-01 (-9.2528e-01)\n",
            "Epoch: [607][50/97]\tTime  0.177 ( 0.183)\tLoss -9.3151e-01 (-9.2573e-01)\n",
            "Epoch: [607][60/97]\tTime  0.176 ( 0.182)\tLoss -9.2490e-01 (-9.2548e-01)\n",
            "Epoch: [607][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2643e-01 (-9.2582e-01)\n",
            "Epoch: [607][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1963e-01 (-9.2593e-01)\n",
            "Epoch: [607][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3072e-01 (-9.2560e-01)\n",
            "Training...\n",
            "Epoch: [608][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.3012e-01 (-9.3012e-01)\n",
            "Epoch: [608][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2801e-01 (-9.2639e-01)\n",
            "Epoch: [608][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2075e-01 (-9.2492e-01)\n",
            "Epoch: [608][30/97]\tTime  0.178 ( 0.186)\tLoss -9.0786e-01 (-9.2508e-01)\n",
            "Epoch: [608][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2902e-01 (-9.2495e-01)\n",
            "Epoch: [608][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2207e-01 (-9.2483e-01)\n",
            "Epoch: [608][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2646e-01 (-9.2456e-01)\n",
            "Epoch: [608][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2054e-01 (-9.2473e-01)\n",
            "Epoch: [608][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1990e-01 (-9.2430e-01)\n",
            "Epoch: [608][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2519e-01 (-9.2418e-01)\n",
            "Training...\n",
            "Epoch: [609][ 0/97]\tTime  0.459 ( 0.459)\tLoss -9.2269e-01 (-9.2269e-01)\n",
            "Epoch: [609][10/97]\tTime  0.178 ( 0.203)\tLoss -9.1803e-01 (-9.2506e-01)\n",
            "Epoch: [609][20/97]\tTime  0.177 ( 0.191)\tLoss -9.3518e-01 (-9.2664e-01)\n",
            "Epoch: [609][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2317e-01 (-9.2644e-01)\n",
            "Epoch: [609][40/97]\tTime  0.176 ( 0.184)\tLoss -9.3155e-01 (-9.2670e-01)\n",
            "Epoch: [609][50/97]\tTime  0.178 ( 0.183)\tLoss -9.3652e-01 (-9.2674e-01)\n",
            "Epoch: [609][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1691e-01 (-9.2614e-01)\n",
            "Epoch: [609][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2825e-01 (-9.2594e-01)\n",
            "Epoch: [609][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2268e-01 (-9.2607e-01)\n",
            "Epoch: [609][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2834e-01 (-9.2596e-01)\n",
            "Training...\n",
            "Epoch: [610][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.3300e-01 (-9.3300e-01)\n",
            "Epoch: [610][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3944e-01 (-9.2905e-01)\n",
            "Epoch: [610][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2297e-01 (-9.2692e-01)\n",
            "Epoch: [610][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2478e-01 (-9.2674e-01)\n",
            "Epoch: [610][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2838e-01 (-9.2616e-01)\n",
            "Epoch: [610][50/97]\tTime  0.178 ( 0.183)\tLoss -9.3636e-01 (-9.2626e-01)\n",
            "Epoch: [610][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2270e-01 (-9.2602e-01)\n",
            "Epoch: [610][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2741e-01 (-9.2577e-01)\n",
            "Epoch: [610][80/97]\tTime  0.177 ( 0.181)\tLoss -9.3491e-01 (-9.2543e-01)\n",
            "Epoch: [610][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3264e-01 (-9.2552e-01)\n",
            "Validating...\n",
            "Top1: 0.8678042763157895\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [611][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.3035e-01 (-9.3035e-01)\n",
            "Epoch: [611][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2864e-01 (-9.2696e-01)\n",
            "Epoch: [611][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2241e-01 (-9.2686e-01)\n",
            "Epoch: [611][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1055e-01 (-9.2554e-01)\n",
            "Epoch: [611][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3236e-01 (-9.2535e-01)\n",
            "Epoch: [611][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2542e-01 (-9.2479e-01)\n",
            "Epoch: [611][60/97]\tTime  0.178 ( 0.182)\tLoss -9.3102e-01 (-9.2451e-01)\n",
            "Epoch: [611][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3553e-01 (-9.2511e-01)\n",
            "Epoch: [611][80/97]\tTime  0.178 ( 0.181)\tLoss -9.1557e-01 (-9.2505e-01)\n",
            "Epoch: [611][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3436e-01 (-9.2531e-01)\n",
            "Training...\n",
            "Epoch: [612][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.2902e-01 (-9.2902e-01)\n",
            "Epoch: [612][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2930e-01 (-9.2500e-01)\n",
            "Epoch: [612][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2755e-01 (-9.2598e-01)\n",
            "Epoch: [612][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2485e-01 (-9.2685e-01)\n",
            "Epoch: [612][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1897e-01 (-9.2613e-01)\n",
            "Epoch: [612][50/97]\tTime  0.177 ( 0.183)\tLoss -9.3006e-01 (-9.2601e-01)\n",
            "Epoch: [612][60/97]\tTime  0.178 ( 0.182)\tLoss -9.2271e-01 (-9.2596e-01)\n",
            "Epoch: [612][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2058e-01 (-9.2625e-01)\n",
            "Epoch: [612][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2234e-01 (-9.2603e-01)\n",
            "Epoch: [612][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3248e-01 (-9.2625e-01)\n",
            "Training...\n",
            "Epoch: [613][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.3808e-01 (-9.3808e-01)\n",
            "Epoch: [613][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1985e-01 (-9.2735e-01)\n",
            "Epoch: [613][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2742e-01 (-9.2731e-01)\n",
            "Epoch: [613][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2043e-01 (-9.2784e-01)\n",
            "Epoch: [613][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2576e-01 (-9.2735e-01)\n",
            "Epoch: [613][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2317e-01 (-9.2733e-01)\n",
            "Epoch: [613][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1473e-01 (-9.2763e-01)\n",
            "Epoch: [613][70/97]\tTime  0.177 ( 0.181)\tLoss -9.1815e-01 (-9.2715e-01)\n",
            "Epoch: [613][80/97]\tTime  0.178 ( 0.181)\tLoss -9.3503e-01 (-9.2689e-01)\n",
            "Epoch: [613][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2025e-01 (-9.2663e-01)\n",
            "Training...\n",
            "Epoch: [614][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.2750e-01 (-9.2750e-01)\n",
            "Epoch: [614][10/97]\tTime  0.178 ( 0.202)\tLoss -9.1428e-01 (-9.2715e-01)\n",
            "Epoch: [614][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3173e-01 (-9.2647e-01)\n",
            "Epoch: [614][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2950e-01 (-9.2598e-01)\n",
            "Epoch: [614][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1519e-01 (-9.2606e-01)\n",
            "Epoch: [614][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2212e-01 (-9.2625e-01)\n",
            "Epoch: [614][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2536e-01 (-9.2637e-01)\n",
            "Epoch: [614][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2417e-01 (-9.2645e-01)\n",
            "Epoch: [614][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2219e-01 (-9.2630e-01)\n",
            "Epoch: [614][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2717e-01 (-9.2651e-01)\n",
            "Training...\n",
            "Epoch: [615][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.2735e-01 (-9.2735e-01)\n",
            "Epoch: [615][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2858e-01 (-9.2593e-01)\n",
            "Epoch: [615][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1548e-01 (-9.2651e-01)\n",
            "Epoch: [615][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2503e-01 (-9.2591e-01)\n",
            "Epoch: [615][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2153e-01 (-9.2626e-01)\n",
            "Epoch: [615][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2212e-01 (-9.2646e-01)\n",
            "Epoch: [615][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2236e-01 (-9.2672e-01)\n",
            "Epoch: [615][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2335e-01 (-9.2651e-01)\n",
            "Epoch: [615][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2365e-01 (-9.2666e-01)\n",
            "Epoch: [615][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1829e-01 (-9.2622e-01)\n",
            "Validating...\n",
            "Top1: 0.8589638157894737\n",
            "Training...\n",
            "Epoch: [616][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.3299e-01 (-9.3299e-01)\n",
            "Epoch: [616][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3561e-01 (-9.2711e-01)\n",
            "Epoch: [616][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2868e-01 (-9.2844e-01)\n",
            "Epoch: [616][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2927e-01 (-9.2779e-01)\n",
            "Epoch: [616][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3131e-01 (-9.2686e-01)\n",
            "Epoch: [616][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2039e-01 (-9.2711e-01)\n",
            "Epoch: [616][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3041e-01 (-9.2729e-01)\n",
            "Epoch: [616][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2898e-01 (-9.2744e-01)\n",
            "Epoch: [616][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2503e-01 (-9.2732e-01)\n",
            "Epoch: [616][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2672e-01 (-9.2710e-01)\n",
            "Training...\n",
            "Epoch: [617][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.2013e-01 (-9.2013e-01)\n",
            "Epoch: [617][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2211e-01 (-9.2482e-01)\n",
            "Epoch: [617][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3306e-01 (-9.2694e-01)\n",
            "Epoch: [617][30/97]\tTime  0.178 ( 0.186)\tLoss -9.3653e-01 (-9.2754e-01)\n",
            "Epoch: [617][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2887e-01 (-9.2747e-01)\n",
            "Epoch: [617][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2448e-01 (-9.2794e-01)\n",
            "Epoch: [617][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3590e-01 (-9.2816e-01)\n",
            "Epoch: [617][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2925e-01 (-9.2811e-01)\n",
            "Epoch: [617][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1753e-01 (-9.2768e-01)\n",
            "Epoch: [617][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1764e-01 (-9.2750e-01)\n",
            "Training...\n",
            "Epoch: [618][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.1692e-01 (-9.1692e-01)\n",
            "Epoch: [618][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2278e-01 (-9.2319e-01)\n",
            "Epoch: [618][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2492e-01 (-9.2479e-01)\n",
            "Epoch: [618][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2403e-01 (-9.2598e-01)\n",
            "Epoch: [618][40/97]\tTime  0.176 ( 0.184)\tLoss -9.2260e-01 (-9.2730e-01)\n",
            "Epoch: [618][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2900e-01 (-9.2734e-01)\n",
            "Epoch: [618][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3522e-01 (-9.2760e-01)\n",
            "Epoch: [618][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2556e-01 (-9.2731e-01)\n",
            "Epoch: [618][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2555e-01 (-9.2733e-01)\n",
            "Epoch: [618][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4244e-01 (-9.2721e-01)\n",
            "Training...\n",
            "Epoch: [619][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.3363e-01 (-9.3363e-01)\n",
            "Epoch: [619][10/97]\tTime  0.178 ( 0.202)\tLoss -9.2420e-01 (-9.2351e-01)\n",
            "Epoch: [619][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2588e-01 (-9.2645e-01)\n",
            "Epoch: [619][30/97]\tTime  0.178 ( 0.186)\tLoss -9.3103e-01 (-9.2792e-01)\n",
            "Epoch: [619][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2960e-01 (-9.2815e-01)\n",
            "Epoch: [619][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2281e-01 (-9.2779e-01)\n",
            "Epoch: [619][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3334e-01 (-9.2763e-01)\n",
            "Epoch: [619][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2642e-01 (-9.2730e-01)\n",
            "Epoch: [619][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2673e-01 (-9.2740e-01)\n",
            "Epoch: [619][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3365e-01 (-9.2734e-01)\n",
            "Training...\n",
            "Epoch: [620][ 0/97]\tTime  0.465 ( 0.465)\tLoss -9.2160e-01 (-9.2160e-01)\n",
            "Epoch: [620][10/97]\tTime  0.178 ( 0.203)\tLoss -9.3146e-01 (-9.2600e-01)\n",
            "Epoch: [620][20/97]\tTime  0.177 ( 0.191)\tLoss -9.3340e-01 (-9.2687e-01)\n",
            "Epoch: [620][30/97]\tTime  0.178 ( 0.187)\tLoss -9.2040e-01 (-9.2672e-01)\n",
            "Epoch: [620][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2755e-01 (-9.2700e-01)\n",
            "Epoch: [620][50/97]\tTime  0.178 ( 0.183)\tLoss -9.1869e-01 (-9.2631e-01)\n",
            "Epoch: [620][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2926e-01 (-9.2684e-01)\n",
            "Epoch: [620][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2820e-01 (-9.2658e-01)\n",
            "Epoch: [620][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1647e-01 (-9.2665e-01)\n",
            "Epoch: [620][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2572e-01 (-9.2634e-01)\n",
            "Validating...\n",
            "Top1: 0.8638980263157895\n",
            "Training...\n",
            "Epoch: [621][ 0/97]\tTime  0.465 ( 0.465)\tLoss -9.3879e-01 (-9.3879e-01)\n",
            "Epoch: [621][10/97]\tTime  0.177 ( 0.203)\tLoss -9.3326e-01 (-9.3092e-01)\n",
            "Epoch: [621][20/97]\tTime  0.177 ( 0.191)\tLoss -9.2163e-01 (-9.3130e-01)\n",
            "Epoch: [621][30/97]\tTime  0.178 ( 0.187)\tLoss -9.2914e-01 (-9.3049e-01)\n",
            "Epoch: [621][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1945e-01 (-9.2965e-01)\n",
            "Epoch: [621][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2535e-01 (-9.2862e-01)\n",
            "Epoch: [621][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2857e-01 (-9.2861e-01)\n",
            "Epoch: [621][70/97]\tTime  0.178 ( 0.181)\tLoss -9.3433e-01 (-9.2843e-01)\n",
            "Epoch: [621][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2666e-01 (-9.2812e-01)\n",
            "Epoch: [621][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2604e-01 (-9.2818e-01)\n",
            "Training...\n",
            "Epoch: [622][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.3338e-01 (-9.3338e-01)\n",
            "Epoch: [622][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3191e-01 (-9.2811e-01)\n",
            "Epoch: [622][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2534e-01 (-9.3033e-01)\n",
            "Epoch: [622][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2340e-01 (-9.2893e-01)\n",
            "Epoch: [622][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3397e-01 (-9.2873e-01)\n",
            "Epoch: [622][50/97]\tTime  0.178 ( 0.183)\tLoss -9.1371e-01 (-9.2810e-01)\n",
            "Epoch: [622][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1807e-01 (-9.2838e-01)\n",
            "Epoch: [622][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2447e-01 (-9.2848e-01)\n",
            "Epoch: [622][80/97]\tTime  0.177 ( 0.181)\tLoss -9.3891e-01 (-9.2864e-01)\n",
            "Epoch: [622][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2617e-01 (-9.2856e-01)\n",
            "Training...\n",
            "Epoch: [623][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.2601e-01 (-9.2601e-01)\n",
            "Epoch: [623][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2128e-01 (-9.3155e-01)\n",
            "Epoch: [623][20/97]\tTime  0.177 ( 0.190)\tLoss -9.1927e-01 (-9.2991e-01)\n",
            "Epoch: [623][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2870e-01 (-9.2919e-01)\n",
            "Epoch: [623][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2248e-01 (-9.2905e-01)\n",
            "Epoch: [623][50/97]\tTime  0.178 ( 0.183)\tLoss -9.2625e-01 (-9.2934e-01)\n",
            "Epoch: [623][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3332e-01 (-9.2936e-01)\n",
            "Epoch: [623][70/97]\tTime  0.178 ( 0.181)\tLoss -9.3881e-01 (-9.2937e-01)\n",
            "Epoch: [623][80/97]\tTime  0.177 ( 0.181)\tLoss -9.1699e-01 (-9.2886e-01)\n",
            "Epoch: [623][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2633e-01 (-9.2883e-01)\n",
            "Training...\n",
            "Epoch: [624][ 0/97]\tTime  0.461 ( 0.461)\tLoss -9.2860e-01 (-9.2860e-01)\n",
            "Epoch: [624][10/97]\tTime  0.177 ( 0.203)\tLoss -9.3148e-01 (-9.3030e-01)\n",
            "Epoch: [624][20/97]\tTime  0.177 ( 0.191)\tLoss -9.3372e-01 (-9.2914e-01)\n",
            "Epoch: [624][30/97]\tTime  0.178 ( 0.186)\tLoss -9.2574e-01 (-9.2857e-01)\n",
            "Epoch: [624][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3351e-01 (-9.2808e-01)\n",
            "Epoch: [624][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4216e-01 (-9.2811e-01)\n",
            "Epoch: [624][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2785e-01 (-9.2778e-01)\n",
            "Epoch: [624][70/97]\tTime  0.184 ( 0.181)\tLoss -9.2578e-01 (-9.2844e-01)\n",
            "Epoch: [624][80/97]\tTime  0.182 ( 0.182)\tLoss -9.2382e-01 (-9.2814e-01)\n",
            "Epoch: [624][90/97]\tTime  0.180 ( 0.181)\tLoss -9.2928e-01 (-9.2764e-01)\n",
            "Training...\n",
            "Epoch: [625][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.2887e-01 (-9.2887e-01)\n",
            "Epoch: [625][10/97]\tTime  0.180 ( 0.205)\tLoss -9.2210e-01 (-9.3006e-01)\n",
            "Epoch: [625][20/97]\tTime  0.178 ( 0.192)\tLoss -9.3141e-01 (-9.2939e-01)\n",
            "Epoch: [625][30/97]\tTime  0.177 ( 0.188)\tLoss -9.2915e-01 (-9.2819e-01)\n",
            "Epoch: [625][40/97]\tTime  0.177 ( 0.185)\tLoss -9.3646e-01 (-9.2793e-01)\n",
            "Epoch: [625][50/97]\tTime  0.177 ( 0.184)\tLoss -9.3680e-01 (-9.2815e-01)\n",
            "Epoch: [625][60/97]\tTime  0.177 ( 0.182)\tLoss -9.1879e-01 (-9.2796e-01)\n",
            "Epoch: [625][70/97]\tTime  0.177 ( 0.182)\tLoss -9.3477e-01 (-9.2823e-01)\n",
            "Epoch: [625][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2022e-01 (-9.2804e-01)\n",
            "Epoch: [625][90/97]\tTime  0.177 ( 0.181)\tLoss -9.2926e-01 (-9.2792e-01)\n",
            "Validating...\n",
            "Top1: 0.8669819078947368\n",
            "Training...\n",
            "Epoch: [626][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.2320e-01 (-9.2320e-01)\n",
            "Epoch: [626][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2750e-01 (-9.2865e-01)\n",
            "Epoch: [626][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2320e-01 (-9.2813e-01)\n",
            "Epoch: [626][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3260e-01 (-9.2898e-01)\n",
            "Epoch: [626][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2361e-01 (-9.2897e-01)\n",
            "Epoch: [626][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3574e-01 (-9.2915e-01)\n",
            "Epoch: [626][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2469e-01 (-9.2866e-01)\n",
            "Epoch: [626][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3796e-01 (-9.2879e-01)\n",
            "Epoch: [626][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2907e-01 (-9.2881e-01)\n",
            "Epoch: [626][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2257e-01 (-9.2891e-01)\n",
            "Training...\n",
            "Epoch: [627][ 0/97]\tTime  0.442 ( 0.442)\tLoss -9.2969e-01 (-9.2969e-01)\n",
            "Epoch: [627][10/97]\tTime  0.177 ( 0.201)\tLoss -9.2155e-01 (-9.3026e-01)\n",
            "Epoch: [627][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2957e-01 (-9.3037e-01)\n",
            "Epoch: [627][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2219e-01 (-9.2976e-01)\n",
            "Epoch: [627][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2761e-01 (-9.2954e-01)\n",
            "Epoch: [627][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2502e-01 (-9.2941e-01)\n",
            "Epoch: [627][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2485e-01 (-9.2919e-01)\n",
            "Epoch: [627][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2968e-01 (-9.2916e-01)\n",
            "Epoch: [627][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3061e-01 (-9.2924e-01)\n",
            "Epoch: [627][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2693e-01 (-9.2916e-01)\n",
            "Training...\n",
            "Epoch: [628][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.2711e-01 (-9.2711e-01)\n",
            "Epoch: [628][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2890e-01 (-9.3270e-01)\n",
            "Epoch: [628][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3623e-01 (-9.3111e-01)\n",
            "Epoch: [628][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2885e-01 (-9.2999e-01)\n",
            "Epoch: [628][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2891e-01 (-9.2964e-01)\n",
            "Epoch: [628][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3808e-01 (-9.2933e-01)\n",
            "Epoch: [628][60/97]\tTime  0.176 ( 0.181)\tLoss -9.2321e-01 (-9.2894e-01)\n",
            "Epoch: [628][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3266e-01 (-9.2897e-01)\n",
            "Epoch: [628][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2718e-01 (-9.2899e-01)\n",
            "Epoch: [628][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3179e-01 (-9.2916e-01)\n",
            "Training...\n",
            "Epoch: [629][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.2447e-01 (-9.2447e-01)\n",
            "Epoch: [629][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3148e-01 (-9.2642e-01)\n",
            "Epoch: [629][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3035e-01 (-9.2767e-01)\n",
            "Epoch: [629][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2220e-01 (-9.2767e-01)\n",
            "Epoch: [629][40/97]\tTime  0.177 ( 0.183)\tLoss -9.2160e-01 (-9.2801e-01)\n",
            "Epoch: [629][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3378e-01 (-9.2868e-01)\n",
            "Epoch: [629][60/97]\tTime  0.178 ( 0.181)\tLoss -9.3044e-01 (-9.2914e-01)\n",
            "Epoch: [629][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3260e-01 (-9.2922e-01)\n",
            "Epoch: [629][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2716e-01 (-9.2868e-01)\n",
            "Epoch: [629][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2488e-01 (-9.2877e-01)\n",
            "Training...\n",
            "Epoch: [630][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.4495e-01 (-9.4495e-01)\n",
            "Epoch: [630][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3612e-01 (-9.3038e-01)\n",
            "Epoch: [630][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2103e-01 (-9.3082e-01)\n",
            "Epoch: [630][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3617e-01 (-9.2996e-01)\n",
            "Epoch: [630][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2974e-01 (-9.2919e-01)\n",
            "Epoch: [630][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2960e-01 (-9.2939e-01)\n",
            "Epoch: [630][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2887e-01 (-9.2931e-01)\n",
            "Epoch: [630][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2048e-01 (-9.2927e-01)\n",
            "Epoch: [630][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3320e-01 (-9.2934e-01)\n",
            "Epoch: [630][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4340e-01 (-9.2919e-01)\n",
            "Validating...\n",
            "Top1: 0.867907072368421\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [631][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.2606e-01 (-9.2606e-01)\n",
            "Epoch: [631][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3262e-01 (-9.3322e-01)\n",
            "Epoch: [631][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3349e-01 (-9.3179e-01)\n",
            "Epoch: [631][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2479e-01 (-9.3022e-01)\n",
            "Epoch: [631][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2895e-01 (-9.2990e-01)\n",
            "Epoch: [631][50/97]\tTime  0.176 ( 0.182)\tLoss -9.3112e-01 (-9.3015e-01)\n",
            "Epoch: [631][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2525e-01 (-9.3030e-01)\n",
            "Epoch: [631][70/97]\tTime  0.176 ( 0.181)\tLoss -9.3168e-01 (-9.3038e-01)\n",
            "Epoch: [631][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3211e-01 (-9.3058e-01)\n",
            "Epoch: [631][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2694e-01 (-9.3030e-01)\n",
            "Training...\n",
            "Epoch: [632][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.2467e-01 (-9.2467e-01)\n",
            "Epoch: [632][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2526e-01 (-9.2990e-01)\n",
            "Epoch: [632][20/97]\tTime  0.178 ( 0.190)\tLoss -9.2673e-01 (-9.3037e-01)\n",
            "Epoch: [632][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3147e-01 (-9.3034e-01)\n",
            "Epoch: [632][40/97]\tTime  0.177 ( 0.184)\tLoss -9.1797e-01 (-9.2928e-01)\n",
            "Epoch: [632][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3830e-01 (-9.2961e-01)\n",
            "Epoch: [632][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2776e-01 (-9.3017e-01)\n",
            "Epoch: [632][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2787e-01 (-9.3023e-01)\n",
            "Epoch: [632][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2477e-01 (-9.3027e-01)\n",
            "Epoch: [632][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2279e-01 (-9.3002e-01)\n",
            "Training...\n",
            "Epoch: [633][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.2926e-01 (-9.2926e-01)\n",
            "Epoch: [633][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2640e-01 (-9.3060e-01)\n",
            "Epoch: [633][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2101e-01 (-9.3040e-01)\n",
            "Epoch: [633][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2765e-01 (-9.3008e-01)\n",
            "Epoch: [633][40/97]\tTime  0.178 ( 0.184)\tLoss -9.3039e-01 (-9.2969e-01)\n",
            "Epoch: [633][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2904e-01 (-9.2956e-01)\n",
            "Epoch: [633][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2079e-01 (-9.2904e-01)\n",
            "Epoch: [633][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2776e-01 (-9.2893e-01)\n",
            "Epoch: [633][80/97]\tTime  0.178 ( 0.180)\tLoss -9.1936e-01 (-9.2913e-01)\n",
            "Epoch: [633][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2310e-01 (-9.2880e-01)\n",
            "Training...\n",
            "Epoch: [634][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.4343e-01 (-9.4343e-01)\n",
            "Epoch: [634][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2479e-01 (-9.2891e-01)\n",
            "Epoch: [634][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2162e-01 (-9.2920e-01)\n",
            "Epoch: [634][30/97]\tTime  0.176 ( 0.186)\tLoss -9.2973e-01 (-9.2925e-01)\n",
            "Epoch: [634][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3251e-01 (-9.2857e-01)\n",
            "Epoch: [634][50/97]\tTime  0.176 ( 0.182)\tLoss -9.3451e-01 (-9.2843e-01)\n",
            "Epoch: [634][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3734e-01 (-9.2862e-01)\n",
            "Epoch: [634][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3632e-01 (-9.2895e-01)\n",
            "Epoch: [634][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2600e-01 (-9.2885e-01)\n",
            "Epoch: [634][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3184e-01 (-9.2892e-01)\n",
            "Training...\n",
            "Epoch: [635][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.2881e-01 (-9.2881e-01)\n",
            "Epoch: [635][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3013e-01 (-9.2768e-01)\n",
            "Epoch: [635][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2624e-01 (-9.2829e-01)\n",
            "Epoch: [635][30/97]\tTime  0.178 ( 0.186)\tLoss -9.3155e-01 (-9.2807e-01)\n",
            "Epoch: [635][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3032e-01 (-9.2788e-01)\n",
            "Epoch: [635][50/97]\tTime  0.177 ( 0.182)\tLoss -9.1720e-01 (-9.2768e-01)\n",
            "Epoch: [635][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2756e-01 (-9.2774e-01)\n",
            "Epoch: [635][70/97]\tTime  0.176 ( 0.181)\tLoss -9.3844e-01 (-9.2797e-01)\n",
            "Epoch: [635][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2979e-01 (-9.2813e-01)\n",
            "Epoch: [635][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2250e-01 (-9.2843e-01)\n",
            "Validating...\n",
            "Top1: 0.870374177631579\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [636][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.2856e-01 (-9.2856e-01)\n",
            "Epoch: [636][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2891e-01 (-9.3141e-01)\n",
            "Epoch: [636][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3175e-01 (-9.3112e-01)\n",
            "Epoch: [636][30/97]\tTime  0.176 ( 0.186)\tLoss -9.3460e-01 (-9.3041e-01)\n",
            "Epoch: [636][40/97]\tTime  0.177 ( 0.184)\tLoss -9.2621e-01 (-9.2971e-01)\n",
            "Epoch: [636][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3302e-01 (-9.3024e-01)\n",
            "Epoch: [636][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4062e-01 (-9.3082e-01)\n",
            "Epoch: [636][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3547e-01 (-9.3045e-01)\n",
            "Epoch: [636][80/97]\tTime  0.178 ( 0.180)\tLoss -9.2192e-01 (-9.3018e-01)\n",
            "Epoch: [636][90/97]\tTime  0.177 ( 0.180)\tLoss -9.1940e-01 (-9.2999e-01)\n",
            "Training...\n",
            "Epoch: [637][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.2887e-01 (-9.2887e-01)\n",
            "Epoch: [637][10/97]\tTime  0.178 ( 0.201)\tLoss -9.2614e-01 (-9.2728e-01)\n",
            "Epoch: [637][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3090e-01 (-9.2867e-01)\n",
            "Epoch: [637][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3288e-01 (-9.2882e-01)\n",
            "Epoch: [637][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3224e-01 (-9.2886e-01)\n",
            "Epoch: [637][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3165e-01 (-9.2886e-01)\n",
            "Epoch: [637][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3400e-01 (-9.2878e-01)\n",
            "Epoch: [637][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2805e-01 (-9.2902e-01)\n",
            "Epoch: [637][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3887e-01 (-9.2923e-01)\n",
            "Epoch: [637][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3356e-01 (-9.2917e-01)\n",
            "Training...\n",
            "Epoch: [638][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.2066e-01 (-9.2066e-01)\n",
            "Epoch: [638][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3235e-01 (-9.2938e-01)\n",
            "Epoch: [638][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3044e-01 (-9.3076e-01)\n",
            "Epoch: [638][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3282e-01 (-9.3098e-01)\n",
            "Epoch: [638][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3659e-01 (-9.3040e-01)\n",
            "Epoch: [638][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2065e-01 (-9.3037e-01)\n",
            "Epoch: [638][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3468e-01 (-9.3062e-01)\n",
            "Epoch: [638][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3420e-01 (-9.3030e-01)\n",
            "Epoch: [638][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3069e-01 (-9.3066e-01)\n",
            "Epoch: [638][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3905e-01 (-9.3088e-01)\n",
            "Training...\n",
            "Epoch: [639][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.3949e-01 (-9.3949e-01)\n",
            "Epoch: [639][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3078e-01 (-9.3382e-01)\n",
            "Epoch: [639][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2531e-01 (-9.3303e-01)\n",
            "Epoch: [639][30/97]\tTime  0.176 ( 0.186)\tLoss -9.3522e-01 (-9.3151e-01)\n",
            "Epoch: [639][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3079e-01 (-9.3167e-01)\n",
            "Epoch: [639][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3404e-01 (-9.3152e-01)\n",
            "Epoch: [639][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3229e-01 (-9.3139e-01)\n",
            "Epoch: [639][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2654e-01 (-9.3140e-01)\n",
            "Epoch: [639][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2900e-01 (-9.3118e-01)\n",
            "Epoch: [639][90/97]\tTime  0.176 ( 0.180)\tLoss -9.2499e-01 (-9.3106e-01)\n",
            "Training...\n",
            "Epoch: [640][ 0/97]\tTime  0.442 ( 0.442)\tLoss -9.3604e-01 (-9.3604e-01)\n",
            "Epoch: [640][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3073e-01 (-9.3258e-01)\n",
            "Epoch: [640][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3698e-01 (-9.3237e-01)\n",
            "Epoch: [640][30/97]\tTime  0.177 ( 0.185)\tLoss -9.1947e-01 (-9.3025e-01)\n",
            "Epoch: [640][40/97]\tTime  0.177 ( 0.183)\tLoss -9.2965e-01 (-9.3014e-01)\n",
            "Epoch: [640][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2983e-01 (-9.3060e-01)\n",
            "Epoch: [640][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2282e-01 (-9.3091e-01)\n",
            "Epoch: [640][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2464e-01 (-9.3064e-01)\n",
            "Epoch: [640][80/97]\tTime  0.176 ( 0.180)\tLoss -9.3567e-01 (-9.3104e-01)\n",
            "Epoch: [640][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3307e-01 (-9.3086e-01)\n",
            "Validating...\n",
            "Top1: 0.8680098684210527\n",
            "Training...\n",
            "Epoch: [641][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.3515e-01 (-9.3515e-01)\n",
            "Epoch: [641][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4293e-01 (-9.3378e-01)\n",
            "Epoch: [641][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3262e-01 (-9.3211e-01)\n",
            "Epoch: [641][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1969e-01 (-9.3160e-01)\n",
            "Epoch: [641][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3151e-01 (-9.3157e-01)\n",
            "Epoch: [641][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3650e-01 (-9.3233e-01)\n",
            "Epoch: [641][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3045e-01 (-9.3221e-01)\n",
            "Epoch: [641][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2813e-01 (-9.3243e-01)\n",
            "Epoch: [641][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1391e-01 (-9.3209e-01)\n",
            "Epoch: [641][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3603e-01 (-9.3209e-01)\n",
            "Training...\n",
            "Epoch: [642][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.2969e-01 (-9.2969e-01)\n",
            "Epoch: [642][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2665e-01 (-9.3023e-01)\n",
            "Epoch: [642][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3208e-01 (-9.2993e-01)\n",
            "Epoch: [642][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2720e-01 (-9.3128e-01)\n",
            "Epoch: [642][40/97]\tTime  0.178 ( 0.184)\tLoss -9.1620e-01 (-9.3119e-01)\n",
            "Epoch: [642][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2413e-01 (-9.3102e-01)\n",
            "Epoch: [642][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4220e-01 (-9.3120e-01)\n",
            "Epoch: [642][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3174e-01 (-9.3117e-01)\n",
            "Epoch: [642][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2908e-01 (-9.3126e-01)\n",
            "Epoch: [642][90/97]\tTime  0.178 ( 0.180)\tLoss -9.3110e-01 (-9.3153e-01)\n",
            "Training...\n",
            "Epoch: [643][ 0/97]\tTime  0.461 ( 0.461)\tLoss -9.2981e-01 (-9.2981e-01)\n",
            "Epoch: [643][10/97]\tTime  0.177 ( 0.203)\tLoss -9.3594e-01 (-9.3285e-01)\n",
            "Epoch: [643][20/97]\tTime  0.178 ( 0.191)\tLoss -9.3463e-01 (-9.3216e-01)\n",
            "Epoch: [643][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3658e-01 (-9.3232e-01)\n",
            "Epoch: [643][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3813e-01 (-9.3216e-01)\n",
            "Epoch: [643][50/97]\tTime  0.177 ( 0.183)\tLoss -9.2960e-01 (-9.3169e-01)\n",
            "Epoch: [643][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2780e-01 (-9.3193e-01)\n",
            "Epoch: [643][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2754e-01 (-9.3193e-01)\n",
            "Epoch: [643][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2413e-01 (-9.3202e-01)\n",
            "Epoch: [643][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3354e-01 (-9.3201e-01)\n",
            "Training...\n",
            "Epoch: [644][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.3184e-01 (-9.3184e-01)\n",
            "Epoch: [644][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2400e-01 (-9.2834e-01)\n",
            "Epoch: [644][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2733e-01 (-9.2983e-01)\n",
            "Epoch: [644][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3131e-01 (-9.3043e-01)\n",
            "Epoch: [644][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3485e-01 (-9.3094e-01)\n",
            "Epoch: [644][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2768e-01 (-9.3095e-01)\n",
            "Epoch: [644][60/97]\tTime  0.176 ( 0.181)\tLoss -9.2886e-01 (-9.3177e-01)\n",
            "Epoch: [644][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3559e-01 (-9.3172e-01)\n",
            "Epoch: [644][80/97]\tTime  0.177 ( 0.180)\tLoss -9.1859e-01 (-9.3150e-01)\n",
            "Epoch: [644][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3136e-01 (-9.3138e-01)\n",
            "Training...\n",
            "Epoch: [645][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.3564e-01 (-9.3564e-01)\n",
            "Epoch: [645][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3444e-01 (-9.3125e-01)\n",
            "Epoch: [645][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3218e-01 (-9.3209e-01)\n",
            "Epoch: [645][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3560e-01 (-9.3269e-01)\n",
            "Epoch: [645][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3445e-01 (-9.3261e-01)\n",
            "Epoch: [645][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3157e-01 (-9.3248e-01)\n",
            "Epoch: [645][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3489e-01 (-9.3289e-01)\n",
            "Epoch: [645][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2742e-01 (-9.3287e-01)\n",
            "Epoch: [645][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2903e-01 (-9.3289e-01)\n",
            "Epoch: [645][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3378e-01 (-9.3258e-01)\n",
            "Validating...\n",
            "Top1: 0.8722245065789473\n",
            "Saving the best model!\n",
            "Training...\n",
            "Epoch: [646][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.3801e-01 (-9.3801e-01)\n",
            "Epoch: [646][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3960e-01 (-9.3505e-01)\n",
            "Epoch: [646][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3464e-01 (-9.3544e-01)\n",
            "Epoch: [646][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2797e-01 (-9.3478e-01)\n",
            "Epoch: [646][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3218e-01 (-9.3475e-01)\n",
            "Epoch: [646][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3087e-01 (-9.3471e-01)\n",
            "Epoch: [646][60/97]\tTime  0.178 ( 0.182)\tLoss -9.3036e-01 (-9.3443e-01)\n",
            "Epoch: [646][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4321e-01 (-9.3412e-01)\n",
            "Epoch: [646][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2628e-01 (-9.3362e-01)\n",
            "Epoch: [646][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3371e-01 (-9.3330e-01)\n",
            "Training...\n",
            "Epoch: [647][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.2903e-01 (-9.2903e-01)\n",
            "Epoch: [647][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3709e-01 (-9.3311e-01)\n",
            "Epoch: [647][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3865e-01 (-9.3219e-01)\n",
            "Epoch: [647][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3407e-01 (-9.3269e-01)\n",
            "Epoch: [647][40/97]\tTime  0.178 ( 0.184)\tLoss -9.2699e-01 (-9.3254e-01)\n",
            "Epoch: [647][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3123e-01 (-9.3206e-01)\n",
            "Epoch: [647][60/97]\tTime  0.177 ( 0.181)\tLoss -9.1175e-01 (-9.3183e-01)\n",
            "Epoch: [647][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3073e-01 (-9.3188e-01)\n",
            "Epoch: [647][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4372e-01 (-9.3243e-01)\n",
            "Epoch: [647][90/97]\tTime  0.176 ( 0.180)\tLoss -9.3225e-01 (-9.3237e-01)\n",
            "Training...\n",
            "Epoch: [648][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.2499e-01 (-9.2499e-01)\n",
            "Epoch: [648][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3159e-01 (-9.3176e-01)\n",
            "Epoch: [648][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3935e-01 (-9.3413e-01)\n",
            "Epoch: [648][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2254e-01 (-9.3386e-01)\n",
            "Epoch: [648][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4077e-01 (-9.3388e-01)\n",
            "Epoch: [648][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3666e-01 (-9.3371e-01)\n",
            "Epoch: [648][60/97]\tTime  0.177 ( 0.181)\tLoss -9.2752e-01 (-9.3323e-01)\n",
            "Epoch: [648][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3805e-01 (-9.3308e-01)\n",
            "Epoch: [648][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4122e-01 (-9.3291e-01)\n",
            "Epoch: [648][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2917e-01 (-9.3304e-01)\n",
            "Training...\n",
            "Epoch: [649][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.3650e-01 (-9.3650e-01)\n",
            "Epoch: [649][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3788e-01 (-9.3064e-01)\n",
            "Epoch: [649][20/97]\tTime  0.176 ( 0.190)\tLoss -9.4040e-01 (-9.3164e-01)\n",
            "Epoch: [649][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2567e-01 (-9.3159e-01)\n",
            "Epoch: [649][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3414e-01 (-9.3233e-01)\n",
            "Epoch: [649][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2948e-01 (-9.3290e-01)\n",
            "Epoch: [649][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3178e-01 (-9.3345e-01)\n",
            "Epoch: [649][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3989e-01 (-9.3342e-01)\n",
            "Epoch: [649][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2541e-01 (-9.3357e-01)\n",
            "Epoch: [649][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2967e-01 (-9.3318e-01)\n",
            "Training...\n",
            "Epoch: [650][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.2362e-01 (-9.2362e-01)\n",
            "Epoch: [650][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3760e-01 (-9.3628e-01)\n",
            "Epoch: [650][20/97]\tTime  0.176 ( 0.190)\tLoss -9.2554e-01 (-9.3424e-01)\n",
            "Epoch: [650][30/97]\tTime  0.176 ( 0.186)\tLoss -9.3482e-01 (-9.3487e-01)\n",
            "Epoch: [650][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3275e-01 (-9.3384e-01)\n",
            "Epoch: [650][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2098e-01 (-9.3338e-01)\n",
            "Epoch: [650][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3989e-01 (-9.3325e-01)\n",
            "Epoch: [650][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3581e-01 (-9.3309e-01)\n",
            "Epoch: [650][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3354e-01 (-9.3259e-01)\n",
            "Epoch: [650][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3586e-01 (-9.3261e-01)\n",
            "Validating...\n",
            "Top1: 0.8707853618421053\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [651][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.3195e-01 (-9.3195e-01)\n",
            "Epoch: [651][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3489e-01 (-9.3585e-01)\n",
            "Epoch: [651][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2903e-01 (-9.3434e-01)\n",
            "Epoch: [651][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3375e-01 (-9.3425e-01)\n",
            "Epoch: [651][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3135e-01 (-9.3390e-01)\n",
            "Epoch: [651][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2958e-01 (-9.3406e-01)\n",
            "Epoch: [651][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3390e-01 (-9.3339e-01)\n",
            "Epoch: [651][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3492e-01 (-9.3364e-01)\n",
            "Epoch: [651][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3938e-01 (-9.3393e-01)\n",
            "Epoch: [651][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3680e-01 (-9.3381e-01)\n",
            "Training...\n",
            "Epoch: [652][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.2736e-01 (-9.2736e-01)\n",
            "Epoch: [652][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3013e-01 (-9.3257e-01)\n",
            "Epoch: [652][20/97]\tTime  0.176 ( 0.190)\tLoss -9.3737e-01 (-9.3445e-01)\n",
            "Epoch: [652][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2819e-01 (-9.3375e-01)\n",
            "Epoch: [652][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4327e-01 (-9.3425e-01)\n",
            "Epoch: [652][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2957e-01 (-9.3402e-01)\n",
            "Epoch: [652][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3789e-01 (-9.3379e-01)\n",
            "Epoch: [652][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3313e-01 (-9.3381e-01)\n",
            "Epoch: [652][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3338e-01 (-9.3386e-01)\n",
            "Epoch: [652][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2605e-01 (-9.3356e-01)\n",
            "Training...\n",
            "Epoch: [653][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.3445e-01 (-9.3445e-01)\n",
            "Epoch: [653][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3343e-01 (-9.3202e-01)\n",
            "Epoch: [653][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3837e-01 (-9.3277e-01)\n",
            "Epoch: [653][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4245e-01 (-9.3404e-01)\n",
            "Epoch: [653][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3517e-01 (-9.3326e-01)\n",
            "Epoch: [653][50/97]\tTime  0.178 ( 0.182)\tLoss -9.3288e-01 (-9.3397e-01)\n",
            "Epoch: [653][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3524e-01 (-9.3432e-01)\n",
            "Epoch: [653][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2978e-01 (-9.3414e-01)\n",
            "Epoch: [653][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3564e-01 (-9.3421e-01)\n",
            "Epoch: [653][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2387e-01 (-9.3431e-01)\n",
            "Training...\n",
            "Epoch: [654][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.4430e-01 (-9.4430e-01)\n",
            "Epoch: [654][10/97]\tTime  0.176 ( 0.202)\tLoss -9.3911e-01 (-9.3699e-01)\n",
            "Epoch: [654][20/97]\tTime  0.176 ( 0.190)\tLoss -9.2766e-01 (-9.3536e-01)\n",
            "Epoch: [654][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4629e-01 (-9.3584e-01)\n",
            "Epoch: [654][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3093e-01 (-9.3533e-01)\n",
            "Epoch: [654][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2843e-01 (-9.3516e-01)\n",
            "Epoch: [654][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4107e-01 (-9.3525e-01)\n",
            "Epoch: [654][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2724e-01 (-9.3486e-01)\n",
            "Epoch: [654][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2901e-01 (-9.3483e-01)\n",
            "Epoch: [654][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3935e-01 (-9.3476e-01)\n",
            "Training...\n",
            "Epoch: [655][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.4205e-01 (-9.4205e-01)\n",
            "Epoch: [655][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4192e-01 (-9.3732e-01)\n",
            "Epoch: [655][20/97]\tTime  0.176 ( 0.190)\tLoss -9.3794e-01 (-9.3772e-01)\n",
            "Epoch: [655][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2950e-01 (-9.3783e-01)\n",
            "Epoch: [655][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3764e-01 (-9.3730e-01)\n",
            "Epoch: [655][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3185e-01 (-9.3643e-01)\n",
            "Epoch: [655][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3909e-01 (-9.3613e-01)\n",
            "Epoch: [655][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3046e-01 (-9.3575e-01)\n",
            "Epoch: [655][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2717e-01 (-9.3585e-01)\n",
            "Epoch: [655][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2339e-01 (-9.3543e-01)\n",
            "Validating...\n",
            "Top1: 0.8662623355263158\n",
            "Training...\n",
            "Epoch: [656][ 0/97]\tTime  0.458 ( 0.458)\tLoss -9.3119e-01 (-9.3119e-01)\n",
            "Epoch: [656][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4587e-01 (-9.3642e-01)\n",
            "Epoch: [656][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2819e-01 (-9.3463e-01)\n",
            "Epoch: [656][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2885e-01 (-9.3439e-01)\n",
            "Epoch: [656][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4233e-01 (-9.3488e-01)\n",
            "Epoch: [656][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3321e-01 (-9.3463e-01)\n",
            "Epoch: [656][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3230e-01 (-9.3473e-01)\n",
            "Epoch: [656][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2897e-01 (-9.3466e-01)\n",
            "Epoch: [656][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3769e-01 (-9.3454e-01)\n",
            "Epoch: [656][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3554e-01 (-9.3453e-01)\n",
            "Training...\n",
            "Epoch: [657][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.3309e-01 (-9.3309e-01)\n",
            "Epoch: [657][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3781e-01 (-9.3429e-01)\n",
            "Epoch: [657][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3285e-01 (-9.3651e-01)\n",
            "Epoch: [657][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4209e-01 (-9.3686e-01)\n",
            "Epoch: [657][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3983e-01 (-9.3642e-01)\n",
            "Epoch: [657][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2641e-01 (-9.3628e-01)\n",
            "Epoch: [657][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3360e-01 (-9.3615e-01)\n",
            "Epoch: [657][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4063e-01 (-9.3640e-01)\n",
            "Epoch: [657][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3372e-01 (-9.3640e-01)\n",
            "Epoch: [657][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3735e-01 (-9.3590e-01)\n",
            "Training...\n",
            "Epoch: [658][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.3250e-01 (-9.3250e-01)\n",
            "Epoch: [658][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2656e-01 (-9.3723e-01)\n",
            "Epoch: [658][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3299e-01 (-9.3625e-01)\n",
            "Epoch: [658][30/97]\tTime  0.177 ( 0.186)\tLoss -9.1811e-01 (-9.3518e-01)\n",
            "Epoch: [658][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4163e-01 (-9.3528e-01)\n",
            "Epoch: [658][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2788e-01 (-9.3463e-01)\n",
            "Epoch: [658][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3873e-01 (-9.3464e-01)\n",
            "Epoch: [658][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3879e-01 (-9.3483e-01)\n",
            "Epoch: [658][80/97]\tTime  0.178 ( 0.180)\tLoss -9.3810e-01 (-9.3506e-01)\n",
            "Epoch: [658][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3643e-01 (-9.3520e-01)\n",
            "Training...\n",
            "Epoch: [659][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.3791e-01 (-9.3791e-01)\n",
            "Epoch: [659][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3034e-01 (-9.3470e-01)\n",
            "Epoch: [659][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3986e-01 (-9.3565e-01)\n",
            "Epoch: [659][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3068e-01 (-9.3547e-01)\n",
            "Epoch: [659][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3517e-01 (-9.3605e-01)\n",
            "Epoch: [659][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4116e-01 (-9.3587e-01)\n",
            "Epoch: [659][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3587e-01 (-9.3592e-01)\n",
            "Epoch: [659][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3665e-01 (-9.3621e-01)\n",
            "Epoch: [659][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2865e-01 (-9.3596e-01)\n",
            "Epoch: [659][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4312e-01 (-9.3593e-01)\n",
            "Training...\n",
            "Epoch: [660][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.3034e-01 (-9.3034e-01)\n",
            "Epoch: [660][10/97]\tTime  0.176 ( 0.202)\tLoss -9.2747e-01 (-9.3261e-01)\n",
            "Epoch: [660][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3474e-01 (-9.3344e-01)\n",
            "Epoch: [660][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3670e-01 (-9.3456e-01)\n",
            "Epoch: [660][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3638e-01 (-9.3460e-01)\n",
            "Epoch: [660][50/97]\tTime  0.176 ( 0.182)\tLoss -9.3805e-01 (-9.3491e-01)\n",
            "Epoch: [660][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3499e-01 (-9.3438e-01)\n",
            "Epoch: [660][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2464e-01 (-9.3438e-01)\n",
            "Epoch: [660][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3101e-01 (-9.3404e-01)\n",
            "Epoch: [660][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3295e-01 (-9.3417e-01)\n",
            "Validating...\n",
            "Top1: 0.8648231907894737\n",
            "Training...\n",
            "Epoch: [661][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.4588e-01 (-9.4588e-01)\n",
            "Epoch: [661][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3411e-01 (-9.3777e-01)\n",
            "Epoch: [661][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2242e-01 (-9.3714e-01)\n",
            "Epoch: [661][30/97]\tTime  0.178 ( 0.186)\tLoss -9.3917e-01 (-9.3696e-01)\n",
            "Epoch: [661][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3437e-01 (-9.3590e-01)\n",
            "Epoch: [661][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4003e-01 (-9.3619e-01)\n",
            "Epoch: [661][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3944e-01 (-9.3609e-01)\n",
            "Epoch: [661][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2875e-01 (-9.3619e-01)\n",
            "Epoch: [661][80/97]\tTime  0.176 ( 0.180)\tLoss -9.3851e-01 (-9.3607e-01)\n",
            "Epoch: [661][90/97]\tTime  0.176 ( 0.180)\tLoss -9.2975e-01 (-9.3612e-01)\n",
            "Training...\n",
            "Epoch: [662][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.3780e-01 (-9.3780e-01)\n",
            "Epoch: [662][10/97]\tTime  0.178 ( 0.201)\tLoss -9.3707e-01 (-9.3613e-01)\n",
            "Epoch: [662][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3346e-01 (-9.3563e-01)\n",
            "Epoch: [662][30/97]\tTime  0.177 ( 0.185)\tLoss -9.3872e-01 (-9.3491e-01)\n",
            "Epoch: [662][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3695e-01 (-9.3527e-01)\n",
            "Epoch: [662][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3601e-01 (-9.3508e-01)\n",
            "Epoch: [662][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4356e-01 (-9.3518e-01)\n",
            "Epoch: [662][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4486e-01 (-9.3509e-01)\n",
            "Epoch: [662][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3655e-01 (-9.3527e-01)\n",
            "Epoch: [662][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3504e-01 (-9.3528e-01)\n",
            "Training...\n",
            "Epoch: [663][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.2879e-01 (-9.2879e-01)\n",
            "Epoch: [663][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3570e-01 (-9.3569e-01)\n",
            "Epoch: [663][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4163e-01 (-9.3536e-01)\n",
            "Epoch: [663][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3229e-01 (-9.3598e-01)\n",
            "Epoch: [663][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3223e-01 (-9.3578e-01)\n",
            "Epoch: [663][50/97]\tTime  0.176 ( 0.182)\tLoss -9.3469e-01 (-9.3594e-01)\n",
            "Epoch: [663][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3666e-01 (-9.3627e-01)\n",
            "Epoch: [663][70/97]\tTime  0.178 ( 0.181)\tLoss -9.2957e-01 (-9.3592e-01)\n",
            "Epoch: [663][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2848e-01 (-9.3616e-01)\n",
            "Epoch: [663][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3298e-01 (-9.3639e-01)\n",
            "Training...\n",
            "Epoch: [664][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.3008e-01 (-9.3008e-01)\n",
            "Epoch: [664][10/97]\tTime  0.177 ( 0.201)\tLoss -9.2635e-01 (-9.3429e-01)\n",
            "Epoch: [664][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3847e-01 (-9.3621e-01)\n",
            "Epoch: [664][30/97]\tTime  0.177 ( 0.186)\tLoss -9.2933e-01 (-9.3608e-01)\n",
            "Epoch: [664][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3996e-01 (-9.3600e-01)\n",
            "Epoch: [664][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3552e-01 (-9.3590e-01)\n",
            "Epoch: [664][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3059e-01 (-9.3572e-01)\n",
            "Epoch: [664][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4129e-01 (-9.3582e-01)\n",
            "Epoch: [664][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3873e-01 (-9.3578e-01)\n",
            "Epoch: [664][90/97]\tTime  0.178 ( 0.180)\tLoss -9.2916e-01 (-9.3557e-01)\n",
            "Training...\n",
            "Epoch: [665][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.2908e-01 (-9.2908e-01)\n",
            "Epoch: [665][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2880e-01 (-9.3806e-01)\n",
            "Epoch: [665][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3300e-01 (-9.3754e-01)\n",
            "Epoch: [665][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3246e-01 (-9.3583e-01)\n",
            "Epoch: [665][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3276e-01 (-9.3589e-01)\n",
            "Epoch: [665][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3984e-01 (-9.3612e-01)\n",
            "Epoch: [665][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3314e-01 (-9.3652e-01)\n",
            "Epoch: [665][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4190e-01 (-9.3633e-01)\n",
            "Epoch: [665][80/97]\tTime  0.176 ( 0.180)\tLoss -9.2977e-01 (-9.3624e-01)\n",
            "Epoch: [665][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4609e-01 (-9.3638e-01)\n",
            "Validating...\n",
            "Top1: 0.864514802631579\n",
            "Training...\n",
            "Epoch: [666][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.4078e-01 (-9.4078e-01)\n",
            "Epoch: [666][10/97]\tTime  0.176 ( 0.201)\tLoss -9.3398e-01 (-9.3952e-01)\n",
            "Epoch: [666][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3237e-01 (-9.3741e-01)\n",
            "Epoch: [666][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4640e-01 (-9.3601e-01)\n",
            "Epoch: [666][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4480e-01 (-9.3605e-01)\n",
            "Epoch: [666][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3074e-01 (-9.3639e-01)\n",
            "Epoch: [666][60/97]\tTime  0.193 ( 0.182)\tLoss -9.4549e-01 (-9.3655e-01)\n",
            "Epoch: [666][70/97]\tTime  0.178 ( 0.181)\tLoss -9.3486e-01 (-9.3621e-01)\n",
            "Epoch: [666][80/97]\tTime  0.178 ( 0.181)\tLoss -9.2817e-01 (-9.3634e-01)\n",
            "Epoch: [666][90/97]\tTime  0.177 ( 0.181)\tLoss -9.3363e-01 (-9.3612e-01)\n",
            "Training...\n",
            "Epoch: [667][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.3353e-01 (-9.3353e-01)\n",
            "Epoch: [667][10/97]\tTime  0.179 ( 0.215)\tLoss -9.2977e-01 (-9.3468e-01)\n",
            "Epoch: [667][20/97]\tTime  0.188 ( 0.201)\tLoss -9.4093e-01 (-9.3548e-01)\n",
            "Epoch: [667][30/97]\tTime  0.188 ( 0.198)\tLoss -9.3320e-01 (-9.3545e-01)\n",
            "Epoch: [667][40/97]\tTime  0.190 ( 0.196)\tLoss -9.2870e-01 (-9.3507e-01)\n",
            "Epoch: [667][50/97]\tTime  0.190 ( 0.195)\tLoss -9.4035e-01 (-9.3590e-01)\n",
            "Epoch: [667][60/97]\tTime  0.191 ( 0.194)\tLoss -9.3451e-01 (-9.3562e-01)\n",
            "Epoch: [667][70/97]\tTime  0.190 ( 0.193)\tLoss -9.3423e-01 (-9.3552e-01)\n",
            "Epoch: [667][80/97]\tTime  0.185 ( 0.193)\tLoss -9.3847e-01 (-9.3558e-01)\n",
            "Epoch: [667][90/97]\tTime  0.183 ( 0.192)\tLoss -9.4514e-01 (-9.3534e-01)\n",
            "Training...\n",
            "Epoch: [668][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.3080e-01 (-9.3080e-01)\n",
            "Epoch: [668][10/97]\tTime  0.184 ( 0.208)\tLoss -9.3558e-01 (-9.3500e-01)\n",
            "Epoch: [668][20/97]\tTime  0.182 ( 0.196)\tLoss -9.3493e-01 (-9.3581e-01)\n",
            "Epoch: [668][30/97]\tTime  0.182 ( 0.192)\tLoss -9.5089e-01 (-9.3694e-01)\n",
            "Epoch: [668][40/97]\tTime  0.211 ( 0.191)\tLoss -9.2306e-01 (-9.3685e-01)\n",
            "Epoch: [668][50/97]\tTime  0.187 ( 0.191)\tLoss -9.4365e-01 (-9.3697e-01)\n",
            "Epoch: [668][60/97]\tTime  0.190 ( 0.190)\tLoss -9.3004e-01 (-9.3681e-01)\n",
            "Epoch: [668][70/97]\tTime  0.190 ( 0.191)\tLoss -9.4488e-01 (-9.3666e-01)\n",
            "Epoch: [668][80/97]\tTime  0.190 ( 0.191)\tLoss -9.3079e-01 (-9.3631e-01)\n",
            "Epoch: [668][90/97]\tTime  0.191 ( 0.191)\tLoss -9.3900e-01 (-9.3627e-01)\n",
            "Training...\n",
            "Epoch: [669][ 0/97]\tTime  0.459 ( 0.459)\tLoss -9.3689e-01 (-9.3689e-01)\n",
            "Epoch: [669][10/97]\tTime  0.177 ( 0.205)\tLoss -9.4214e-01 (-9.3903e-01)\n",
            "Epoch: [669][20/97]\tTime  0.177 ( 0.192)\tLoss -9.4446e-01 (-9.3764e-01)\n",
            "Epoch: [669][30/97]\tTime  0.177 ( 0.187)\tLoss -9.4072e-01 (-9.3725e-01)\n",
            "Epoch: [669][40/97]\tTime  0.177 ( 0.185)\tLoss -9.3103e-01 (-9.3759e-01)\n",
            "Epoch: [669][50/97]\tTime  0.177 ( 0.183)\tLoss -9.3556e-01 (-9.3744e-01)\n",
            "Epoch: [669][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3947e-01 (-9.3769e-01)\n",
            "Epoch: [669][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2254e-01 (-9.3731e-01)\n",
            "Epoch: [669][80/97]\tTime  0.177 ( 0.181)\tLoss -9.2588e-01 (-9.3712e-01)\n",
            "Epoch: [669][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3645e-01 (-9.3732e-01)\n",
            "Training...\n",
            "Epoch: [670][ 0/97]\tTime  0.463 ( 0.463)\tLoss -9.3962e-01 (-9.3962e-01)\n",
            "Epoch: [670][10/97]\tTime  0.178 ( 0.203)\tLoss -9.3924e-01 (-9.3951e-01)\n",
            "Epoch: [670][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3323e-01 (-9.3786e-01)\n",
            "Epoch: [670][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3985e-01 (-9.3766e-01)\n",
            "Epoch: [670][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3896e-01 (-9.3762e-01)\n",
            "Epoch: [670][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3419e-01 (-9.3768e-01)\n",
            "Epoch: [670][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3555e-01 (-9.3703e-01)\n",
            "Epoch: [670][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3935e-01 (-9.3743e-01)\n",
            "Epoch: [670][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3435e-01 (-9.3726e-01)\n",
            "Epoch: [670][90/97]\tTime  0.176 ( 0.180)\tLoss -9.2815e-01 (-9.3702e-01)\n",
            "Validating...\n",
            "Top1: 0.8669819078947368\n",
            "Training...\n",
            "Epoch: [671][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.3942e-01 (-9.3942e-01)\n",
            "Epoch: [671][10/97]\tTime  0.177 ( 0.201)\tLoss -9.2982e-01 (-9.3749e-01)\n",
            "Epoch: [671][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3224e-01 (-9.3628e-01)\n",
            "Epoch: [671][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3080e-01 (-9.3655e-01)\n",
            "Epoch: [671][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3829e-01 (-9.3629e-01)\n",
            "Epoch: [671][50/97]\tTime  0.178 ( 0.182)\tLoss -9.3917e-01 (-9.3668e-01)\n",
            "Epoch: [671][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4097e-01 (-9.3701e-01)\n",
            "Epoch: [671][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4342e-01 (-9.3669e-01)\n",
            "Epoch: [671][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3021e-01 (-9.3666e-01)\n",
            "Epoch: [671][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3912e-01 (-9.3658e-01)\n",
            "Training...\n",
            "Epoch: [672][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.4074e-01 (-9.4074e-01)\n",
            "Epoch: [672][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2934e-01 (-9.3741e-01)\n",
            "Epoch: [672][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4723e-01 (-9.3877e-01)\n",
            "Epoch: [672][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3359e-01 (-9.3863e-01)\n",
            "Epoch: [672][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3589e-01 (-9.3732e-01)\n",
            "Epoch: [672][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3488e-01 (-9.3745e-01)\n",
            "Epoch: [672][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4054e-01 (-9.3784e-01)\n",
            "Epoch: [672][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3859e-01 (-9.3813e-01)\n",
            "Epoch: [672][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2755e-01 (-9.3786e-01)\n",
            "Epoch: [672][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3717e-01 (-9.3767e-01)\n",
            "Training...\n",
            "Epoch: [673][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.4031e-01 (-9.4031e-01)\n",
            "Epoch: [673][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3858e-01 (-9.3909e-01)\n",
            "Epoch: [673][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4436e-01 (-9.3886e-01)\n",
            "Epoch: [673][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4509e-01 (-9.3868e-01)\n",
            "Epoch: [673][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4053e-01 (-9.3855e-01)\n",
            "Epoch: [673][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4162e-01 (-9.3885e-01)\n",
            "Epoch: [673][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3814e-01 (-9.3821e-01)\n",
            "Epoch: [673][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3782e-01 (-9.3809e-01)\n",
            "Epoch: [673][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2996e-01 (-9.3763e-01)\n",
            "Epoch: [673][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4198e-01 (-9.3761e-01)\n",
            "Training...\n",
            "Epoch: [674][ 0/97]\tTime  0.462 ( 0.462)\tLoss -9.4042e-01 (-9.4042e-01)\n",
            "Epoch: [674][10/97]\tTime  0.177 ( 0.203)\tLoss -9.3050e-01 (-9.3584e-01)\n",
            "Epoch: [674][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4350e-01 (-9.3755e-01)\n",
            "Epoch: [674][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3323e-01 (-9.3775e-01)\n",
            "Epoch: [674][40/97]\tTime  0.176 ( 0.184)\tLoss -9.2709e-01 (-9.3747e-01)\n",
            "Epoch: [674][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2463e-01 (-9.3715e-01)\n",
            "Epoch: [674][60/97]\tTime  0.176 ( 0.182)\tLoss -9.4257e-01 (-9.3751e-01)\n",
            "Epoch: [674][70/97]\tTime  0.176 ( 0.181)\tLoss -9.4124e-01 (-9.3762e-01)\n",
            "Epoch: [674][80/97]\tTime  0.176 ( 0.180)\tLoss -9.3501e-01 (-9.3751e-01)\n",
            "Epoch: [674][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3886e-01 (-9.3803e-01)\n",
            "Training...\n",
            "Epoch: [675][ 0/97]\tTime  0.464 ( 0.464)\tLoss -9.3961e-01 (-9.3961e-01)\n",
            "Epoch: [675][10/97]\tTime  0.177 ( 0.206)\tLoss -9.4039e-01 (-9.3719e-01)\n",
            "Epoch: [675][20/97]\tTime  0.178 ( 0.192)\tLoss -9.3312e-01 (-9.3820e-01)\n",
            "Epoch: [675][30/97]\tTime  0.177 ( 0.188)\tLoss -9.4534e-01 (-9.3842e-01)\n",
            "Epoch: [675][40/97]\tTime  0.177 ( 0.185)\tLoss -9.3235e-01 (-9.3777e-01)\n",
            "Epoch: [675][50/97]\tTime  0.177 ( 0.184)\tLoss -9.3619e-01 (-9.3770e-01)\n",
            "Epoch: [675][60/97]\tTime  0.177 ( 0.183)\tLoss -9.2735e-01 (-9.3760e-01)\n",
            "Epoch: [675][70/97]\tTime  0.177 ( 0.182)\tLoss -9.4762e-01 (-9.3803e-01)\n",
            "Epoch: [675][80/97]\tTime  0.177 ( 0.181)\tLoss -9.3992e-01 (-9.3784e-01)\n",
            "Epoch: [675][90/97]\tTime  0.177 ( 0.181)\tLoss -9.3584e-01 (-9.3761e-01)\n",
            "Validating...\n",
            "Top1: 0.8670847039473685\n",
            "Training...\n",
            "Epoch: [676][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.3288e-01 (-9.3288e-01)\n",
            "Epoch: [676][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4699e-01 (-9.4327e-01)\n",
            "Epoch: [676][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3804e-01 (-9.4063e-01)\n",
            "Epoch: [676][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3932e-01 (-9.4022e-01)\n",
            "Epoch: [676][40/97]\tTime  0.178 ( 0.184)\tLoss -9.3855e-01 (-9.3996e-01)\n",
            "Epoch: [676][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3496e-01 (-9.3938e-01)\n",
            "Epoch: [676][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4759e-01 (-9.3985e-01)\n",
            "Epoch: [676][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4131e-01 (-9.3997e-01)\n",
            "Epoch: [676][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4710e-01 (-9.4008e-01)\n",
            "Epoch: [676][90/97]\tTime  0.176 ( 0.180)\tLoss -9.3811e-01 (-9.3955e-01)\n",
            "Training...\n",
            "Epoch: [677][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.3108e-01 (-9.3108e-01)\n",
            "Epoch: [677][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3899e-01 (-9.3586e-01)\n",
            "Epoch: [677][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4528e-01 (-9.3802e-01)\n",
            "Epoch: [677][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4420e-01 (-9.3848e-01)\n",
            "Epoch: [677][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3790e-01 (-9.3833e-01)\n",
            "Epoch: [677][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3654e-01 (-9.3846e-01)\n",
            "Epoch: [677][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4181e-01 (-9.3869e-01)\n",
            "Epoch: [677][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4810e-01 (-9.3877e-01)\n",
            "Epoch: [677][80/97]\tTime  0.176 ( 0.180)\tLoss -9.3774e-01 (-9.3895e-01)\n",
            "Epoch: [677][90/97]\tTime  0.176 ( 0.180)\tLoss -9.4308e-01 (-9.3894e-01)\n",
            "Training...\n",
            "Epoch: [678][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.4174e-01 (-9.4174e-01)\n",
            "Epoch: [678][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3451e-01 (-9.3951e-01)\n",
            "Epoch: [678][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2514e-01 (-9.3890e-01)\n",
            "Epoch: [678][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4389e-01 (-9.4004e-01)\n",
            "Epoch: [678][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3135e-01 (-9.3963e-01)\n",
            "Epoch: [678][50/97]\tTime  0.177 ( 0.183)\tLoss -9.3346e-01 (-9.3998e-01)\n",
            "Epoch: [678][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4232e-01 (-9.3983e-01)\n",
            "Epoch: [678][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4998e-01 (-9.3971e-01)\n",
            "Epoch: [678][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4050e-01 (-9.3944e-01)\n",
            "Epoch: [678][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4443e-01 (-9.3961e-01)\n",
            "Training...\n",
            "Epoch: [679][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.3867e-01 (-9.3867e-01)\n",
            "Epoch: [679][10/97]\tTime  0.177 ( 0.202)\tLoss -9.2826e-01 (-9.3965e-01)\n",
            "Epoch: [679][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4219e-01 (-9.3999e-01)\n",
            "Epoch: [679][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4057e-01 (-9.4052e-01)\n",
            "Epoch: [679][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4127e-01 (-9.4079e-01)\n",
            "Epoch: [679][50/97]\tTime  0.178 ( 0.183)\tLoss -9.3702e-01 (-9.4045e-01)\n",
            "Epoch: [679][60/97]\tTime  0.177 ( 0.182)\tLoss -9.2723e-01 (-9.3988e-01)\n",
            "Epoch: [679][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3782e-01 (-9.3954e-01)\n",
            "Epoch: [679][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4515e-01 (-9.3936e-01)\n",
            "Epoch: [679][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4412e-01 (-9.3917e-01)\n",
            "Training...\n",
            "Epoch: [680][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.4806e-01 (-9.4806e-01)\n",
            "Epoch: [680][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3014e-01 (-9.4081e-01)\n",
            "Epoch: [680][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4070e-01 (-9.3971e-01)\n",
            "Epoch: [680][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4366e-01 (-9.4077e-01)\n",
            "Epoch: [680][40/97]\tTime  0.176 ( 0.184)\tLoss -9.4274e-01 (-9.4003e-01)\n",
            "Epoch: [680][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3557e-01 (-9.3955e-01)\n",
            "Epoch: [680][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3923e-01 (-9.3966e-01)\n",
            "Epoch: [680][70/97]\tTime  0.178 ( 0.181)\tLoss -9.3653e-01 (-9.3951e-01)\n",
            "Epoch: [680][80/97]\tTime  0.177 ( 0.180)\tLoss -9.2955e-01 (-9.3969e-01)\n",
            "Epoch: [680][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4327e-01 (-9.3955e-01)\n",
            "Validating...\n",
            "Top1: 0.8683182565789473\n",
            "Training...\n",
            "Epoch: [681][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.4594e-01 (-9.4594e-01)\n",
            "Epoch: [681][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4935e-01 (-9.3982e-01)\n",
            "Epoch: [681][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3750e-01 (-9.4152e-01)\n",
            "Epoch: [681][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3647e-01 (-9.4063e-01)\n",
            "Epoch: [681][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3844e-01 (-9.4024e-01)\n",
            "Epoch: [681][50/97]\tTime  0.178 ( 0.182)\tLoss -9.4536e-01 (-9.3962e-01)\n",
            "Epoch: [681][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3978e-01 (-9.3982e-01)\n",
            "Epoch: [681][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4616e-01 (-9.4002e-01)\n",
            "Epoch: [681][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3716e-01 (-9.3985e-01)\n",
            "Epoch: [681][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4455e-01 (-9.3979e-01)\n",
            "Training...\n",
            "Epoch: [682][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.3362e-01 (-9.3362e-01)\n",
            "Epoch: [682][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4884e-01 (-9.3755e-01)\n",
            "Epoch: [682][20/97]\tTime  0.176 ( 0.190)\tLoss -9.3683e-01 (-9.3859e-01)\n",
            "Epoch: [682][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4606e-01 (-9.3984e-01)\n",
            "Epoch: [682][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4573e-01 (-9.3983e-01)\n",
            "Epoch: [682][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4840e-01 (-9.3992e-01)\n",
            "Epoch: [682][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4650e-01 (-9.3998e-01)\n",
            "Epoch: [682][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3108e-01 (-9.3988e-01)\n",
            "Epoch: [682][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3831e-01 (-9.3934e-01)\n",
            "Epoch: [682][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4456e-01 (-9.3964e-01)\n",
            "Training...\n",
            "Epoch: [683][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.4904e-01 (-9.4904e-01)\n",
            "Epoch: [683][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3262e-01 (-9.4286e-01)\n",
            "Epoch: [683][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5238e-01 (-9.4231e-01)\n",
            "Epoch: [683][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4913e-01 (-9.4170e-01)\n",
            "Epoch: [683][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3813e-01 (-9.4052e-01)\n",
            "Epoch: [683][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4017e-01 (-9.3993e-01)\n",
            "Epoch: [683][60/97]\tTime  0.178 ( 0.181)\tLoss -9.3913e-01 (-9.3994e-01)\n",
            "Epoch: [683][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4218e-01 (-9.3984e-01)\n",
            "Epoch: [683][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4301e-01 (-9.3998e-01)\n",
            "Epoch: [683][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3989e-01 (-9.4024e-01)\n",
            "Training...\n",
            "Epoch: [684][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.3821e-01 (-9.3821e-01)\n",
            "Epoch: [684][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4689e-01 (-9.4223e-01)\n",
            "Epoch: [684][20/97]\tTime  0.177 ( 0.190)\tLoss -9.2970e-01 (-9.4055e-01)\n",
            "Epoch: [684][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4777e-01 (-9.4101e-01)\n",
            "Epoch: [684][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4038e-01 (-9.4093e-01)\n",
            "Epoch: [684][50/97]\tTime  0.176 ( 0.182)\tLoss -9.3440e-01 (-9.4058e-01)\n",
            "Epoch: [684][60/97]\tTime  0.176 ( 0.181)\tLoss -9.3962e-01 (-9.4037e-01)\n",
            "Epoch: [684][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4237e-01 (-9.4078e-01)\n",
            "Epoch: [684][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3864e-01 (-9.4073e-01)\n",
            "Epoch: [684][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4423e-01 (-9.4069e-01)\n",
            "Training...\n",
            "Epoch: [685][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.3879e-01 (-9.3879e-01)\n",
            "Epoch: [685][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4835e-01 (-9.4199e-01)\n",
            "Epoch: [685][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4403e-01 (-9.4142e-01)\n",
            "Epoch: [685][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4211e-01 (-9.4217e-01)\n",
            "Epoch: [685][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4162e-01 (-9.4152e-01)\n",
            "Epoch: [685][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3868e-01 (-9.4101e-01)\n",
            "Epoch: [685][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3329e-01 (-9.4079e-01)\n",
            "Epoch: [685][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3741e-01 (-9.4018e-01)\n",
            "Epoch: [685][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3475e-01 (-9.4021e-01)\n",
            "Epoch: [685][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4048e-01 (-9.4005e-01)\n",
            "Validating...\n",
            "Top1: 0.8702713815789473\n",
            "Training...\n",
            "Epoch: [686][ 0/97]\tTime  0.466 ( 0.466)\tLoss -9.4363e-01 (-9.4363e-01)\n",
            "Epoch: [686][10/97]\tTime  0.178 ( 0.203)\tLoss -9.3691e-01 (-9.3942e-01)\n",
            "Epoch: [686][20/97]\tTime  0.177 ( 0.191)\tLoss -9.4538e-01 (-9.3838e-01)\n",
            "Epoch: [686][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3913e-01 (-9.3986e-01)\n",
            "Epoch: [686][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4312e-01 (-9.4064e-01)\n",
            "Epoch: [686][50/97]\tTime  0.178 ( 0.183)\tLoss -9.4302e-01 (-9.4097e-01)\n",
            "Epoch: [686][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3901e-01 (-9.4096e-01)\n",
            "Epoch: [686][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4361e-01 (-9.4093e-01)\n",
            "Epoch: [686][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4486e-01 (-9.4101e-01)\n",
            "Epoch: [686][90/97]\tTime  0.177 ( 0.180)\tLoss -9.2810e-01 (-9.4081e-01)\n",
            "Training...\n",
            "Epoch: [687][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.5221e-01 (-9.5221e-01)\n",
            "Epoch: [687][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3510e-01 (-9.3934e-01)\n",
            "Epoch: [687][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4519e-01 (-9.4054e-01)\n",
            "Epoch: [687][30/97]\tTime  0.178 ( 0.186)\tLoss -9.3758e-01 (-9.4106e-01)\n",
            "Epoch: [687][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3121e-01 (-9.4121e-01)\n",
            "Epoch: [687][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3523e-01 (-9.4098e-01)\n",
            "Epoch: [687][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3839e-01 (-9.4119e-01)\n",
            "Epoch: [687][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3957e-01 (-9.4090e-01)\n",
            "Epoch: [687][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4543e-01 (-9.4065e-01)\n",
            "Epoch: [687][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3089e-01 (-9.4039e-01)\n",
            "Training...\n",
            "Epoch: [688][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.4622e-01 (-9.4622e-01)\n",
            "Epoch: [688][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4590e-01 (-9.3862e-01)\n",
            "Epoch: [688][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4185e-01 (-9.4001e-01)\n",
            "Epoch: [688][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3577e-01 (-9.4079e-01)\n",
            "Epoch: [688][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3004e-01 (-9.3983e-01)\n",
            "Epoch: [688][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2618e-01 (-9.3973e-01)\n",
            "Epoch: [688][60/97]\tTime  0.176 ( 0.181)\tLoss -9.3468e-01 (-9.3959e-01)\n",
            "Epoch: [688][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4173e-01 (-9.3982e-01)\n",
            "Epoch: [688][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5167e-01 (-9.4031e-01)\n",
            "Epoch: [688][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4699e-01 (-9.4057e-01)\n",
            "Training...\n",
            "Epoch: [689][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.4184e-01 (-9.4184e-01)\n",
            "Epoch: [689][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4072e-01 (-9.4254e-01)\n",
            "Epoch: [689][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3401e-01 (-9.4133e-01)\n",
            "Epoch: [689][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4478e-01 (-9.4167e-01)\n",
            "Epoch: [689][40/97]\tTime  0.176 ( 0.184)\tLoss -9.3918e-01 (-9.4085e-01)\n",
            "Epoch: [689][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4240e-01 (-9.4043e-01)\n",
            "Epoch: [689][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4010e-01 (-9.4084e-01)\n",
            "Epoch: [689][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3436e-01 (-9.4090e-01)\n",
            "Epoch: [689][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3486e-01 (-9.4056e-01)\n",
            "Epoch: [689][90/97]\tTime  0.176 ( 0.180)\tLoss -9.4422e-01 (-9.4053e-01)\n",
            "Training...\n",
            "Epoch: [690][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.4346e-01 (-9.4346e-01)\n",
            "Epoch: [690][10/97]\tTime  0.176 ( 0.202)\tLoss -9.5524e-01 (-9.4360e-01)\n",
            "Epoch: [690][20/97]\tTime  0.176 ( 0.190)\tLoss -9.3705e-01 (-9.4181e-01)\n",
            "Epoch: [690][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3403e-01 (-9.4047e-01)\n",
            "Epoch: [690][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3999e-01 (-9.4011e-01)\n",
            "Epoch: [690][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4842e-01 (-9.4033e-01)\n",
            "Epoch: [690][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3774e-01 (-9.4064e-01)\n",
            "Epoch: [690][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3769e-01 (-9.4053e-01)\n",
            "Epoch: [690][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4821e-01 (-9.4071e-01)\n",
            "Epoch: [690][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4104e-01 (-9.4067e-01)\n",
            "Validating...\n",
            "Top1: 0.8709909539473685\n",
            "Training...\n",
            "Epoch: [691][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.3790e-01 (-9.3790e-01)\n",
            "Epoch: [691][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4274e-01 (-9.4220e-01)\n",
            "Epoch: [691][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4028e-01 (-9.4239e-01)\n",
            "Epoch: [691][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4151e-01 (-9.4176e-01)\n",
            "Epoch: [691][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4299e-01 (-9.4270e-01)\n",
            "Epoch: [691][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3770e-01 (-9.4269e-01)\n",
            "Epoch: [691][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4137e-01 (-9.4200e-01)\n",
            "Epoch: [691][70/97]\tTime  0.176 ( 0.181)\tLoss -9.4846e-01 (-9.4229e-01)\n",
            "Epoch: [691][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4003e-01 (-9.4222e-01)\n",
            "Epoch: [691][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4204e-01 (-9.4192e-01)\n",
            "Training...\n",
            "Epoch: [692][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.4463e-01 (-9.4463e-01)\n",
            "Epoch: [692][10/97]\tTime  0.177 ( 0.201)\tLoss -9.4677e-01 (-9.4294e-01)\n",
            "Epoch: [692][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4610e-01 (-9.4278e-01)\n",
            "Epoch: [692][30/97]\tTime  0.177 ( 0.185)\tLoss -9.3860e-01 (-9.4194e-01)\n",
            "Epoch: [692][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4560e-01 (-9.4220e-01)\n",
            "Epoch: [692][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4100e-01 (-9.4203e-01)\n",
            "Epoch: [692][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4416e-01 (-9.4208e-01)\n",
            "Epoch: [692][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4208e-01 (-9.4241e-01)\n",
            "Epoch: [692][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3887e-01 (-9.4234e-01)\n",
            "Epoch: [692][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4282e-01 (-9.4211e-01)\n",
            "Training...\n",
            "Epoch: [693][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.3724e-01 (-9.3724e-01)\n",
            "Epoch: [693][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3974e-01 (-9.4166e-01)\n",
            "Epoch: [693][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4212e-01 (-9.4062e-01)\n",
            "Epoch: [693][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4321e-01 (-9.4228e-01)\n",
            "Epoch: [693][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3847e-01 (-9.4201e-01)\n",
            "Epoch: [693][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3644e-01 (-9.4153e-01)\n",
            "Epoch: [693][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4083e-01 (-9.4125e-01)\n",
            "Epoch: [693][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4237e-01 (-9.4093e-01)\n",
            "Epoch: [693][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4409e-01 (-9.4099e-01)\n",
            "Epoch: [693][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4105e-01 (-9.4105e-01)\n",
            "Training...\n",
            "Epoch: [694][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.4150e-01 (-9.4150e-01)\n",
            "Epoch: [694][10/97]\tTime  0.178 ( 0.202)\tLoss -9.4756e-01 (-9.4168e-01)\n",
            "Epoch: [694][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3634e-01 (-9.4137e-01)\n",
            "Epoch: [694][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4688e-01 (-9.4127e-01)\n",
            "Epoch: [694][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3993e-01 (-9.4126e-01)\n",
            "Epoch: [694][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3728e-01 (-9.4142e-01)\n",
            "Epoch: [694][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4539e-01 (-9.4186e-01)\n",
            "Epoch: [694][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5097e-01 (-9.4176e-01)\n",
            "Epoch: [694][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4043e-01 (-9.4178e-01)\n",
            "Epoch: [694][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4229e-01 (-9.4189e-01)\n",
            "Training...\n",
            "Epoch: [695][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.4308e-01 (-9.4308e-01)\n",
            "Epoch: [695][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5349e-01 (-9.4444e-01)\n",
            "Epoch: [695][20/97]\tTime  0.176 ( 0.190)\tLoss -9.4880e-01 (-9.4428e-01)\n",
            "Epoch: [695][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4261e-01 (-9.4355e-01)\n",
            "Epoch: [695][40/97]\tTime  0.176 ( 0.183)\tLoss -9.3652e-01 (-9.4253e-01)\n",
            "Epoch: [695][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4728e-01 (-9.4245e-01)\n",
            "Epoch: [695][60/97]\tTime  0.176 ( 0.181)\tLoss -9.4978e-01 (-9.4290e-01)\n",
            "Epoch: [695][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4857e-01 (-9.4292e-01)\n",
            "Epoch: [695][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3473e-01 (-9.4282e-01)\n",
            "Epoch: [695][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3008e-01 (-9.4261e-01)\n",
            "Validating...\n",
            "Top1: 0.8674958881578947\n",
            "Training...\n",
            "Epoch: [696][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.4596e-01 (-9.4596e-01)\n",
            "Epoch: [696][10/97]\tTime  0.176 ( 0.202)\tLoss -9.3978e-01 (-9.4131e-01)\n",
            "Epoch: [696][20/97]\tTime  0.176 ( 0.190)\tLoss -9.4102e-01 (-9.4136e-01)\n",
            "Epoch: [696][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3477e-01 (-9.4179e-01)\n",
            "Epoch: [696][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4707e-01 (-9.4209e-01)\n",
            "Epoch: [696][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5076e-01 (-9.4239e-01)\n",
            "Epoch: [696][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4386e-01 (-9.4195e-01)\n",
            "Epoch: [696][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4315e-01 (-9.4193e-01)\n",
            "Epoch: [696][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3733e-01 (-9.4207e-01)\n",
            "Epoch: [696][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5031e-01 (-9.4199e-01)\n",
            "Training...\n",
            "Epoch: [697][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.4862e-01 (-9.4862e-01)\n",
            "Epoch: [697][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4153e-01 (-9.4356e-01)\n",
            "Epoch: [697][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4150e-01 (-9.4313e-01)\n",
            "Epoch: [697][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3852e-01 (-9.4368e-01)\n",
            "Epoch: [697][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3924e-01 (-9.4334e-01)\n",
            "Epoch: [697][50/97]\tTime  0.177 ( 0.183)\tLoss -9.3851e-01 (-9.4288e-01)\n",
            "Epoch: [697][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4459e-01 (-9.4272e-01)\n",
            "Epoch: [697][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3725e-01 (-9.4254e-01)\n",
            "Epoch: [697][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4145e-01 (-9.4261e-01)\n",
            "Epoch: [697][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4390e-01 (-9.4279e-01)\n",
            "Training...\n",
            "Epoch: [698][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.3733e-01 (-9.3733e-01)\n",
            "Epoch: [698][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4177e-01 (-9.4263e-01)\n",
            "Epoch: [698][20/97]\tTime  0.176 ( 0.190)\tLoss -9.3798e-01 (-9.4179e-01)\n",
            "Epoch: [698][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4236e-01 (-9.4159e-01)\n",
            "Epoch: [698][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4556e-01 (-9.4192e-01)\n",
            "Epoch: [698][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4433e-01 (-9.4229e-01)\n",
            "Epoch: [698][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4813e-01 (-9.4258e-01)\n",
            "Epoch: [698][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5104e-01 (-9.4255e-01)\n",
            "Epoch: [698][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3328e-01 (-9.4200e-01)\n",
            "Epoch: [698][90/97]\tTime  0.176 ( 0.180)\tLoss -9.4691e-01 (-9.4219e-01)\n",
            "Training...\n",
            "Epoch: [699][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.3958e-01 (-9.3958e-01)\n",
            "Epoch: [699][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5298e-01 (-9.4203e-01)\n",
            "Epoch: [699][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4258e-01 (-9.4314e-01)\n",
            "Epoch: [699][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4226e-01 (-9.4317e-01)\n",
            "Epoch: [699][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4095e-01 (-9.4357e-01)\n",
            "Epoch: [699][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5148e-01 (-9.4405e-01)\n",
            "Epoch: [699][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4302e-01 (-9.4387e-01)\n",
            "Epoch: [699][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5257e-01 (-9.4387e-01)\n",
            "Epoch: [699][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4250e-01 (-9.4370e-01)\n",
            "Epoch: [699][90/97]\tTime  0.178 ( 0.180)\tLoss -9.3983e-01 (-9.4359e-01)\n",
            "Training...\n",
            "Epoch: [700][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.4072e-01 (-9.4072e-01)\n",
            "Epoch: [700][10/97]\tTime  0.177 ( 0.201)\tLoss -9.3375e-01 (-9.4239e-01)\n",
            "Epoch: [700][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4610e-01 (-9.4307e-01)\n",
            "Epoch: [700][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4684e-01 (-9.4397e-01)\n",
            "Epoch: [700][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4892e-01 (-9.4401e-01)\n",
            "Epoch: [700][50/97]\tTime  0.177 ( 0.182)\tLoss -9.2889e-01 (-9.4354e-01)\n",
            "Epoch: [700][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4641e-01 (-9.4346e-01)\n",
            "Epoch: [700][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4500e-01 (-9.4305e-01)\n",
            "Epoch: [700][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5443e-01 (-9.4329e-01)\n",
            "Epoch: [700][90/97]\tTime  0.176 ( 0.180)\tLoss -9.5189e-01 (-9.4306e-01)\n",
            "Validating...\n",
            "Top1: 0.87109375\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [701][ 0/97]\tTime  0.440 ( 0.440)\tLoss -9.5178e-01 (-9.5178e-01)\n",
            "Epoch: [701][10/97]\tTime  0.177 ( 0.201)\tLoss -9.4785e-01 (-9.4345e-01)\n",
            "Epoch: [701][20/97]\tTime  0.177 ( 0.189)\tLoss -9.3577e-01 (-9.4271e-01)\n",
            "Epoch: [701][30/97]\tTime  0.177 ( 0.185)\tLoss -9.5096e-01 (-9.4297e-01)\n",
            "Epoch: [701][40/97]\tTime  0.177 ( 0.183)\tLoss -9.3601e-01 (-9.4281e-01)\n",
            "Epoch: [701][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3849e-01 (-9.4292e-01)\n",
            "Epoch: [701][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4164e-01 (-9.4315e-01)\n",
            "Epoch: [701][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4099e-01 (-9.4329e-01)\n",
            "Epoch: [701][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4003e-01 (-9.4296e-01)\n",
            "Epoch: [701][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4133e-01 (-9.4298e-01)\n",
            "Training...\n",
            "Epoch: [702][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.3337e-01 (-9.3337e-01)\n",
            "Epoch: [702][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4627e-01 (-9.4049e-01)\n",
            "Epoch: [702][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3798e-01 (-9.4250e-01)\n",
            "Epoch: [702][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4956e-01 (-9.4326e-01)\n",
            "Epoch: [702][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4706e-01 (-9.4348e-01)\n",
            "Epoch: [702][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4082e-01 (-9.4325e-01)\n",
            "Epoch: [702][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4275e-01 (-9.4306e-01)\n",
            "Epoch: [702][70/97]\tTime  0.176 ( 0.181)\tLoss -9.4819e-01 (-9.4346e-01)\n",
            "Epoch: [702][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4956e-01 (-9.4347e-01)\n",
            "Epoch: [702][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4144e-01 (-9.4384e-01)\n",
            "Training...\n",
            "Epoch: [703][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.4099e-01 (-9.4099e-01)\n",
            "Epoch: [703][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4193e-01 (-9.4272e-01)\n",
            "Epoch: [703][20/97]\tTime  0.178 ( 0.190)\tLoss -9.3821e-01 (-9.4266e-01)\n",
            "Epoch: [703][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4798e-01 (-9.4281e-01)\n",
            "Epoch: [703][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4881e-01 (-9.4386e-01)\n",
            "Epoch: [703][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4421e-01 (-9.4350e-01)\n",
            "Epoch: [703][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4931e-01 (-9.4311e-01)\n",
            "Epoch: [703][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3774e-01 (-9.4276e-01)\n",
            "Epoch: [703][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5055e-01 (-9.4282e-01)\n",
            "Epoch: [703][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3744e-01 (-9.4289e-01)\n",
            "Training...\n",
            "Epoch: [704][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.4433e-01 (-9.4433e-01)\n",
            "Epoch: [704][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4203e-01 (-9.4589e-01)\n",
            "Epoch: [704][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4629e-01 (-9.4504e-01)\n",
            "Epoch: [704][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4831e-01 (-9.4523e-01)\n",
            "Epoch: [704][40/97]\tTime  0.178 ( 0.183)\tLoss -9.5190e-01 (-9.4559e-01)\n",
            "Epoch: [704][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4724e-01 (-9.4526e-01)\n",
            "Epoch: [704][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4775e-01 (-9.4545e-01)\n",
            "Epoch: [704][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4481e-01 (-9.4490e-01)\n",
            "Epoch: [704][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3612e-01 (-9.4447e-01)\n",
            "Epoch: [704][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4701e-01 (-9.4409e-01)\n",
            "Training...\n",
            "Epoch: [705][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.4869e-01 (-9.4869e-01)\n",
            "Epoch: [705][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3722e-01 (-9.4315e-01)\n",
            "Epoch: [705][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4216e-01 (-9.4334e-01)\n",
            "Epoch: [705][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3763e-01 (-9.4363e-01)\n",
            "Epoch: [705][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4547e-01 (-9.4291e-01)\n",
            "Epoch: [705][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3841e-01 (-9.4258e-01)\n",
            "Epoch: [705][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5188e-01 (-9.4307e-01)\n",
            "Epoch: [705][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3682e-01 (-9.4323e-01)\n",
            "Epoch: [705][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4033e-01 (-9.4363e-01)\n",
            "Epoch: [705][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4046e-01 (-9.4345e-01)\n",
            "Validating...\n",
            "Top1: 0.8692434210526315\n",
            "Training...\n",
            "Epoch: [706][ 0/97]\tTime  0.463 ( 0.463)\tLoss -9.4093e-01 (-9.4093e-01)\n",
            "Epoch: [706][10/97]\tTime  0.177 ( 0.203)\tLoss -9.4756e-01 (-9.4276e-01)\n",
            "Epoch: [706][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4150e-01 (-9.4207e-01)\n",
            "Epoch: [706][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4670e-01 (-9.4324e-01)\n",
            "Epoch: [706][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3661e-01 (-9.4395e-01)\n",
            "Epoch: [706][50/97]\tTime  0.178 ( 0.182)\tLoss -9.5369e-01 (-9.4403e-01)\n",
            "Epoch: [706][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4417e-01 (-9.4424e-01)\n",
            "Epoch: [706][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4589e-01 (-9.4433e-01)\n",
            "Epoch: [706][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4187e-01 (-9.4432e-01)\n",
            "Epoch: [706][90/97]\tTime  0.176 ( 0.180)\tLoss -9.4584e-01 (-9.4423e-01)\n",
            "Training...\n",
            "Epoch: [707][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.4299e-01 (-9.4299e-01)\n",
            "Epoch: [707][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4023e-01 (-9.4651e-01)\n",
            "Epoch: [707][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4210e-01 (-9.4544e-01)\n",
            "Epoch: [707][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4311e-01 (-9.4527e-01)\n",
            "Epoch: [707][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4543e-01 (-9.4489e-01)\n",
            "Epoch: [707][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4585e-01 (-9.4478e-01)\n",
            "Epoch: [707][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5131e-01 (-9.4545e-01)\n",
            "Epoch: [707][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5385e-01 (-9.4533e-01)\n",
            "Epoch: [707][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4435e-01 (-9.4521e-01)\n",
            "Epoch: [707][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3745e-01 (-9.4502e-01)\n",
            "Training...\n",
            "Epoch: [708][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.4835e-01 (-9.4835e-01)\n",
            "Epoch: [708][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4067e-01 (-9.4354e-01)\n",
            "Epoch: [708][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5197e-01 (-9.4352e-01)\n",
            "Epoch: [708][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4690e-01 (-9.4356e-01)\n",
            "Epoch: [708][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3973e-01 (-9.4459e-01)\n",
            "Epoch: [708][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4295e-01 (-9.4463e-01)\n",
            "Epoch: [708][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3891e-01 (-9.4482e-01)\n",
            "Epoch: [708][70/97]\tTime  0.176 ( 0.181)\tLoss -9.4063e-01 (-9.4501e-01)\n",
            "Epoch: [708][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4275e-01 (-9.4495e-01)\n",
            "Epoch: [708][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4784e-01 (-9.4507e-01)\n",
            "Training...\n",
            "Epoch: [709][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.5286e-01 (-9.5286e-01)\n",
            "Epoch: [709][10/97]\tTime  0.176 ( 0.202)\tLoss -9.4005e-01 (-9.4366e-01)\n",
            "Epoch: [709][20/97]\tTime  0.176 ( 0.190)\tLoss -9.4015e-01 (-9.4293e-01)\n",
            "Epoch: [709][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4608e-01 (-9.4396e-01)\n",
            "Epoch: [709][40/97]\tTime  0.177 ( 0.184)\tLoss -9.3917e-01 (-9.4379e-01)\n",
            "Epoch: [709][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4597e-01 (-9.4356e-01)\n",
            "Epoch: [709][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4660e-01 (-9.4381e-01)\n",
            "Epoch: [709][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4003e-01 (-9.4373e-01)\n",
            "Epoch: [709][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4082e-01 (-9.4378e-01)\n",
            "Epoch: [709][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4893e-01 (-9.4402e-01)\n",
            "Training...\n",
            "Epoch: [710][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.3925e-01 (-9.3925e-01)\n",
            "Epoch: [710][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4985e-01 (-9.4423e-01)\n",
            "Epoch: [710][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3737e-01 (-9.4477e-01)\n",
            "Epoch: [710][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4274e-01 (-9.4506e-01)\n",
            "Epoch: [710][40/97]\tTime  0.178 ( 0.184)\tLoss -9.4452e-01 (-9.4473e-01)\n",
            "Epoch: [710][50/97]\tTime  0.177 ( 0.182)\tLoss -9.3981e-01 (-9.4465e-01)\n",
            "Epoch: [710][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4455e-01 (-9.4469e-01)\n",
            "Epoch: [710][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3959e-01 (-9.4479e-01)\n",
            "Epoch: [710][80/97]\tTime  0.176 ( 0.180)\tLoss -9.4849e-01 (-9.4470e-01)\n",
            "Epoch: [710][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4400e-01 (-9.4455e-01)\n",
            "Validating...\n",
            "Top1: 0.8668791118421053\n",
            "Training...\n",
            "Epoch: [711][ 0/97]\tTime  0.460 ( 0.460)\tLoss -9.4278e-01 (-9.4278e-01)\n",
            "Epoch: [711][10/97]\tTime  0.177 ( 0.203)\tLoss -9.4729e-01 (-9.4821e-01)\n",
            "Epoch: [711][20/97]\tTime  0.176 ( 0.190)\tLoss -9.4285e-01 (-9.4690e-01)\n",
            "Epoch: [711][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3540e-01 (-9.4613e-01)\n",
            "Epoch: [711][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4772e-01 (-9.4563e-01)\n",
            "Epoch: [711][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5269e-01 (-9.4554e-01)\n",
            "Epoch: [711][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5206e-01 (-9.4602e-01)\n",
            "Epoch: [711][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3821e-01 (-9.4572e-01)\n",
            "Epoch: [711][80/97]\tTime  0.177 ( 0.180)\tLoss -9.3639e-01 (-9.4542e-01)\n",
            "Epoch: [711][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4665e-01 (-9.4567e-01)\n",
            "Training...\n",
            "Epoch: [712][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.4454e-01 (-9.4454e-01)\n",
            "Epoch: [712][10/97]\tTime  0.177 ( 0.202)\tLoss -9.3984e-01 (-9.4524e-01)\n",
            "Epoch: [712][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4903e-01 (-9.4673e-01)\n",
            "Epoch: [712][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3926e-01 (-9.4570e-01)\n",
            "Epoch: [712][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4561e-01 (-9.4554e-01)\n",
            "Epoch: [712][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5386e-01 (-9.4554e-01)\n",
            "Epoch: [712][60/97]\tTime  0.177 ( 0.181)\tLoss -9.3898e-01 (-9.4547e-01)\n",
            "Epoch: [712][70/97]\tTime  0.176 ( 0.181)\tLoss -9.5468e-01 (-9.4548e-01)\n",
            "Epoch: [712][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4305e-01 (-9.4551e-01)\n",
            "Epoch: [712][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4510e-01 (-9.4518e-01)\n",
            "Training...\n",
            "Epoch: [713][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.4161e-01 (-9.4161e-01)\n",
            "Epoch: [713][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4540e-01 (-9.4437e-01)\n",
            "Epoch: [713][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4409e-01 (-9.4531e-01)\n",
            "Epoch: [713][30/97]\tTime  0.176 ( 0.186)\tLoss -9.4297e-01 (-9.4608e-01)\n",
            "Epoch: [713][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4793e-01 (-9.4593e-01)\n",
            "Epoch: [713][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5487e-01 (-9.4607e-01)\n",
            "Epoch: [713][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4957e-01 (-9.4555e-01)\n",
            "Epoch: [713][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2929e-01 (-9.4533e-01)\n",
            "Epoch: [713][80/97]\tTime  0.178 ( 0.180)\tLoss -9.5434e-01 (-9.4524e-01)\n",
            "Epoch: [713][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3573e-01 (-9.4543e-01)\n",
            "Training...\n",
            "Epoch: [714][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.4005e-01 (-9.4005e-01)\n",
            "Epoch: [714][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5218e-01 (-9.4528e-01)\n",
            "Epoch: [714][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4218e-01 (-9.4602e-01)\n",
            "Epoch: [714][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3986e-01 (-9.4572e-01)\n",
            "Epoch: [714][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4746e-01 (-9.4602e-01)\n",
            "Epoch: [714][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4258e-01 (-9.4488e-01)\n",
            "Epoch: [714][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4449e-01 (-9.4455e-01)\n",
            "Epoch: [714][70/97]\tTime  0.176 ( 0.181)\tLoss -9.4846e-01 (-9.4449e-01)\n",
            "Epoch: [714][80/97]\tTime  0.176 ( 0.180)\tLoss -9.4230e-01 (-9.4475e-01)\n",
            "Epoch: [714][90/97]\tTime  0.176 ( 0.180)\tLoss -9.4055e-01 (-9.4483e-01)\n",
            "Training...\n",
            "Epoch: [715][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.4149e-01 (-9.4149e-01)\n",
            "Epoch: [715][10/97]\tTime  0.177 ( 0.201)\tLoss -9.4344e-01 (-9.4630e-01)\n",
            "Epoch: [715][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4446e-01 (-9.4652e-01)\n",
            "Epoch: [715][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4340e-01 (-9.4626e-01)\n",
            "Epoch: [715][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4660e-01 (-9.4611e-01)\n",
            "Epoch: [715][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4084e-01 (-9.4606e-01)\n",
            "Epoch: [715][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4119e-01 (-9.4584e-01)\n",
            "Epoch: [715][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4338e-01 (-9.4573e-01)\n",
            "Epoch: [715][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5063e-01 (-9.4603e-01)\n",
            "Epoch: [715][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5401e-01 (-9.4604e-01)\n",
            "Validating...\n",
            "Top1: 0.8687294407894737\n",
            "Training...\n",
            "Epoch: [716][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.4701e-01 (-9.4701e-01)\n",
            "Epoch: [716][10/97]\tTime  0.176 ( 0.202)\tLoss -9.4719e-01 (-9.4564e-01)\n",
            "Epoch: [716][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4337e-01 (-9.4577e-01)\n",
            "Epoch: [716][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4291e-01 (-9.4600e-01)\n",
            "Epoch: [716][40/97]\tTime  0.176 ( 0.184)\tLoss -9.4179e-01 (-9.4524e-01)\n",
            "Epoch: [716][50/97]\tTime  0.176 ( 0.182)\tLoss -9.3271e-01 (-9.4545e-01)\n",
            "Epoch: [716][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5826e-01 (-9.4551e-01)\n",
            "Epoch: [716][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4043e-01 (-9.4579e-01)\n",
            "Epoch: [716][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4845e-01 (-9.4593e-01)\n",
            "Epoch: [716][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4470e-01 (-9.4618e-01)\n",
            "Training...\n",
            "Epoch: [717][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.3941e-01 (-9.3941e-01)\n",
            "Epoch: [717][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5090e-01 (-9.4741e-01)\n",
            "Epoch: [717][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5076e-01 (-9.4862e-01)\n",
            "Epoch: [717][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5517e-01 (-9.4735e-01)\n",
            "Epoch: [717][40/97]\tTime  0.177 ( 0.183)\tLoss -9.5206e-01 (-9.4683e-01)\n",
            "Epoch: [717][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4641e-01 (-9.4648e-01)\n",
            "Epoch: [717][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4615e-01 (-9.4665e-01)\n",
            "Epoch: [717][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5489e-01 (-9.4687e-01)\n",
            "Epoch: [717][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5682e-01 (-9.4665e-01)\n",
            "Epoch: [717][90/97]\tTime  0.178 ( 0.180)\tLoss -9.3368e-01 (-9.4648e-01)\n",
            "Training...\n",
            "Epoch: [718][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.4973e-01 (-9.4973e-01)\n",
            "Epoch: [718][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4686e-01 (-9.4780e-01)\n",
            "Epoch: [718][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4360e-01 (-9.4763e-01)\n",
            "Epoch: [718][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4968e-01 (-9.4820e-01)\n",
            "Epoch: [718][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4634e-01 (-9.4827e-01)\n",
            "Epoch: [718][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5350e-01 (-9.4826e-01)\n",
            "Epoch: [718][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4252e-01 (-9.4832e-01)\n",
            "Epoch: [718][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5297e-01 (-9.4788e-01)\n",
            "Epoch: [718][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4984e-01 (-9.4796e-01)\n",
            "Epoch: [718][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4625e-01 (-9.4794e-01)\n",
            "Training...\n",
            "Epoch: [719][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.5269e-01 (-9.5269e-01)\n",
            "Epoch: [719][10/97]\tTime  0.177 ( 0.201)\tLoss -9.4092e-01 (-9.4866e-01)\n",
            "Epoch: [719][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4944e-01 (-9.4895e-01)\n",
            "Epoch: [719][30/97]\tTime  0.177 ( 0.186)\tLoss -9.3636e-01 (-9.4804e-01)\n",
            "Epoch: [719][40/97]\tTime  0.178 ( 0.183)\tLoss -9.4472e-01 (-9.4790e-01)\n",
            "Epoch: [719][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4667e-01 (-9.4744e-01)\n",
            "Epoch: [719][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4355e-01 (-9.4715e-01)\n",
            "Epoch: [719][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5414e-01 (-9.4734e-01)\n",
            "Epoch: [719][80/97]\tTime  0.176 ( 0.180)\tLoss -9.4785e-01 (-9.4753e-01)\n",
            "Epoch: [719][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4845e-01 (-9.4728e-01)\n",
            "Training...\n",
            "Epoch: [720][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.5413e-01 (-9.5413e-01)\n",
            "Epoch: [720][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5308e-01 (-9.4953e-01)\n",
            "Epoch: [720][20/97]\tTime  0.178 ( 0.190)\tLoss -9.4284e-01 (-9.4786e-01)\n",
            "Epoch: [720][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4594e-01 (-9.4718e-01)\n",
            "Epoch: [720][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5638e-01 (-9.4715e-01)\n",
            "Epoch: [720][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5096e-01 (-9.4756e-01)\n",
            "Epoch: [720][60/97]\tTime  0.178 ( 0.181)\tLoss -9.4816e-01 (-9.4737e-01)\n",
            "Epoch: [720][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4608e-01 (-9.4689e-01)\n",
            "Epoch: [720][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4967e-01 (-9.4701e-01)\n",
            "Epoch: [720][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4241e-01 (-9.4697e-01)\n",
            "Validating...\n",
            "Top1: 0.867907072368421\n",
            "Training...\n",
            "Epoch: [721][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.4258e-01 (-9.4258e-01)\n",
            "Epoch: [721][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4821e-01 (-9.4752e-01)\n",
            "Epoch: [721][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3943e-01 (-9.4665e-01)\n",
            "Epoch: [721][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4815e-01 (-9.4671e-01)\n",
            "Epoch: [721][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4107e-01 (-9.4657e-01)\n",
            "Epoch: [721][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4358e-01 (-9.4636e-01)\n",
            "Epoch: [721][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4560e-01 (-9.4643e-01)\n",
            "Epoch: [721][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4583e-01 (-9.4636e-01)\n",
            "Epoch: [721][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4998e-01 (-9.4626e-01)\n",
            "Epoch: [721][90/97]\tTime  0.176 ( 0.180)\tLoss -9.5054e-01 (-9.4646e-01)\n",
            "Training...\n",
            "Epoch: [722][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.4026e-01 (-9.4026e-01)\n",
            "Epoch: [722][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4115e-01 (-9.4901e-01)\n",
            "Epoch: [722][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5570e-01 (-9.5017e-01)\n",
            "Epoch: [722][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4579e-01 (-9.4887e-01)\n",
            "Epoch: [722][40/97]\tTime  0.176 ( 0.184)\tLoss -9.4781e-01 (-9.4842e-01)\n",
            "Epoch: [722][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4684e-01 (-9.4798e-01)\n",
            "Epoch: [722][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5026e-01 (-9.4778e-01)\n",
            "Epoch: [722][70/97]\tTime  0.177 ( 0.181)\tLoss -9.2772e-01 (-9.4760e-01)\n",
            "Epoch: [722][80/97]\tTime  0.178 ( 0.180)\tLoss -9.4284e-01 (-9.4760e-01)\n",
            "Epoch: [722][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4772e-01 (-9.4740e-01)\n",
            "Training...\n",
            "Epoch: [723][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.5102e-01 (-9.5102e-01)\n",
            "Epoch: [723][10/97]\tTime  0.178 ( 0.202)\tLoss -9.3898e-01 (-9.4567e-01)\n",
            "Epoch: [723][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5260e-01 (-9.4607e-01)\n",
            "Epoch: [723][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4491e-01 (-9.4709e-01)\n",
            "Epoch: [723][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4284e-01 (-9.4700e-01)\n",
            "Epoch: [723][50/97]\tTime  0.178 ( 0.182)\tLoss -9.4989e-01 (-9.4725e-01)\n",
            "Epoch: [723][60/97]\tTime  0.177 ( 0.182)\tLoss -9.3635e-01 (-9.4724e-01)\n",
            "Epoch: [723][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4857e-01 (-9.4700e-01)\n",
            "Epoch: [723][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4780e-01 (-9.4699e-01)\n",
            "Epoch: [723][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5165e-01 (-9.4709e-01)\n",
            "Training...\n",
            "Epoch: [724][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.5577e-01 (-9.5577e-01)\n",
            "Epoch: [724][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5089e-01 (-9.4997e-01)\n",
            "Epoch: [724][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4412e-01 (-9.4928e-01)\n",
            "Epoch: [724][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5127e-01 (-9.4896e-01)\n",
            "Epoch: [724][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4499e-01 (-9.4908e-01)\n",
            "Epoch: [724][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4925e-01 (-9.4867e-01)\n",
            "Epoch: [724][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5539e-01 (-9.4839e-01)\n",
            "Epoch: [724][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4968e-01 (-9.4833e-01)\n",
            "Epoch: [724][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4224e-01 (-9.4789e-01)\n",
            "Epoch: [724][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5332e-01 (-9.4794e-01)\n",
            "Training...\n",
            "Epoch: [725][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.3650e-01 (-9.3650e-01)\n",
            "Epoch: [725][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5259e-01 (-9.4660e-01)\n",
            "Epoch: [725][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4463e-01 (-9.4757e-01)\n",
            "Epoch: [725][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5253e-01 (-9.4802e-01)\n",
            "Epoch: [725][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4620e-01 (-9.4852e-01)\n",
            "Epoch: [725][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4728e-01 (-9.4827e-01)\n",
            "Epoch: [725][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5553e-01 (-9.4843e-01)\n",
            "Epoch: [725][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3974e-01 (-9.4832e-01)\n",
            "Epoch: [725][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5272e-01 (-9.4847e-01)\n",
            "Epoch: [725][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4600e-01 (-9.4828e-01)\n",
            "Validating...\n",
            "Top1: 0.8696546052631579\n",
            "Training...\n",
            "Epoch: [726][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.5054e-01 (-9.5054e-01)\n",
            "Epoch: [726][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4700e-01 (-9.4954e-01)\n",
            "Epoch: [726][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5382e-01 (-9.5037e-01)\n",
            "Epoch: [726][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5577e-01 (-9.4934e-01)\n",
            "Epoch: [726][40/97]\tTime  0.177 ( 0.183)\tLoss -9.5136e-01 (-9.4904e-01)\n",
            "Epoch: [726][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5544e-01 (-9.4888e-01)\n",
            "Epoch: [726][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4692e-01 (-9.4828e-01)\n",
            "Epoch: [726][70/97]\tTime  0.176 ( 0.181)\tLoss -9.5212e-01 (-9.4810e-01)\n",
            "Epoch: [726][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4915e-01 (-9.4851e-01)\n",
            "Epoch: [726][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4270e-01 (-9.4853e-01)\n",
            "Training...\n",
            "Epoch: [727][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.5048e-01 (-9.5048e-01)\n",
            "Epoch: [727][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5209e-01 (-9.4989e-01)\n",
            "Epoch: [727][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4699e-01 (-9.4926e-01)\n",
            "Epoch: [727][30/97]\tTime  0.176 ( 0.186)\tLoss -9.5069e-01 (-9.4819e-01)\n",
            "Epoch: [727][40/97]\tTime  0.176 ( 0.184)\tLoss -9.4801e-01 (-9.4800e-01)\n",
            "Epoch: [727][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5063e-01 (-9.4771e-01)\n",
            "Epoch: [727][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4459e-01 (-9.4775e-01)\n",
            "Epoch: [727][70/97]\tTime  0.177 ( 0.181)\tLoss -9.3388e-01 (-9.4760e-01)\n",
            "Epoch: [727][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5323e-01 (-9.4768e-01)\n",
            "Epoch: [727][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5063e-01 (-9.4743e-01)\n",
            "Training...\n",
            "Epoch: [728][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.4419e-01 (-9.4419e-01)\n",
            "Epoch: [728][10/97]\tTime  0.176 ( 0.202)\tLoss -9.5133e-01 (-9.4813e-01)\n",
            "Epoch: [728][20/97]\tTime  0.176 ( 0.190)\tLoss -9.4310e-01 (-9.4796e-01)\n",
            "Epoch: [728][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5028e-01 (-9.4887e-01)\n",
            "Epoch: [728][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4941e-01 (-9.4900e-01)\n",
            "Epoch: [728][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5050e-01 (-9.4925e-01)\n",
            "Epoch: [728][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4338e-01 (-9.4901e-01)\n",
            "Epoch: [728][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4234e-01 (-9.4837e-01)\n",
            "Epoch: [728][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4466e-01 (-9.4848e-01)\n",
            "Epoch: [728][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5023e-01 (-9.4833e-01)\n",
            "Training...\n",
            "Epoch: [729][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.3923e-01 (-9.3923e-01)\n",
            "Epoch: [729][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5683e-01 (-9.4638e-01)\n",
            "Epoch: [729][20/97]\tTime  0.178 ( 0.190)\tLoss -9.5531e-01 (-9.4871e-01)\n",
            "Epoch: [729][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5203e-01 (-9.4865e-01)\n",
            "Epoch: [729][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4423e-01 (-9.4897e-01)\n",
            "Epoch: [729][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5055e-01 (-9.4870e-01)\n",
            "Epoch: [729][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5118e-01 (-9.4877e-01)\n",
            "Epoch: [729][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5175e-01 (-9.4871e-01)\n",
            "Epoch: [729][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4005e-01 (-9.4852e-01)\n",
            "Epoch: [729][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4683e-01 (-9.4846e-01)\n",
            "Training...\n",
            "Epoch: [730][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.4519e-01 (-9.4519e-01)\n",
            "Epoch: [730][10/97]\tTime  0.177 ( 0.201)\tLoss -9.4799e-01 (-9.4803e-01)\n",
            "Epoch: [730][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4128e-01 (-9.4718e-01)\n",
            "Epoch: [730][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4988e-01 (-9.4750e-01)\n",
            "Epoch: [730][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4128e-01 (-9.4718e-01)\n",
            "Epoch: [730][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4945e-01 (-9.4733e-01)\n",
            "Epoch: [730][60/97]\tTime  0.176 ( 0.181)\tLoss -9.4003e-01 (-9.4759e-01)\n",
            "Epoch: [730][70/97]\tTime  0.176 ( 0.181)\tLoss -9.4839e-01 (-9.4790e-01)\n",
            "Epoch: [730][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4642e-01 (-9.4809e-01)\n",
            "Epoch: [730][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5598e-01 (-9.4821e-01)\n",
            "Validating...\n",
            "Top1: 0.8708881578947368\n",
            "Training...\n",
            "Epoch: [731][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.5439e-01 (-9.5439e-01)\n",
            "Epoch: [731][10/97]\tTime  0.176 ( 0.201)\tLoss -9.5428e-01 (-9.4954e-01)\n",
            "Epoch: [731][20/97]\tTime  0.176 ( 0.190)\tLoss -9.4551e-01 (-9.4911e-01)\n",
            "Epoch: [731][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5831e-01 (-9.4851e-01)\n",
            "Epoch: [731][40/97]\tTime  0.177 ( 0.183)\tLoss -9.5325e-01 (-9.4861e-01)\n",
            "Epoch: [731][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5512e-01 (-9.4823e-01)\n",
            "Epoch: [731][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4013e-01 (-9.4802e-01)\n",
            "Epoch: [731][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5484e-01 (-9.4830e-01)\n",
            "Epoch: [731][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4302e-01 (-9.4856e-01)\n",
            "Epoch: [731][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4080e-01 (-9.4826e-01)\n",
            "Training...\n",
            "Epoch: [732][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5151e-01 (-9.5151e-01)\n",
            "Epoch: [732][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5272e-01 (-9.4817e-01)\n",
            "Epoch: [732][20/97]\tTime  0.177 ( 0.190)\tLoss -9.3989e-01 (-9.4884e-01)\n",
            "Epoch: [732][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4588e-01 (-9.4839e-01)\n",
            "Epoch: [732][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5240e-01 (-9.4883e-01)\n",
            "Epoch: [732][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5004e-01 (-9.4876e-01)\n",
            "Epoch: [732][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4799e-01 (-9.4868e-01)\n",
            "Epoch: [732][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4144e-01 (-9.4836e-01)\n",
            "Epoch: [732][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4705e-01 (-9.4821e-01)\n",
            "Epoch: [732][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5124e-01 (-9.4831e-01)\n",
            "Training...\n",
            "Epoch: [733][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.4970e-01 (-9.4970e-01)\n",
            "Epoch: [733][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5556e-01 (-9.5020e-01)\n",
            "Epoch: [733][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5249e-01 (-9.4996e-01)\n",
            "Epoch: [733][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4583e-01 (-9.4972e-01)\n",
            "Epoch: [733][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4445e-01 (-9.4917e-01)\n",
            "Epoch: [733][50/97]\tTime  0.176 ( 0.182)\tLoss -9.5006e-01 (-9.4840e-01)\n",
            "Epoch: [733][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4597e-01 (-9.4813e-01)\n",
            "Epoch: [733][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4989e-01 (-9.4830e-01)\n",
            "Epoch: [733][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4141e-01 (-9.4824e-01)\n",
            "Epoch: [733][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4711e-01 (-9.4813e-01)\n",
            "Training...\n",
            "Epoch: [734][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.5554e-01 (-9.5554e-01)\n",
            "Epoch: [734][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4875e-01 (-9.5132e-01)\n",
            "Epoch: [734][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4832e-01 (-9.5076e-01)\n",
            "Epoch: [734][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5445e-01 (-9.5056e-01)\n",
            "Epoch: [734][40/97]\tTime  0.177 ( 0.183)\tLoss -9.5078e-01 (-9.5051e-01)\n",
            "Epoch: [734][50/97]\tTime  0.178 ( 0.182)\tLoss -9.4225e-01 (-9.5044e-01)\n",
            "Epoch: [734][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5561e-01 (-9.5008e-01)\n",
            "Epoch: [734][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5119e-01 (-9.4985e-01)\n",
            "Epoch: [734][80/97]\tTime  0.176 ( 0.180)\tLoss -9.5216e-01 (-9.4960e-01)\n",
            "Epoch: [734][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5775e-01 (-9.4989e-01)\n",
            "Training...\n",
            "Epoch: [735][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.4162e-01 (-9.4162e-01)\n",
            "Epoch: [735][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4957e-01 (-9.4920e-01)\n",
            "Epoch: [735][20/97]\tTime  0.177 ( 0.190)\tLoss -9.6132e-01 (-9.4971e-01)\n",
            "Epoch: [735][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5104e-01 (-9.4965e-01)\n",
            "Epoch: [735][40/97]\tTime  0.176 ( 0.183)\tLoss -9.5613e-01 (-9.4950e-01)\n",
            "Epoch: [735][50/97]\tTime  0.176 ( 0.182)\tLoss -9.4306e-01 (-9.4938e-01)\n",
            "Epoch: [735][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5296e-01 (-9.4953e-01)\n",
            "Epoch: [735][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5100e-01 (-9.4931e-01)\n",
            "Epoch: [735][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4402e-01 (-9.4947e-01)\n",
            "Epoch: [735][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4987e-01 (-9.4927e-01)\n",
            "Validating...\n",
            "Top1: 0.8692434210526315\n",
            "Training...\n",
            "Epoch: [736][ 0/97]\tTime  0.464 ( 0.464)\tLoss -9.5604e-01 (-9.5604e-01)\n",
            "Epoch: [736][10/97]\tTime  0.176 ( 0.203)\tLoss -9.5016e-01 (-9.5110e-01)\n",
            "Epoch: [736][20/97]\tTime  0.177 ( 0.191)\tLoss -9.3879e-01 (-9.4961e-01)\n",
            "Epoch: [736][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4668e-01 (-9.4950e-01)\n",
            "Epoch: [736][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5121e-01 (-9.4998e-01)\n",
            "Epoch: [736][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4644e-01 (-9.5018e-01)\n",
            "Epoch: [736][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5072e-01 (-9.5009e-01)\n",
            "Epoch: [736][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4946e-01 (-9.4977e-01)\n",
            "Epoch: [736][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4388e-01 (-9.4959e-01)\n",
            "Epoch: [736][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5024e-01 (-9.4955e-01)\n",
            "Training...\n",
            "Epoch: [737][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.4687e-01 (-9.4687e-01)\n",
            "Epoch: [737][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5285e-01 (-9.4903e-01)\n",
            "Epoch: [737][20/97]\tTime  0.176 ( 0.190)\tLoss -9.5340e-01 (-9.4949e-01)\n",
            "Epoch: [737][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5006e-01 (-9.4905e-01)\n",
            "Epoch: [737][40/97]\tTime  0.178 ( 0.184)\tLoss -9.5287e-01 (-9.4922e-01)\n",
            "Epoch: [737][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5651e-01 (-9.4930e-01)\n",
            "Epoch: [737][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4287e-01 (-9.4918e-01)\n",
            "Epoch: [737][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5348e-01 (-9.4935e-01)\n",
            "Epoch: [737][80/97]\tTime  0.178 ( 0.180)\tLoss -9.4968e-01 (-9.4961e-01)\n",
            "Epoch: [737][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4225e-01 (-9.4928e-01)\n",
            "Training...\n",
            "Epoch: [738][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.4603e-01 (-9.4603e-01)\n",
            "Epoch: [738][10/97]\tTime  0.177 ( 0.201)\tLoss -9.4911e-01 (-9.4747e-01)\n",
            "Epoch: [738][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5377e-01 (-9.4850e-01)\n",
            "Epoch: [738][30/97]\tTime  0.176 ( 0.186)\tLoss -9.5204e-01 (-9.4956e-01)\n",
            "Epoch: [738][40/97]\tTime  0.176 ( 0.183)\tLoss -9.4841e-01 (-9.5019e-01)\n",
            "Epoch: [738][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4781e-01 (-9.5022e-01)\n",
            "Epoch: [738][60/97]\tTime  0.178 ( 0.181)\tLoss -9.4953e-01 (-9.5017e-01)\n",
            "Epoch: [738][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5235e-01 (-9.5021e-01)\n",
            "Epoch: [738][80/97]\tTime  0.177 ( 0.180)\tLoss -9.6142e-01 (-9.5004e-01)\n",
            "Epoch: [738][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5777e-01 (-9.5006e-01)\n",
            "Training...\n",
            "Epoch: [739][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.5616e-01 (-9.5616e-01)\n",
            "Epoch: [739][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4820e-01 (-9.4969e-01)\n",
            "Epoch: [739][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5070e-01 (-9.5024e-01)\n",
            "Epoch: [739][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5202e-01 (-9.5058e-01)\n",
            "Epoch: [739][40/97]\tTime  0.176 ( 0.184)\tLoss -9.5114e-01 (-9.5012e-01)\n",
            "Epoch: [739][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4581e-01 (-9.4964e-01)\n",
            "Epoch: [739][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5056e-01 (-9.4979e-01)\n",
            "Epoch: [739][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4471e-01 (-9.5000e-01)\n",
            "Epoch: [739][80/97]\tTime  0.178 ( 0.180)\tLoss -9.6040e-01 (-9.5053e-01)\n",
            "Epoch: [739][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4908e-01 (-9.5031e-01)\n",
            "Training...\n",
            "Epoch: [740][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.4950e-01 (-9.4950e-01)\n",
            "Epoch: [740][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5194e-01 (-9.4936e-01)\n",
            "Epoch: [740][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4559e-01 (-9.4999e-01)\n",
            "Epoch: [740][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4845e-01 (-9.5015e-01)\n",
            "Epoch: [740][40/97]\tTime  0.176 ( 0.184)\tLoss -9.4826e-01 (-9.4989e-01)\n",
            "Epoch: [740][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4137e-01 (-9.4975e-01)\n",
            "Epoch: [740][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5079e-01 (-9.5015e-01)\n",
            "Epoch: [740][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4777e-01 (-9.5019e-01)\n",
            "Epoch: [740][80/97]\tTime  0.177 ( 0.180)\tLoss -9.6046e-01 (-9.5051e-01)\n",
            "Epoch: [740][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5552e-01 (-9.5026e-01)\n",
            "Validating...\n",
            "Top1: 0.8707853618421053\n",
            "Training...\n",
            "Epoch: [741][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.5135e-01 (-9.5135e-01)\n",
            "Epoch: [741][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5161e-01 (-9.4879e-01)\n",
            "Epoch: [741][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5804e-01 (-9.5052e-01)\n",
            "Epoch: [741][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4809e-01 (-9.5101e-01)\n",
            "Epoch: [741][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5124e-01 (-9.5127e-01)\n",
            "Epoch: [741][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5135e-01 (-9.5112e-01)\n",
            "Epoch: [741][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5380e-01 (-9.5094e-01)\n",
            "Epoch: [741][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5626e-01 (-9.5123e-01)\n",
            "Epoch: [741][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5825e-01 (-9.5108e-01)\n",
            "Epoch: [741][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4300e-01 (-9.5093e-01)\n",
            "Training...\n",
            "Epoch: [742][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.5357e-01 (-9.5357e-01)\n",
            "Epoch: [742][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5360e-01 (-9.5195e-01)\n",
            "Epoch: [742][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4673e-01 (-9.5075e-01)\n",
            "Epoch: [742][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4602e-01 (-9.4988e-01)\n",
            "Epoch: [742][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5779e-01 (-9.5015e-01)\n",
            "Epoch: [742][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4849e-01 (-9.5053e-01)\n",
            "Epoch: [742][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5040e-01 (-9.5066e-01)\n",
            "Epoch: [742][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5487e-01 (-9.5106e-01)\n",
            "Epoch: [742][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4801e-01 (-9.5099e-01)\n",
            "Epoch: [742][90/97]\tTime  0.177 ( 0.180)\tLoss -9.6120e-01 (-9.5094e-01)\n",
            "Training...\n",
            "Epoch: [743][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.5009e-01 (-9.5009e-01)\n",
            "Epoch: [743][10/97]\tTime  0.176 ( 0.202)\tLoss -9.4257e-01 (-9.4978e-01)\n",
            "Epoch: [743][20/97]\tTime  0.176 ( 0.190)\tLoss -9.5570e-01 (-9.4996e-01)\n",
            "Epoch: [743][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4914e-01 (-9.5000e-01)\n",
            "Epoch: [743][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5593e-01 (-9.5023e-01)\n",
            "Epoch: [743][50/97]\tTime  0.178 ( 0.182)\tLoss -9.5067e-01 (-9.5039e-01)\n",
            "Epoch: [743][60/97]\tTime  0.177 ( 0.181)\tLoss -9.4787e-01 (-9.5089e-01)\n",
            "Epoch: [743][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4477e-01 (-9.5076e-01)\n",
            "Epoch: [743][80/97]\tTime  0.177 ( 0.180)\tLoss -9.4139e-01 (-9.5057e-01)\n",
            "Epoch: [743][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4613e-01 (-9.5041e-01)\n",
            "Training...\n",
            "Epoch: [744][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.4841e-01 (-9.4841e-01)\n",
            "Epoch: [744][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5075e-01 (-9.4782e-01)\n",
            "Epoch: [744][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5910e-01 (-9.4972e-01)\n",
            "Epoch: [744][30/97]\tTime  0.177 ( 0.185)\tLoss -9.5612e-01 (-9.5034e-01)\n",
            "Epoch: [744][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4862e-01 (-9.5071e-01)\n",
            "Epoch: [744][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5003e-01 (-9.5060e-01)\n",
            "Epoch: [744][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5641e-01 (-9.5079e-01)\n",
            "Epoch: [744][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5829e-01 (-9.5103e-01)\n",
            "Epoch: [744][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5642e-01 (-9.5091e-01)\n",
            "Epoch: [744][90/97]\tTime  0.177 ( 0.180)\tLoss -9.3925e-01 (-9.5067e-01)\n",
            "Training...\n",
            "Epoch: [745][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.6155e-01 (-9.6155e-01)\n",
            "Epoch: [745][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4358e-01 (-9.5001e-01)\n",
            "Epoch: [745][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5183e-01 (-9.5108e-01)\n",
            "Epoch: [745][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4399e-01 (-9.5084e-01)\n",
            "Epoch: [745][40/97]\tTime  0.178 ( 0.184)\tLoss -9.5751e-01 (-9.5126e-01)\n",
            "Epoch: [745][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4263e-01 (-9.5058e-01)\n",
            "Epoch: [745][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5110e-01 (-9.5111e-01)\n",
            "Epoch: [745][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5030e-01 (-9.5095e-01)\n",
            "Epoch: [745][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5272e-01 (-9.5118e-01)\n",
            "Epoch: [745][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4948e-01 (-9.5094e-01)\n",
            "Validating...\n",
            "Top1: 0.8673930921052632\n",
            "Training...\n",
            "Epoch: [746][ 0/97]\tTime  0.441 ( 0.441)\tLoss -9.4217e-01 (-9.4217e-01)\n",
            "Epoch: [746][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5468e-01 (-9.4877e-01)\n",
            "Epoch: [746][20/97]\tTime  0.177 ( 0.189)\tLoss -9.5506e-01 (-9.5017e-01)\n",
            "Epoch: [746][30/97]\tTime  0.177 ( 0.185)\tLoss -9.5305e-01 (-9.4995e-01)\n",
            "Epoch: [746][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4580e-01 (-9.5037e-01)\n",
            "Epoch: [746][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5271e-01 (-9.5057e-01)\n",
            "Epoch: [746][60/97]\tTime  0.177 ( 0.181)\tLoss -9.5403e-01 (-9.5107e-01)\n",
            "Epoch: [746][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5028e-01 (-9.5108e-01)\n",
            "Epoch: [746][80/97]\tTime  0.178 ( 0.180)\tLoss -9.5301e-01 (-9.5111e-01)\n",
            "Epoch: [746][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5182e-01 (-9.5098e-01)\n",
            "Training...\n",
            "Epoch: [747][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.5351e-01 (-9.5351e-01)\n",
            "Epoch: [747][10/97]\tTime  0.176 ( 0.201)\tLoss -9.5903e-01 (-9.5264e-01)\n",
            "Epoch: [747][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5133e-01 (-9.5232e-01)\n",
            "Epoch: [747][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4677e-01 (-9.5162e-01)\n",
            "Epoch: [747][40/97]\tTime  0.177 ( 0.183)\tLoss -9.4819e-01 (-9.5151e-01)\n",
            "Epoch: [747][50/97]\tTime  0.177 ( 0.182)\tLoss -9.4892e-01 (-9.5155e-01)\n",
            "Epoch: [747][60/97]\tTime  0.178 ( 0.181)\tLoss -9.4504e-01 (-9.5143e-01)\n",
            "Epoch: [747][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4442e-01 (-9.5109e-01)\n",
            "Epoch: [747][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5635e-01 (-9.5108e-01)\n",
            "Epoch: [747][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4934e-01 (-9.5108e-01)\n",
            "Training...\n",
            "Epoch: [748][ 0/97]\tTime  0.459 ( 0.459)\tLoss -9.4947e-01 (-9.4947e-01)\n",
            "Epoch: [748][10/97]\tTime  0.177 ( 0.203)\tLoss -9.5530e-01 (-9.4950e-01)\n",
            "Epoch: [748][20/97]\tTime  0.177 ( 0.191)\tLoss -9.4972e-01 (-9.4985e-01)\n",
            "Epoch: [748][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5299e-01 (-9.5035e-01)\n",
            "Epoch: [748][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5219e-01 (-9.5058e-01)\n",
            "Epoch: [748][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5108e-01 (-9.5061e-01)\n",
            "Epoch: [748][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5259e-01 (-9.5076e-01)\n",
            "Epoch: [748][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5756e-01 (-9.5108e-01)\n",
            "Epoch: [748][80/97]\tTime  0.178 ( 0.181)\tLoss -9.4939e-01 (-9.5070e-01)\n",
            "Epoch: [748][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5249e-01 (-9.5073e-01)\n",
            "Training...\n",
            "Epoch: [749][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.4969e-01 (-9.4969e-01)\n",
            "Epoch: [749][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5670e-01 (-9.5276e-01)\n",
            "Epoch: [749][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4480e-01 (-9.5159e-01)\n",
            "Epoch: [749][30/97]\tTime  0.177 ( 0.187)\tLoss -9.5476e-01 (-9.5138e-01)\n",
            "Epoch: [749][40/97]\tTime  0.177 ( 0.185)\tLoss -9.5114e-01 (-9.5116e-01)\n",
            "Epoch: [749][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5700e-01 (-9.5147e-01)\n",
            "Epoch: [749][60/97]\tTime  0.177 ( 0.183)\tLoss -9.5076e-01 (-9.5136e-01)\n",
            "Epoch: [749][70/97]\tTime  0.198 ( 0.184)\tLoss -9.4773e-01 (-9.5132e-01)\n",
            "Epoch: [749][80/97]\tTime  0.190 ( 0.184)\tLoss -9.4447e-01 (-9.5104e-01)\n",
            "Epoch: [749][90/97]\tTime  0.192 ( 0.185)\tLoss -9.5501e-01 (-9.5124e-01)\n",
            "Training...\n",
            "Epoch: [750][ 0/97]\tTime  0.465 ( 0.465)\tLoss -9.5689e-01 (-9.5689e-01)\n",
            "Epoch: [750][10/97]\tTime  0.191 ( 0.217)\tLoss -9.4880e-01 (-9.5141e-01)\n",
            "Epoch: [750][20/97]\tTime  0.191 ( 0.204)\tLoss -9.4972e-01 (-9.5106e-01)\n",
            "Epoch: [750][30/97]\tTime  0.190 ( 0.200)\tLoss -9.4825e-01 (-9.5172e-01)\n",
            "Epoch: [750][40/97]\tTime  0.192 ( 0.198)\tLoss -9.5932e-01 (-9.5161e-01)\n",
            "Epoch: [750][50/97]\tTime  0.192 ( 0.197)\tLoss -9.5736e-01 (-9.5147e-01)\n",
            "Epoch: [750][60/97]\tTime  0.191 ( 0.196)\tLoss -9.5719e-01 (-9.5152e-01)\n",
            "Epoch: [750][70/97]\tTime  0.190 ( 0.195)\tLoss -9.5351e-01 (-9.5163e-01)\n",
            "Epoch: [750][80/97]\tTime  0.177 ( 0.194)\tLoss -9.5333e-01 (-9.5157e-01)\n",
            "Epoch: [750][90/97]\tTime  0.177 ( 0.192)\tLoss -9.4999e-01 (-9.5142e-01)\n",
            "Validating...\n",
            "Top1: 0.8705797697368421\n",
            "Saving...\n",
            "Training...\n",
            "Epoch: [751][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.6344e-01 (-9.6344e-01)\n",
            "Epoch: [751][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5002e-01 (-9.5041e-01)\n",
            "Epoch: [751][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5304e-01 (-9.5106e-01)\n",
            "Epoch: [751][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5669e-01 (-9.5097e-01)\n",
            "Epoch: [751][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5394e-01 (-9.5063e-01)\n",
            "Epoch: [751][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4836e-01 (-9.5074e-01)\n",
            "Epoch: [751][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4623e-01 (-9.5057e-01)\n",
            "Epoch: [751][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4915e-01 (-9.5024e-01)\n",
            "Epoch: [751][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4953e-01 (-9.5043e-01)\n",
            "Epoch: [751][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5801e-01 (-9.5049e-01)\n",
            "Training...\n",
            "Epoch: [752][ 0/97]\tTime  0.455 ( 0.455)\tLoss -9.5210e-01 (-9.5210e-01)\n",
            "Epoch: [752][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5089e-01 (-9.5421e-01)\n",
            "Epoch: [752][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5008e-01 (-9.5397e-01)\n",
            "Epoch: [752][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4678e-01 (-9.5245e-01)\n",
            "Epoch: [752][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5421e-01 (-9.5210e-01)\n",
            "Epoch: [752][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5516e-01 (-9.5206e-01)\n",
            "Epoch: [752][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5183e-01 (-9.5192e-01)\n",
            "Epoch: [752][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5562e-01 (-9.5210e-01)\n",
            "Epoch: [752][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4625e-01 (-9.5201e-01)\n",
            "Epoch: [752][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5196e-01 (-9.5185e-01)\n",
            "Training...\n",
            "Epoch: [753][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.3939e-01 (-9.3939e-01)\n",
            "Epoch: [753][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5009e-01 (-9.4704e-01)\n",
            "Epoch: [753][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5274e-01 (-9.4978e-01)\n",
            "Epoch: [753][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5232e-01 (-9.5020e-01)\n",
            "Epoch: [753][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4156e-01 (-9.5051e-01)\n",
            "Epoch: [753][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5981e-01 (-9.5065e-01)\n",
            "Epoch: [753][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5634e-01 (-9.5072e-01)\n",
            "Epoch: [753][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5697e-01 (-9.5081e-01)\n",
            "Epoch: [753][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5317e-01 (-9.5116e-01)\n",
            "Epoch: [753][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5170e-01 (-9.5134e-01)\n",
            "Training...\n",
            "Epoch: [754][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.5131e-01 (-9.5131e-01)\n",
            "Epoch: [754][10/97]\tTime  0.176 ( 0.202)\tLoss -9.6146e-01 (-9.5199e-01)\n",
            "Epoch: [754][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5449e-01 (-9.5144e-01)\n",
            "Epoch: [754][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5600e-01 (-9.5223e-01)\n",
            "Epoch: [754][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5125e-01 (-9.5242e-01)\n",
            "Epoch: [754][50/97]\tTime  0.178 ( 0.183)\tLoss -9.4988e-01 (-9.5199e-01)\n",
            "Epoch: [754][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4777e-01 (-9.5188e-01)\n",
            "Epoch: [754][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4959e-01 (-9.5199e-01)\n",
            "Epoch: [754][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5035e-01 (-9.5184e-01)\n",
            "Epoch: [754][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4563e-01 (-9.5167e-01)\n",
            "Training...\n",
            "Epoch: [755][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.5546e-01 (-9.5546e-01)\n",
            "Epoch: [755][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4907e-01 (-9.5300e-01)\n",
            "Epoch: [755][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4016e-01 (-9.5266e-01)\n",
            "Epoch: [755][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4914e-01 (-9.5222e-01)\n",
            "Epoch: [755][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5435e-01 (-9.5291e-01)\n",
            "Epoch: [755][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5502e-01 (-9.5248e-01)\n",
            "Epoch: [755][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4862e-01 (-9.5214e-01)\n",
            "Epoch: [755][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4948e-01 (-9.5209e-01)\n",
            "Epoch: [755][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5948e-01 (-9.5230e-01)\n",
            "Epoch: [755][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5320e-01 (-9.5195e-01)\n",
            "Validating...\n",
            "Top1: 0.8701685855263158\n",
            "Training...\n",
            "Epoch: [756][ 0/97]\tTime  0.466 ( 0.466)\tLoss -9.5633e-01 (-9.5633e-01)\n",
            "Epoch: [756][10/97]\tTime  0.177 ( 0.203)\tLoss -9.5445e-01 (-9.4928e-01)\n",
            "Epoch: [756][20/97]\tTime  0.177 ( 0.191)\tLoss -9.4307e-01 (-9.5194e-01)\n",
            "Epoch: [756][30/97]\tTime  0.178 ( 0.187)\tLoss -9.5230e-01 (-9.5156e-01)\n",
            "Epoch: [756][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4757e-01 (-9.5152e-01)\n",
            "Epoch: [756][50/97]\tTime  0.178 ( 0.183)\tLoss -9.4866e-01 (-9.5141e-01)\n",
            "Epoch: [756][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5823e-01 (-9.5146e-01)\n",
            "Epoch: [756][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5609e-01 (-9.5143e-01)\n",
            "Epoch: [756][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4949e-01 (-9.5128e-01)\n",
            "Epoch: [756][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5260e-01 (-9.5134e-01)\n",
            "Training...\n",
            "Epoch: [757][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5469e-01 (-9.5469e-01)\n",
            "Epoch: [757][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4972e-01 (-9.5230e-01)\n",
            "Epoch: [757][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4893e-01 (-9.5268e-01)\n",
            "Epoch: [757][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4417e-01 (-9.5205e-01)\n",
            "Epoch: [757][40/97]\tTime  0.176 ( 0.184)\tLoss -9.5262e-01 (-9.5229e-01)\n",
            "Epoch: [757][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5727e-01 (-9.5176e-01)\n",
            "Epoch: [757][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5673e-01 (-9.5208e-01)\n",
            "Epoch: [757][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4277e-01 (-9.5176e-01)\n",
            "Epoch: [757][80/97]\tTime  0.177 ( 0.181)\tLoss -9.6007e-01 (-9.5201e-01)\n",
            "Epoch: [757][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5484e-01 (-9.5202e-01)\n",
            "Training...\n",
            "Epoch: [758][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.4395e-01 (-9.4395e-01)\n",
            "Epoch: [758][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4911e-01 (-9.5261e-01)\n",
            "Epoch: [758][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5024e-01 (-9.5254e-01)\n",
            "Epoch: [758][30/97]\tTime  0.177 ( 0.186)\tLoss -9.6278e-01 (-9.5205e-01)\n",
            "Epoch: [758][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5037e-01 (-9.5222e-01)\n",
            "Epoch: [758][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5285e-01 (-9.5251e-01)\n",
            "Epoch: [758][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5366e-01 (-9.5281e-01)\n",
            "Epoch: [758][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4484e-01 (-9.5254e-01)\n",
            "Epoch: [758][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4561e-01 (-9.5241e-01)\n",
            "Epoch: [758][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5170e-01 (-9.5249e-01)\n",
            "Training...\n",
            "Epoch: [759][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.4709e-01 (-9.4709e-01)\n",
            "Epoch: [759][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4947e-01 (-9.5027e-01)\n",
            "Epoch: [759][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5614e-01 (-9.5146e-01)\n",
            "Epoch: [759][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5998e-01 (-9.5191e-01)\n",
            "Epoch: [759][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4871e-01 (-9.5234e-01)\n",
            "Epoch: [759][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5523e-01 (-9.5212e-01)\n",
            "Epoch: [759][60/97]\tTime  0.184 ( 0.183)\tLoss -9.5296e-01 (-9.5193e-01)\n",
            "Epoch: [759][70/97]\tTime  0.178 ( 0.182)\tLoss -9.5771e-01 (-9.5194e-01)\n",
            "Epoch: [759][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4854e-01 (-9.5196e-01)\n",
            "Epoch: [759][90/97]\tTime  0.178 ( 0.181)\tLoss -9.4662e-01 (-9.5181e-01)\n",
            "Training...\n",
            "Epoch: [760][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.4947e-01 (-9.4947e-01)\n",
            "Epoch: [760][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4773e-01 (-9.5306e-01)\n",
            "Epoch: [760][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5412e-01 (-9.5258e-01)\n",
            "Epoch: [760][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4595e-01 (-9.5136e-01)\n",
            "Epoch: [760][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5045e-01 (-9.5158e-01)\n",
            "Epoch: [760][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5638e-01 (-9.5146e-01)\n",
            "Epoch: [760][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5125e-01 (-9.5175e-01)\n",
            "Epoch: [760][70/97]\tTime  0.178 ( 0.181)\tLoss -9.6317e-01 (-9.5205e-01)\n",
            "Epoch: [760][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5585e-01 (-9.5221e-01)\n",
            "Epoch: [760][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5274e-01 (-9.5205e-01)\n",
            "Validating...\n",
            "Top1: 0.870374177631579\n",
            "Training...\n",
            "Epoch: [761][ 0/97]\tTime  0.458 ( 0.458)\tLoss -9.4638e-01 (-9.4638e-01)\n",
            "Epoch: [761][10/97]\tTime  0.177 ( 0.203)\tLoss -9.5001e-01 (-9.5229e-01)\n",
            "Epoch: [761][20/97]\tTime  0.177 ( 0.191)\tLoss -9.5556e-01 (-9.5267e-01)\n",
            "Epoch: [761][30/97]\tTime  0.177 ( 0.187)\tLoss -9.4061e-01 (-9.5280e-01)\n",
            "Epoch: [761][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5660e-01 (-9.5286e-01)\n",
            "Epoch: [761][50/97]\tTime  0.178 ( 0.183)\tLoss -9.4852e-01 (-9.5260e-01)\n",
            "Epoch: [761][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5248e-01 (-9.5286e-01)\n",
            "Epoch: [761][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5134e-01 (-9.5294e-01)\n",
            "Epoch: [761][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5922e-01 (-9.5314e-01)\n",
            "Epoch: [761][90/97]\tTime  0.178 ( 0.181)\tLoss -9.5516e-01 (-9.5315e-01)\n",
            "Training...\n",
            "Epoch: [762][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.5743e-01 (-9.5743e-01)\n",
            "Epoch: [762][10/97]\tTime  0.177 ( 0.202)\tLoss -9.6069e-01 (-9.5284e-01)\n",
            "Epoch: [762][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5712e-01 (-9.5280e-01)\n",
            "Epoch: [762][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5177e-01 (-9.5292e-01)\n",
            "Epoch: [762][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5580e-01 (-9.5292e-01)\n",
            "Epoch: [762][50/97]\tTime  0.178 ( 0.183)\tLoss -9.4604e-01 (-9.5229e-01)\n",
            "Epoch: [762][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5161e-01 (-9.5252e-01)\n",
            "Epoch: [762][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4906e-01 (-9.5218e-01)\n",
            "Epoch: [762][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5254e-01 (-9.5186e-01)\n",
            "Epoch: [762][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5165e-01 (-9.5197e-01)\n",
            "Training...\n",
            "Epoch: [763][ 0/97]\tTime  0.442 ( 0.442)\tLoss -9.5900e-01 (-9.5900e-01)\n",
            "Epoch: [763][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5993e-01 (-9.5381e-01)\n",
            "Epoch: [763][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5670e-01 (-9.5372e-01)\n",
            "Epoch: [763][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4886e-01 (-9.5259e-01)\n",
            "Epoch: [763][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5444e-01 (-9.5306e-01)\n",
            "Epoch: [763][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5045e-01 (-9.5337e-01)\n",
            "Epoch: [763][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5451e-01 (-9.5341e-01)\n",
            "Epoch: [763][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5023e-01 (-9.5347e-01)\n",
            "Epoch: [763][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4418e-01 (-9.5305e-01)\n",
            "Epoch: [763][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5554e-01 (-9.5272e-01)\n",
            "Training...\n",
            "Epoch: [764][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.5503e-01 (-9.5503e-01)\n",
            "Epoch: [764][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5087e-01 (-9.5375e-01)\n",
            "Epoch: [764][20/97]\tTime  0.178 ( 0.190)\tLoss -9.5624e-01 (-9.5443e-01)\n",
            "Epoch: [764][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4770e-01 (-9.5371e-01)\n",
            "Epoch: [764][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4673e-01 (-9.5318e-01)\n",
            "Epoch: [764][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4883e-01 (-9.5260e-01)\n",
            "Epoch: [764][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4761e-01 (-9.5231e-01)\n",
            "Epoch: [764][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5365e-01 (-9.5232e-01)\n",
            "Epoch: [764][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4655e-01 (-9.5234e-01)\n",
            "Epoch: [764][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4741e-01 (-9.5246e-01)\n",
            "Training...\n",
            "Epoch: [765][ 0/97]\tTime  0.444 ( 0.444)\tLoss -9.5852e-01 (-9.5852e-01)\n",
            "Epoch: [765][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5320e-01 (-9.5151e-01)\n",
            "Epoch: [765][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5192e-01 (-9.5289e-01)\n",
            "Epoch: [765][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4842e-01 (-9.5289e-01)\n",
            "Epoch: [765][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4780e-01 (-9.5350e-01)\n",
            "Epoch: [765][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5883e-01 (-9.5348e-01)\n",
            "Epoch: [765][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5140e-01 (-9.5313e-01)\n",
            "Epoch: [765][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5448e-01 (-9.5331e-01)\n",
            "Epoch: [765][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5903e-01 (-9.5338e-01)\n",
            "Epoch: [765][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5219e-01 (-9.5356e-01)\n",
            "Validating...\n",
            "Top1: 0.8714021381578947\n",
            "Training...\n",
            "Epoch: [766][ 0/97]\tTime  0.442 ( 0.442)\tLoss -9.5163e-01 (-9.5163e-01)\n",
            "Epoch: [766][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5525e-01 (-9.5567e-01)\n",
            "Epoch: [766][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5058e-01 (-9.5463e-01)\n",
            "Epoch: [766][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5264e-01 (-9.5397e-01)\n",
            "Epoch: [766][40/97]\tTime  0.178 ( 0.184)\tLoss -9.5581e-01 (-9.5347e-01)\n",
            "Epoch: [766][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5872e-01 (-9.5344e-01)\n",
            "Epoch: [766][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5347e-01 (-9.5373e-01)\n",
            "Epoch: [766][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5608e-01 (-9.5420e-01)\n",
            "Epoch: [766][80/97]\tTime  0.177 ( 0.180)\tLoss -9.5522e-01 (-9.5408e-01)\n",
            "Epoch: [766][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5368e-01 (-9.5407e-01)\n",
            "Training...\n",
            "Epoch: [767][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.5579e-01 (-9.5579e-01)\n",
            "Epoch: [767][10/97]\tTime  0.176 ( 0.202)\tLoss -9.6126e-01 (-9.5432e-01)\n",
            "Epoch: [767][20/97]\tTime  0.178 ( 0.190)\tLoss -9.4609e-01 (-9.5342e-01)\n",
            "Epoch: [767][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5055e-01 (-9.5381e-01)\n",
            "Epoch: [767][40/97]\tTime  0.178 ( 0.184)\tLoss -9.5813e-01 (-9.5387e-01)\n",
            "Epoch: [767][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5618e-01 (-9.5397e-01)\n",
            "Epoch: [767][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5456e-01 (-9.5370e-01)\n",
            "Epoch: [767][70/97]\tTime  0.177 ( 0.181)\tLoss -9.6127e-01 (-9.5341e-01)\n",
            "Epoch: [767][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5058e-01 (-9.5337e-01)\n",
            "Epoch: [767][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5795e-01 (-9.5325e-01)\n",
            "Training...\n",
            "Epoch: [768][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5447e-01 (-9.5447e-01)\n",
            "Epoch: [768][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5169e-01 (-9.5211e-01)\n",
            "Epoch: [768][20/97]\tTime  0.178 ( 0.190)\tLoss -9.4358e-01 (-9.5289e-01)\n",
            "Epoch: [768][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5908e-01 (-9.5397e-01)\n",
            "Epoch: [768][40/97]\tTime  0.178 ( 0.184)\tLoss -9.5171e-01 (-9.5320e-01)\n",
            "Epoch: [768][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5444e-01 (-9.5285e-01)\n",
            "Epoch: [768][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5988e-01 (-9.5344e-01)\n",
            "Epoch: [768][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5366e-01 (-9.5362e-01)\n",
            "Epoch: [768][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5718e-01 (-9.5340e-01)\n",
            "Epoch: [768][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5468e-01 (-9.5360e-01)\n",
            "Training...\n",
            "Epoch: [769][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.6440e-01 (-9.6440e-01)\n",
            "Epoch: [769][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4020e-01 (-9.5116e-01)\n",
            "Epoch: [769][20/97]\tTime  0.178 ( 0.190)\tLoss -9.5754e-01 (-9.5223e-01)\n",
            "Epoch: [769][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5737e-01 (-9.5235e-01)\n",
            "Epoch: [769][40/97]\tTime  0.178 ( 0.184)\tLoss -9.5152e-01 (-9.5247e-01)\n",
            "Epoch: [769][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5235e-01 (-9.5275e-01)\n",
            "Epoch: [769][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5265e-01 (-9.5274e-01)\n",
            "Epoch: [769][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5351e-01 (-9.5289e-01)\n",
            "Epoch: [769][80/97]\tTime  0.178 ( 0.181)\tLoss -9.6067e-01 (-9.5273e-01)\n",
            "Epoch: [769][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5598e-01 (-9.5277e-01)\n",
            "Training...\n",
            "Epoch: [770][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.6455e-01 (-9.6455e-01)\n",
            "Epoch: [770][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5862e-01 (-9.5558e-01)\n",
            "Epoch: [770][20/97]\tTime  0.178 ( 0.190)\tLoss -9.5166e-01 (-9.5441e-01)\n",
            "Epoch: [770][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4776e-01 (-9.5365e-01)\n",
            "Epoch: [770][40/97]\tTime  0.178 ( 0.184)\tLoss -9.4711e-01 (-9.5244e-01)\n",
            "Epoch: [770][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5090e-01 (-9.5256e-01)\n",
            "Epoch: [770][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5611e-01 (-9.5275e-01)\n",
            "Epoch: [770][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4745e-01 (-9.5290e-01)\n",
            "Epoch: [770][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5429e-01 (-9.5289e-01)\n",
            "Epoch: [770][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4691e-01 (-9.5298e-01)\n",
            "Validating...\n",
            "Top1: 0.8687294407894737\n",
            "Training...\n",
            "Epoch: [771][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.6000e-01 (-9.6000e-01)\n",
            "Epoch: [771][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5647e-01 (-9.5292e-01)\n",
            "Epoch: [771][20/97]\tTime  0.178 ( 0.190)\tLoss -9.5433e-01 (-9.5361e-01)\n",
            "Epoch: [771][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5623e-01 (-9.5360e-01)\n",
            "Epoch: [771][40/97]\tTime  0.178 ( 0.184)\tLoss -9.5410e-01 (-9.5343e-01)\n",
            "Epoch: [771][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5568e-01 (-9.5361e-01)\n",
            "Epoch: [771][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5011e-01 (-9.5299e-01)\n",
            "Epoch: [771][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4316e-01 (-9.5318e-01)\n",
            "Epoch: [771][80/97]\tTime  0.178 ( 0.181)\tLoss -9.6116e-01 (-9.5316e-01)\n",
            "Epoch: [771][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4939e-01 (-9.5308e-01)\n",
            "Training...\n",
            "Epoch: [772][ 0/97]\tTime  0.454 ( 0.454)\tLoss -9.5652e-01 (-9.5652e-01)\n",
            "Epoch: [772][10/97]\tTime  0.177 ( 0.203)\tLoss -9.4827e-01 (-9.5564e-01)\n",
            "Epoch: [772][20/97]\tTime  0.177 ( 0.191)\tLoss -9.5151e-01 (-9.5460e-01)\n",
            "Epoch: [772][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5399e-01 (-9.5416e-01)\n",
            "Epoch: [772][40/97]\tTime  0.178 ( 0.184)\tLoss -9.4922e-01 (-9.5399e-01)\n",
            "Epoch: [772][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5067e-01 (-9.5345e-01)\n",
            "Epoch: [772][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5406e-01 (-9.5318e-01)\n",
            "Epoch: [772][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4448e-01 (-9.5273e-01)\n",
            "Epoch: [772][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5322e-01 (-9.5298e-01)\n",
            "Epoch: [772][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4654e-01 (-9.5298e-01)\n",
            "Training...\n",
            "Epoch: [773][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.5564e-01 (-9.5564e-01)\n",
            "Epoch: [773][10/97]\tTime  0.177 ( 0.201)\tLoss -9.5603e-01 (-9.5192e-01)\n",
            "Epoch: [773][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4798e-01 (-9.5241e-01)\n",
            "Epoch: [773][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5139e-01 (-9.5188e-01)\n",
            "Epoch: [773][40/97]\tTime  0.178 ( 0.184)\tLoss -9.4964e-01 (-9.5201e-01)\n",
            "Epoch: [773][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5840e-01 (-9.5239e-01)\n",
            "Epoch: [773][60/97]\tTime  0.178 ( 0.182)\tLoss -9.4681e-01 (-9.5253e-01)\n",
            "Epoch: [773][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4306e-01 (-9.5265e-01)\n",
            "Epoch: [773][80/97]\tTime  0.178 ( 0.181)\tLoss -9.4562e-01 (-9.5276e-01)\n",
            "Epoch: [773][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5562e-01 (-9.5287e-01)\n",
            "Training...\n",
            "Epoch: [774][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5265e-01 (-9.5265e-01)\n",
            "Epoch: [774][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5271e-01 (-9.5574e-01)\n",
            "Epoch: [774][20/97]\tTime  0.177 ( 0.190)\tLoss -9.6249e-01 (-9.5416e-01)\n",
            "Epoch: [774][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5668e-01 (-9.5446e-01)\n",
            "Epoch: [774][40/97]\tTime  0.178 ( 0.184)\tLoss -9.4627e-01 (-9.5400e-01)\n",
            "Epoch: [774][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5230e-01 (-9.5379e-01)\n",
            "Epoch: [774][60/97]\tTime  0.178 ( 0.182)\tLoss -9.4994e-01 (-9.5327e-01)\n",
            "Epoch: [774][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5167e-01 (-9.5343e-01)\n",
            "Epoch: [774][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5808e-01 (-9.5353e-01)\n",
            "Epoch: [774][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5471e-01 (-9.5363e-01)\n",
            "Training...\n",
            "Epoch: [775][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.5588e-01 (-9.5588e-01)\n",
            "Epoch: [775][10/97]\tTime  0.178 ( 0.203)\tLoss -9.5033e-01 (-9.5296e-01)\n",
            "Epoch: [775][20/97]\tTime  0.177 ( 0.191)\tLoss -9.4646e-01 (-9.5287e-01)\n",
            "Epoch: [775][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5581e-01 (-9.5287e-01)\n",
            "Epoch: [775][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5865e-01 (-9.5335e-01)\n",
            "Epoch: [775][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5317e-01 (-9.5325e-01)\n",
            "Epoch: [775][60/97]\tTime  0.178 ( 0.182)\tLoss -9.4811e-01 (-9.5356e-01)\n",
            "Epoch: [775][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4757e-01 (-9.5350e-01)\n",
            "Epoch: [775][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5382e-01 (-9.5356e-01)\n",
            "Epoch: [775][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5177e-01 (-9.5385e-01)\n",
            "Validating...\n",
            "Top1: 0.869860197368421\n",
            "Training...\n",
            "Epoch: [776][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.5813e-01 (-9.5813e-01)\n",
            "Epoch: [776][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5236e-01 (-9.5127e-01)\n",
            "Epoch: [776][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5525e-01 (-9.5283e-01)\n",
            "Epoch: [776][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4384e-01 (-9.5270e-01)\n",
            "Epoch: [776][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5987e-01 (-9.5313e-01)\n",
            "Epoch: [776][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5884e-01 (-9.5311e-01)\n",
            "Epoch: [776][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5185e-01 (-9.5304e-01)\n",
            "Epoch: [776][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5205e-01 (-9.5313e-01)\n",
            "Epoch: [776][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5617e-01 (-9.5309e-01)\n",
            "Epoch: [776][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5615e-01 (-9.5318e-01)\n",
            "Training...\n",
            "Epoch: [777][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5783e-01 (-9.5783e-01)\n",
            "Epoch: [777][10/97]\tTime  0.178 ( 0.203)\tLoss -9.4829e-01 (-9.5289e-01)\n",
            "Epoch: [777][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5244e-01 (-9.5472e-01)\n",
            "Epoch: [777][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5751e-01 (-9.5470e-01)\n",
            "Epoch: [777][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5363e-01 (-9.5424e-01)\n",
            "Epoch: [777][50/97]\tTime  0.176 ( 0.183)\tLoss -9.5522e-01 (-9.5353e-01)\n",
            "Epoch: [777][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4983e-01 (-9.5331e-01)\n",
            "Epoch: [777][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5153e-01 (-9.5346e-01)\n",
            "Epoch: [777][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5399e-01 (-9.5349e-01)\n",
            "Epoch: [777][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4804e-01 (-9.5316e-01)\n",
            "Training...\n",
            "Epoch: [778][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5911e-01 (-9.5911e-01)\n",
            "Epoch: [778][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5058e-01 (-9.5317e-01)\n",
            "Epoch: [778][20/97]\tTime  0.177 ( 0.190)\tLoss -9.6146e-01 (-9.5242e-01)\n",
            "Epoch: [778][30/97]\tTime  0.178 ( 0.186)\tLoss -9.6109e-01 (-9.5359e-01)\n",
            "Epoch: [778][40/97]\tTime  0.183 ( 0.185)\tLoss -9.5401e-01 (-9.5389e-01)\n",
            "Epoch: [778][50/97]\tTime  0.177 ( 0.184)\tLoss -9.5827e-01 (-9.5357e-01)\n",
            "Epoch: [778][60/97]\tTime  0.178 ( 0.183)\tLoss -9.4791e-01 (-9.5358e-01)\n",
            "Epoch: [778][70/97]\tTime  0.177 ( 0.182)\tLoss -9.4883e-01 (-9.5345e-01)\n",
            "Epoch: [778][80/97]\tTime  0.178 ( 0.181)\tLoss -9.4462e-01 (-9.5372e-01)\n",
            "Epoch: [778][90/97]\tTime  0.177 ( 0.181)\tLoss -9.5628e-01 (-9.5390e-01)\n",
            "Training...\n",
            "Epoch: [779][ 0/97]\tTime  0.464 ( 0.464)\tLoss -9.5699e-01 (-9.5699e-01)\n",
            "Epoch: [779][10/97]\tTime  0.177 ( 0.203)\tLoss -9.5009e-01 (-9.5403e-01)\n",
            "Epoch: [779][20/97]\tTime  0.177 ( 0.191)\tLoss -9.6040e-01 (-9.5345e-01)\n",
            "Epoch: [779][30/97]\tTime  0.177 ( 0.187)\tLoss -9.5143e-01 (-9.5282e-01)\n",
            "Epoch: [779][40/97]\tTime  0.178 ( 0.184)\tLoss -9.4858e-01 (-9.5332e-01)\n",
            "Epoch: [779][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5333e-01 (-9.5321e-01)\n",
            "Epoch: [779][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5647e-01 (-9.5326e-01)\n",
            "Epoch: [779][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4835e-01 (-9.5327e-01)\n",
            "Epoch: [779][80/97]\tTime  0.178 ( 0.181)\tLoss -9.4858e-01 (-9.5358e-01)\n",
            "Epoch: [779][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5060e-01 (-9.5387e-01)\n",
            "Training...\n",
            "Epoch: [780][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.5317e-01 (-9.5317e-01)\n",
            "Epoch: [780][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5286e-01 (-9.5423e-01)\n",
            "Epoch: [780][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5632e-01 (-9.5522e-01)\n",
            "Epoch: [780][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5253e-01 (-9.5466e-01)\n",
            "Epoch: [780][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4698e-01 (-9.5414e-01)\n",
            "Epoch: [780][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5437e-01 (-9.5405e-01)\n",
            "Epoch: [780][60/97]\tTime  0.178 ( 0.182)\tLoss -9.6050e-01 (-9.5436e-01)\n",
            "Epoch: [780][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5760e-01 (-9.5422e-01)\n",
            "Epoch: [780][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5329e-01 (-9.5424e-01)\n",
            "Epoch: [780][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5659e-01 (-9.5428e-01)\n",
            "Validating...\n",
            "Top1: 0.870374177631579\n",
            "Training...\n",
            "Epoch: [781][ 0/97]\tTime  0.457 ( 0.457)\tLoss -9.5259e-01 (-9.5259e-01)\n",
            "Epoch: [781][10/97]\tTime  0.178 ( 0.203)\tLoss -9.5886e-01 (-9.5469e-01)\n",
            "Epoch: [781][20/97]\tTime  0.177 ( 0.191)\tLoss -9.4752e-01 (-9.5425e-01)\n",
            "Epoch: [781][30/97]\tTime  0.177 ( 0.186)\tLoss -9.4207e-01 (-9.5280e-01)\n",
            "Epoch: [781][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5671e-01 (-9.5328e-01)\n",
            "Epoch: [781][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4142e-01 (-9.5319e-01)\n",
            "Epoch: [781][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5360e-01 (-9.5328e-01)\n",
            "Epoch: [781][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5595e-01 (-9.5300e-01)\n",
            "Epoch: [781][80/97]\tTime  0.178 ( 0.181)\tLoss -9.4772e-01 (-9.5272e-01)\n",
            "Epoch: [781][90/97]\tTime  0.177 ( 0.181)\tLoss -9.5527e-01 (-9.5317e-01)\n",
            "Training...\n",
            "Epoch: [782][ 0/97]\tTime  0.461 ( 0.461)\tLoss -9.5834e-01 (-9.5834e-01)\n",
            "Epoch: [782][10/97]\tTime  0.178 ( 0.203)\tLoss -9.5053e-01 (-9.5300e-01)\n",
            "Epoch: [782][20/97]\tTime  0.177 ( 0.191)\tLoss -9.5661e-01 (-9.5353e-01)\n",
            "Epoch: [782][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5201e-01 (-9.5338e-01)\n",
            "Epoch: [782][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5403e-01 (-9.5377e-01)\n",
            "Epoch: [782][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5835e-01 (-9.5395e-01)\n",
            "Epoch: [782][60/97]\tTime  0.178 ( 0.182)\tLoss -9.5140e-01 (-9.5375e-01)\n",
            "Epoch: [782][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5330e-01 (-9.5355e-01)\n",
            "Epoch: [782][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5889e-01 (-9.5376e-01)\n",
            "Epoch: [782][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5717e-01 (-9.5380e-01)\n",
            "Training...\n",
            "Epoch: [783][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.5303e-01 (-9.5303e-01)\n",
            "Epoch: [783][10/97]\tTime  0.178 ( 0.202)\tLoss -9.4425e-01 (-9.5420e-01)\n",
            "Epoch: [783][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5129e-01 (-9.5390e-01)\n",
            "Epoch: [783][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5748e-01 (-9.5351e-01)\n",
            "Epoch: [783][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5211e-01 (-9.5348e-01)\n",
            "Epoch: [783][50/97]\tTime  0.177 ( 0.183)\tLoss -9.4987e-01 (-9.5337e-01)\n",
            "Epoch: [783][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5234e-01 (-9.5268e-01)\n",
            "Epoch: [783][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4844e-01 (-9.5280e-01)\n",
            "Epoch: [783][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5556e-01 (-9.5268e-01)\n",
            "Epoch: [783][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5452e-01 (-9.5291e-01)\n",
            "Training...\n",
            "Epoch: [784][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.5763e-01 (-9.5763e-01)\n",
            "Epoch: [784][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5341e-01 (-9.5476e-01)\n",
            "Epoch: [784][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5484e-01 (-9.5370e-01)\n",
            "Epoch: [784][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4981e-01 (-9.5397e-01)\n",
            "Epoch: [784][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5303e-01 (-9.5383e-01)\n",
            "Epoch: [784][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5506e-01 (-9.5425e-01)\n",
            "Epoch: [784][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5135e-01 (-9.5395e-01)\n",
            "Epoch: [784][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5465e-01 (-9.5396e-01)\n",
            "Epoch: [784][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5620e-01 (-9.5386e-01)\n",
            "Epoch: [784][90/97]\tTime  0.177 ( 0.180)\tLoss -9.6126e-01 (-9.5379e-01)\n",
            "Training...\n",
            "Epoch: [785][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5825e-01 (-9.5825e-01)\n",
            "Epoch: [785][10/97]\tTime  0.178 ( 0.202)\tLoss -9.4956e-01 (-9.5357e-01)\n",
            "Epoch: [785][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5274e-01 (-9.5426e-01)\n",
            "Epoch: [785][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4727e-01 (-9.5427e-01)\n",
            "Epoch: [785][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5875e-01 (-9.5409e-01)\n",
            "Epoch: [785][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5674e-01 (-9.5428e-01)\n",
            "Epoch: [785][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5196e-01 (-9.5421e-01)\n",
            "Epoch: [785][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4876e-01 (-9.5396e-01)\n",
            "Epoch: [785][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5529e-01 (-9.5401e-01)\n",
            "Epoch: [785][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5549e-01 (-9.5398e-01)\n",
            "Validating...\n",
            "Top1: 0.869860197368421\n",
            "Training...\n",
            "Epoch: [786][ 0/97]\tTime  0.448 ( 0.448)\tLoss -9.5249e-01 (-9.5249e-01)\n",
            "Epoch: [786][10/97]\tTime  0.178 ( 0.202)\tLoss -9.5218e-01 (-9.5540e-01)\n",
            "Epoch: [786][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5029e-01 (-9.5351e-01)\n",
            "Epoch: [786][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5763e-01 (-9.5276e-01)\n",
            "Epoch: [786][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4496e-01 (-9.5245e-01)\n",
            "Epoch: [786][50/97]\tTime  0.177 ( 0.183)\tLoss -9.6217e-01 (-9.5305e-01)\n",
            "Epoch: [786][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5677e-01 (-9.5313e-01)\n",
            "Epoch: [786][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4898e-01 (-9.5320e-01)\n",
            "Epoch: [786][80/97]\tTime  0.178 ( 0.181)\tLoss -9.5657e-01 (-9.5284e-01)\n",
            "Epoch: [786][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5752e-01 (-9.5275e-01)\n",
            "Training...\n",
            "Epoch: [787][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5954e-01 (-9.5954e-01)\n",
            "Epoch: [787][10/97]\tTime  0.178 ( 0.202)\tLoss -9.4939e-01 (-9.5513e-01)\n",
            "Epoch: [787][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4849e-01 (-9.5416e-01)\n",
            "Epoch: [787][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5702e-01 (-9.5495e-01)\n",
            "Epoch: [787][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5469e-01 (-9.5448e-01)\n",
            "Epoch: [787][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5227e-01 (-9.5407e-01)\n",
            "Epoch: [787][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4117e-01 (-9.5361e-01)\n",
            "Epoch: [787][70/97]\tTime  0.177 ( 0.181)\tLoss -9.5454e-01 (-9.5365e-01)\n",
            "Epoch: [787][80/97]\tTime  0.177 ( 0.181)\tLoss -9.3865e-01 (-9.5325e-01)\n",
            "Epoch: [787][90/97]\tTime  0.177 ( 0.180)\tLoss -9.5196e-01 (-9.5319e-01)\n",
            "Training...\n",
            "Epoch: [788][ 0/97]\tTime  0.446 ( 0.446)\tLoss -9.6517e-01 (-9.6517e-01)\n",
            "Epoch: [788][10/97]\tTime  0.178 ( 0.202)\tLoss -9.4884e-01 (-9.5593e-01)\n",
            "Epoch: [788][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5267e-01 (-9.5527e-01)\n",
            "Epoch: [788][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4960e-01 (-9.5413e-01)\n",
            "Epoch: [788][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5696e-01 (-9.5377e-01)\n",
            "Epoch: [788][50/97]\tTime  0.178 ( 0.183)\tLoss -9.6677e-01 (-9.5363e-01)\n",
            "Epoch: [788][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5382e-01 (-9.5345e-01)\n",
            "Epoch: [788][70/97]\tTime  0.177 ( 0.181)\tLoss -9.4197e-01 (-9.5324e-01)\n",
            "Epoch: [788][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5096e-01 (-9.5299e-01)\n",
            "Epoch: [788][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4485e-01 (-9.5287e-01)\n",
            "Training...\n",
            "Epoch: [789][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.5830e-01 (-9.5830e-01)\n",
            "Epoch: [789][10/97]\tTime  0.178 ( 0.201)\tLoss -9.5690e-01 (-9.5144e-01)\n",
            "Epoch: [789][20/97]\tTime  0.177 ( 0.190)\tLoss -9.4626e-01 (-9.5274e-01)\n",
            "Epoch: [789][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4942e-01 (-9.5282e-01)\n",
            "Epoch: [789][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5298e-01 (-9.5332e-01)\n",
            "Epoch: [789][50/97]\tTime  0.177 ( 0.182)\tLoss -9.5146e-01 (-9.5394e-01)\n",
            "Epoch: [789][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5286e-01 (-9.5387e-01)\n",
            "Epoch: [789][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5700e-01 (-9.5384e-01)\n",
            "Epoch: [789][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4907e-01 (-9.5388e-01)\n",
            "Epoch: [789][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4648e-01 (-9.5383e-01)\n",
            "Training...\n",
            "Epoch: [790][ 0/97]\tTime  0.449 ( 0.449)\tLoss -9.5698e-01 (-9.5698e-01)\n",
            "Epoch: [790][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5997e-01 (-9.5670e-01)\n",
            "Epoch: [790][20/97]\tTime  0.177 ( 0.190)\tLoss -9.6326e-01 (-9.5616e-01)\n",
            "Epoch: [790][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5538e-01 (-9.5498e-01)\n",
            "Epoch: [790][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5798e-01 (-9.5471e-01)\n",
            "Epoch: [790][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5024e-01 (-9.5467e-01)\n",
            "Epoch: [790][60/97]\tTime  0.177 ( 0.182)\tLoss -9.4289e-01 (-9.5482e-01)\n",
            "Epoch: [790][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4871e-01 (-9.5452e-01)\n",
            "Epoch: [790][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5056e-01 (-9.5420e-01)\n",
            "Epoch: [790][90/97]\tTime  0.177 ( 0.180)\tLoss -9.4205e-01 (-9.5411e-01)\n",
            "Validating...\n",
            "Top1: 0.8697574013157895\n",
            "Training...\n",
            "Epoch: [791][ 0/97]\tTime  0.450 ( 0.450)\tLoss -9.5472e-01 (-9.5472e-01)\n",
            "Epoch: [791][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5554e-01 (-9.5326e-01)\n",
            "Epoch: [791][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5631e-01 (-9.5389e-01)\n",
            "Epoch: [791][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5413e-01 (-9.5389e-01)\n",
            "Epoch: [791][40/97]\tTime  0.177 ( 0.184)\tLoss -9.6162e-01 (-9.5421e-01)\n",
            "Epoch: [791][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5705e-01 (-9.5405e-01)\n",
            "Epoch: [791][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5369e-01 (-9.5394e-01)\n",
            "Epoch: [791][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5897e-01 (-9.5432e-01)\n",
            "Epoch: [791][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5197e-01 (-9.5435e-01)\n",
            "Epoch: [791][90/97]\tTime  0.178 ( 0.180)\tLoss -9.6609e-01 (-9.5448e-01)\n",
            "Training...\n",
            "Epoch: [792][ 0/97]\tTime  0.443 ( 0.443)\tLoss -9.5050e-01 (-9.5050e-01)\n",
            "Epoch: [792][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5065e-01 (-9.5192e-01)\n",
            "Epoch: [792][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5683e-01 (-9.5268e-01)\n",
            "Epoch: [792][30/97]\tTime  0.178 ( 0.186)\tLoss -9.6068e-01 (-9.5355e-01)\n",
            "Epoch: [792][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5250e-01 (-9.5302e-01)\n",
            "Epoch: [792][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5544e-01 (-9.5295e-01)\n",
            "Epoch: [792][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5400e-01 (-9.5320e-01)\n",
            "Epoch: [792][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5315e-01 (-9.5334e-01)\n",
            "Epoch: [792][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5728e-01 (-9.5359e-01)\n",
            "Epoch: [792][90/97]\tTime  0.179 ( 0.180)\tLoss -9.5078e-01 (-9.5354e-01)\n",
            "Training...\n",
            "Epoch: [793][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.4979e-01 (-9.4979e-01)\n",
            "Epoch: [793][10/97]\tTime  0.177 ( 0.202)\tLoss -9.6320e-01 (-9.5487e-01)\n",
            "Epoch: [793][20/97]\tTime  0.177 ( 0.191)\tLoss -9.5673e-01 (-9.5458e-01)\n",
            "Epoch: [793][30/97]\tTime  0.178 ( 0.186)\tLoss -9.5772e-01 (-9.5519e-01)\n",
            "Epoch: [793][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5611e-01 (-9.5485e-01)\n",
            "Epoch: [793][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5161e-01 (-9.5477e-01)\n",
            "Epoch: [793][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5678e-01 (-9.5476e-01)\n",
            "Epoch: [793][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5264e-01 (-9.5449e-01)\n",
            "Epoch: [793][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5002e-01 (-9.5433e-01)\n",
            "Epoch: [793][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4845e-01 (-9.5426e-01)\n",
            "Training...\n",
            "Epoch: [794][ 0/97]\tTime  0.453 ( 0.453)\tLoss -9.5714e-01 (-9.5714e-01)\n",
            "Epoch: [794][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5478e-01 (-9.5531e-01)\n",
            "Epoch: [794][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5188e-01 (-9.5480e-01)\n",
            "Epoch: [794][30/97]\tTime  0.178 ( 0.186)\tLoss -9.4862e-01 (-9.5471e-01)\n",
            "Epoch: [794][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4969e-01 (-9.5453e-01)\n",
            "Epoch: [794][50/97]\tTime  0.178 ( 0.183)\tLoss -9.4591e-01 (-9.5414e-01)\n",
            "Epoch: [794][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5406e-01 (-9.5384e-01)\n",
            "Epoch: [794][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5240e-01 (-9.5386e-01)\n",
            "Epoch: [794][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4797e-01 (-9.5382e-01)\n",
            "Epoch: [794][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5902e-01 (-9.5412e-01)\n",
            "Training...\n",
            "Epoch: [795][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.5970e-01 (-9.5970e-01)\n",
            "Epoch: [795][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5491e-01 (-9.5549e-01)\n",
            "Epoch: [795][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5061e-01 (-9.5318e-01)\n",
            "Epoch: [795][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5467e-01 (-9.5326e-01)\n",
            "Epoch: [795][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5227e-01 (-9.5287e-01)\n",
            "Epoch: [795][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5369e-01 (-9.5346e-01)\n",
            "Epoch: [795][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5360e-01 (-9.5342e-01)\n",
            "Epoch: [795][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4773e-01 (-9.5333e-01)\n",
            "Epoch: [795][80/97]\tTime  0.177 ( 0.181)\tLoss -9.4856e-01 (-9.5324e-01)\n",
            "Epoch: [795][90/97]\tTime  0.178 ( 0.180)\tLoss -9.6068e-01 (-9.5339e-01)\n",
            "Validating...\n",
            "Top1: 0.8702713815789473\n",
            "Training...\n",
            "Epoch: [796][ 0/97]\tTime  0.451 ( 0.451)\tLoss -9.5935e-01 (-9.5935e-01)\n",
            "Epoch: [796][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4671e-01 (-9.5304e-01)\n",
            "Epoch: [796][20/97]\tTime  0.177 ( 0.190)\tLoss -9.5580e-01 (-9.5272e-01)\n",
            "Epoch: [796][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5415e-01 (-9.5265e-01)\n",
            "Epoch: [796][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5452e-01 (-9.5282e-01)\n",
            "Epoch: [796][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5543e-01 (-9.5303e-01)\n",
            "Epoch: [796][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5463e-01 (-9.5314e-01)\n",
            "Epoch: [796][70/97]\tTime  0.178 ( 0.181)\tLoss -9.4680e-01 (-9.5330e-01)\n",
            "Epoch: [796][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5841e-01 (-9.5349e-01)\n",
            "Epoch: [796][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5361e-01 (-9.5321e-01)\n",
            "Training...\n",
            "Epoch: [797][ 0/97]\tTime  0.456 ( 0.456)\tLoss -9.5185e-01 (-9.5185e-01)\n",
            "Epoch: [797][10/97]\tTime  0.177 ( 0.203)\tLoss -9.5234e-01 (-9.5420e-01)\n",
            "Epoch: [797][20/97]\tTime  0.177 ( 0.191)\tLoss -9.5563e-01 (-9.5381e-01)\n",
            "Epoch: [797][30/97]\tTime  0.177 ( 0.186)\tLoss -9.6130e-01 (-9.5414e-01)\n",
            "Epoch: [797][40/97]\tTime  0.177 ( 0.184)\tLoss -9.4929e-01 (-9.5408e-01)\n",
            "Epoch: [797][50/97]\tTime  0.178 ( 0.183)\tLoss -9.5107e-01 (-9.5388e-01)\n",
            "Epoch: [797][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5400e-01 (-9.5371e-01)\n",
            "Epoch: [797][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5912e-01 (-9.5402e-01)\n",
            "Epoch: [797][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5563e-01 (-9.5398e-01)\n",
            "Epoch: [797][90/97]\tTime  0.178 ( 0.180)\tLoss -9.4815e-01 (-9.5404e-01)\n",
            "Training...\n",
            "Epoch: [798][ 0/97]\tTime  0.447 ( 0.447)\tLoss -9.5486e-01 (-9.5486e-01)\n",
            "Epoch: [798][10/97]\tTime  0.177 ( 0.202)\tLoss -9.4769e-01 (-9.5329e-01)\n",
            "Epoch: [798][20/97]\tTime  0.178 ( 0.190)\tLoss -9.4940e-01 (-9.5339e-01)\n",
            "Epoch: [798][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5180e-01 (-9.5331e-01)\n",
            "Epoch: [798][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5312e-01 (-9.5289e-01)\n",
            "Epoch: [798][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5696e-01 (-9.5306e-01)\n",
            "Epoch: [798][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5295e-01 (-9.5275e-01)\n",
            "Epoch: [798][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5071e-01 (-9.5293e-01)\n",
            "Epoch: [798][80/97]\tTime  0.177 ( 0.181)\tLoss -9.5161e-01 (-9.5290e-01)\n",
            "Epoch: [798][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5213e-01 (-9.5302e-01)\n",
            "Training...\n",
            "Epoch: [799][ 0/97]\tTime  0.445 ( 0.445)\tLoss -9.5441e-01 (-9.5441e-01)\n",
            "Epoch: [799][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5488e-01 (-9.5468e-01)\n",
            "Epoch: [799][20/97]\tTime  0.178 ( 0.190)\tLoss -9.4780e-01 (-9.5332e-01)\n",
            "Epoch: [799][30/97]\tTime  0.177 ( 0.186)\tLoss -9.5110e-01 (-9.5285e-01)\n",
            "Epoch: [799][40/97]\tTime  0.177 ( 0.184)\tLoss -9.5437e-01 (-9.5338e-01)\n",
            "Epoch: [799][50/97]\tTime  0.177 ( 0.183)\tLoss -9.5146e-01 (-9.5311e-01)\n",
            "Epoch: [799][60/97]\tTime  0.177 ( 0.182)\tLoss -9.5209e-01 (-9.5320e-01)\n",
            "Epoch: [799][70/97]\tTime  0.178 ( 0.181)\tLoss -9.5618e-01 (-9.5310e-01)\n",
            "Epoch: [799][80/97]\tTime  0.177 ( 0.181)\tLoss -9.6035e-01 (-9.5295e-01)\n",
            "Epoch: [799][90/97]\tTime  0.178 ( 0.180)\tLoss -9.5538e-01 (-9.5317e-01)\n",
            "Training...\n",
            "Epoch: [800][ 0/97]\tTime  0.452 ( 0.452)\tLoss -9.6177e-01 (-9.6177e-01)\n",
            "Epoch: [800][10/97]\tTime  0.177 ( 0.202)\tLoss -9.5174e-01 (-9.5288e-01)\n",
            "Epoch: [800][20/97]\tTime  0.178 ( 0.191)\tLoss -9.5385e-01 (-9.5377e-01)\n",
            "Epoch: [800][30/97]\tTime  0.183 ( 0.187)\tLoss -9.5358e-01 (-9.5406e-01)\n",
            "Epoch: [800][40/97]\tTime  0.181 ( 0.186)\tLoss -9.5986e-01 (-9.5391e-01)\n",
            "Epoch: [800][50/97]\tTime  0.181 ( 0.185)\tLoss -9.6095e-01 (-9.5383e-01)\n",
            "Epoch: [800][60/97]\tTime  0.180 ( 0.184)\tLoss -9.5771e-01 (-9.5384e-01)\n",
            "Epoch: [800][70/97]\tTime  0.180 ( 0.183)\tLoss -9.5830e-01 (-9.5409e-01)\n",
            "Epoch: [800][80/97]\tTime  0.177 ( 0.183)\tLoss -9.5735e-01 (-9.5398e-01)\n",
            "Epoch: [800][90/97]\tTime  0.177 ( 0.182)\tLoss -9.5660e-01 (-9.5377e-01)\n",
            "Validating...\n",
            "Top1: 0.8692434210526315\n",
            "Saving...\n",
            "Best accuracy: 0.8722245065789473\n",
            "Saving the model at the last epoch.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "from os import path, makedirs\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.backends import cudnn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to set up training and validation for SimSiam.\n",
        "    Handles directory creation, data preparation, model setup, training loop, and checkpointing.\n",
        "    \"\"\"\n",
        "    # Create experiment directory if it doesn't exist\n",
        "    if not path.exists(args.exp_dir):\n",
        "        makedirs(args.exp_dir)\n",
        "\n",
        "    # Setup trial-specific directory and logger for TensorBoard\n",
        "    trial_dir = path.join(args.exp_dir, args.trial)\n",
        "    logger = SummaryWriter(trial_dir)\n",
        "    print(vars(args))  # Print experiment configuration\n",
        "\n",
        "    # Define data augmentation for training\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(args.img_dim, scale=(0.2, 1.)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # Random brightness, contrast, saturation, hue\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-10 mean and std\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-10 training dataset with TwoCropsTransform for SimSiam\n",
        "    train_set = datasets.CIFAR10(root=args.data_root,\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=TwoCropsTransform(train_transforms))\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_set,\n",
        "                              batch_size=args.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=args.num_workers,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "\n",
        "    # Initialize SimSiam model\n",
        "    model = SimSiam(args)\n",
        "\n",
        "    # Define SGD optimizer with momentum and weight decay\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=args.learning_rate,\n",
        "                          momentum=args.momentum,\n",
        "                          weight_decay=args.weight_decay)\n",
        "\n",
        "    # Initialize loss function (original or simplified version)\n",
        "    criterion = SimSiamLoss(args.loss_version)\n",
        "\n",
        "    # Move model and loss to GPU if available\n",
        "    if args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        model = model.cuda(args.gpu)\n",
        "        criterion = criterion.cuda(args.gpu)\n",
        "        cudnn.benchmark = True  # Enable auto-tuning for faster training\n",
        "\n",
        "    # Resume from a checkpoint if provided\n",
        "    start_epoch = 1\n",
        "    if args.resume is not None:\n",
        "        if path.isfile(args.resume):\n",
        "            start_epoch, model, optimizer = load_checkpoint(model, optimizer, args.resume)\n",
        "            print(\"Loaded checkpoint '{}' (epoch {})\".format(args.resume, start_epoch))\n",
        "        else:\n",
        "            print(\"No checkpoint found at '{}'\".format(args.resume))\n",
        "\n",
        "    # Training and validation loop\n",
        "    best_acc = 0.0\n",
        "    validation = KNNValidation(args, model.encoder)  # Initialize KNN validation\n",
        "    for epoch in range(start_epoch, args.epochs + 1):\n",
        "        adjust_learning_rate(optimizer, epoch, args)  # Update learning rate\n",
        "        print(\"Training...\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_loss = train(train_loader, model, criterion, optimizer, epoch, args)\n",
        "        logger.add_scalar('Loss/train', train_loss, epoch)  # Log training loss\n",
        "\n",
        "        # Perform KNN validation periodically\n",
        "        if epoch % args.eval_freq == 0:\n",
        "            print(\"Validating...\")\n",
        "            val_top1_acc = validation.eval()  # Evaluate KNN accuracy\n",
        "            print('Top1: {}'.format(val_top1_acc))\n",
        "\n",
        "            # Save the best model checkpoint\n",
        "            if val_top1_acc > best_acc:\n",
        "                best_acc = val_top1_acc\n",
        "                save_checkpoint(epoch, model, optimizer, best_acc,\n",
        "                                path.join(trial_dir, '{}_best.pth'.format(args.trial)),\n",
        "                                'Saving the best model!')\n",
        "            logger.add_scalar('Acc/val_top1', val_top1_acc, epoch)  # Log validation accuracy\n",
        "\n",
        "        # Save model periodically\n",
        "        if epoch % args.save_freq == 0:\n",
        "            save_checkpoint(epoch, model, optimizer, val_top1_acc,\n",
        "                            path.join(trial_dir, 'ckpt_epoch_{}_{}.pth'.format(epoch, args.trial)),\n",
        "                            'Saving...')\n",
        "\n",
        "    print('Best accuracy:', best_acc)\n",
        "\n",
        "    # Save the final model checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer, val_top1_acc,\n",
        "                    path.join(trial_dir, '{}_last.pth'.format(args.trial)),\n",
        "                    'Saving the model at the last epoch.')\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, args):\n",
        "    \"\"\"\n",
        "    Train the SimSiam model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        model (nn.Module): SimSiam model.\n",
        "        criterion (nn.Module): Loss function (e.g., SimSiamLoss).\n",
        "        optimizer (Optimizer): Optimizer (e.g., SGD).\n",
        "        epoch (int): Current epoch number.\n",
        "        args (Namespace): Experiment arguments.\n",
        "\n",
        "    Returns:\n",
        "        float: Average training loss for the epoch.\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter('Time', ':6.3f')  # Measure batch processing time\n",
        "    losses = AverageMeter('Loss', ':.4e')  # Track average loss\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, losses],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "    end = time.time()\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        if args.gpu is not None:\n",
        "            images[0] = images[0].cuda(args.gpu, non_blocking=True)\n",
        "            images[1] = images[1].cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outs = model(im_aug1=images[0], im_aug2=images[1])\n",
        "        loss = criterion(outs['z1'], outs['z2'], outs['p1'], outs['p2'])  # Compute SimSiam loss\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update loss and batch time\n",
        "        losses.update(loss.item(), images[0].size(0))\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:  # Display progress periodically\n",
        "            progress.display(i)\n",
        "\n",
        "    return losses.avg  # Return average loss\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"\n",
        "    Adjust the learning rate using a cosine annealing schedule.\n",
        "\n",
        "    Args:\n",
        "        optimizer (Optimizer): Optimizer to update.\n",
        "        epoch (int): Current epoch number.\n",
        "        args (Namespace): Experiment arguments.\n",
        "    \"\"\"\n",
        "    lr = args.learning_rate * 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Helper class to compute and store the average and current value of metrics.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    \"\"\"\n",
        "    Helper class to display progress during training or validation.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, acc, filename, msg):\n",
        "    \"\"\"\n",
        "    Save model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        epoch (int): Current epoch number.\n",
        "        model (nn.Module): Model to save.\n",
        "        optimizer (Optimizer): Optimizer to save.\n",
        "        acc (float): Accuracy value to save.\n",
        "        filename (str): Path to save the checkpoint file.\n",
        "        msg (str): Message to display after saving.\n",
        "    \"\"\"\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'arch': args.arch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'top1_acc': acc\n",
        "    }\n",
        "    torch.save(state, filename)\n",
        "    print(msg)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "    \"\"\"\n",
        "    Load model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model to load checkpoint into.\n",
        "        optimizer (Optimizer): Optimizer to load checkpoint into.\n",
        "        filename (str): Path to the checkpoint file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (start_epoch, model, optimizer)\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filename, map_location='cuda:0')\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return start_epoch, model, optimizer\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning SimSiam's learned representations on CIFAR-10 dataset\n"
      ],
      "metadata": {
        "id": "flid1Pc6oU_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJZ1phd-O09k",
        "outputId": "7d9075e4-f620-4e43-e9da-d33fbf07dab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed Arguments: Namespace(data='./data', arch='resnet18', num_cls=10, workers=8, epochs=200, start_epoch=0, batch_size=256, lr=30.0, schedule=[60, 80], momentum=0.9, weight_decay=0.0, print_freq=500, resume=None, evaluate=False, world_size=-1, rank=-1, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', seed=42, gpu=0, multiprocessing_distributed=False, pretrained='/home/opencvuniv/simsiam/experiments/1/ckpt_epoch_800_1.pth')\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Modify sys.argv to avoid Jupyter/IPython issues with argparse\n",
        "if 'ipykernel_launcher' in sys.argv[0]:\n",
        "    sys.argv = [sys.argv[0]]\n",
        "\n",
        "# Define experiment arguments as a Namespace object\n",
        "args = argparse.Namespace(\n",
        "    data='./data',                    # Path to dataset\n",
        "    arch='resnet18',                  # Backbone architecture\n",
        "    num_cls=10,                       # Number of output classes\n",
        "    workers=8,                        # Number of data loading workers\n",
        "    epochs=200,                       # Total number of training epochs\n",
        "    start_epoch=0,                    # Starting epoch (useful for resuming training)\n",
        "    batch_size=256,                   # Batch size for training\n",
        "    lr=30.,                           # Initial learning rate\n",
        "    schedule=[60, 80],                # Learning rate schedule (decay epochs)\n",
        "    momentum=0.9,                     # SGD momentum\n",
        "    weight_decay=0.0,                 # Weight decay for regularization\n",
        "    print_freq=500,                   # Frequency of printing logs\n",
        "    resume=None,                      # Path to checkpoint for resuming training\n",
        "    evaluate=False,                   # Flag for evaluation mode\n",
        "    world_size=-1,                    # World size for distributed training\n",
        "    rank=-1,                          # Rank for distributed training\n",
        "    dist_url='tcp://224.66.41.62:23456',  # URL for initializing distributed training\n",
        "    dist_backend='nccl',              # Backend for distributed training\n",
        "    seed=42,                          # Random seed for reproducibility\n",
        "    gpu=0,                            # GPU ID to use (if available)\n",
        "    multiprocessing_distributed=False, # Flag for multiprocessing distributed training\n",
        "    pretrained='' # Path to pre-trained checkpoint after 800th epoch\n",
        ")\n",
        "\n",
        "# Print the parsed arguments for verification and debugging\n",
        "print(\"Parsed Arguments:\", args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5y86boCO09l",
        "outputId": "6e660bb1-c3b3-4cc1-d32b-50511194f881"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_10307/3337336294.py:57: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.\n",
            "  warnings.warn('You have chosen to seed training. '\n",
            "/tmp/ipykernel_10307/3337336294.py:64: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n",
            "  warnings.warn('You have chosen a specific GPU. This will completely '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use GPU: 0 for training\n",
            "=> creating model 'resnet18'\n",
            "=> loading checkpoint '/home/opencvuniv/simsiam/experiments/1/ckpt_epoch_800_1.pth'\n",
            "=> loaded pre-trained model '/home/opencvuniv/simsiam/experiments/1/ckpt_epoch_800_1.pth'\n",
            "Epoch: [0][  0/195]\tTime  0.151 ( 0.151)\tData  0.134 ( 0.134)\tLoss 2.3027e+00 (2.3027e+00)\tAcc@1   7.42 (  7.42)\tAcc@5  49.22 ( 49.22)\n",
            "Test: [ 0/39]\tTime  0.121 ( 0.121)\tLoss 1.9001e+01 (1.9001e+01)\tAcc@1  32.42 ( 32.42)\tAcc@5  47.66 ( 47.66)\n",
            " * Acc@1 31.430 Acc@5 50.000\n",
            "=> loading '/home/opencvuniv/simsiam/experiments/1/ckpt_epoch_800_1.pth' for sanity check\n",
            "=> sanity check passed.\n",
            "Epoch: [1][  0/195]\tTime  0.187 ( 0.187)\tData  0.171 ( 0.171)\tLoss 1.5877e+01 (1.5877e+01)\tAcc@1  35.55 ( 35.55)\tAcc@5  53.91 ( 53.91)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 5.2912e-01 (5.2912e-01)\tAcc@1  87.89 ( 87.89)\tAcc@5  98.83 ( 98.83)\n",
            " * Acc@1 83.113 Acc@5 98.898\n",
            "Epoch: [2][  0/195]\tTime  0.153 ( 0.153)\tData  0.137 ( 0.137)\tLoss 5.2871e-01 (5.2871e-01)\tAcc@1  87.89 ( 87.89)\tAcc@5  98.83 ( 98.83)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 4.3413e-01 (4.3413e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5  98.05 ( 98.05)\n",
            " * Acc@1 87.340 Acc@5 98.588\n",
            "Epoch: [3][  0/195]\tTime  0.136 ( 0.136)\tData  0.120 ( 0.120)\tLoss 4.2910e-01 (4.2910e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 4.8113e-01 (4.8113e-01)\tAcc@1  86.33 ( 86.33)\tAcc@5  98.83 ( 98.83)\n",
            " * Acc@1 84.425 Acc@5 99.119\n",
            "Epoch: [4][  0/195]\tTime  0.147 ( 0.147)\tData  0.130 ( 0.130)\tLoss 4.5692e-01 (4.5692e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 3.1395e-01 (3.1395e-01)\tAcc@1  89.45 ( 89.45)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 89.263 Acc@5 99.449\n",
            "Epoch: [5][  0/195]\tTime  0.145 ( 0.145)\tData  0.129 ( 0.129)\tLoss 3.9923e-01 (3.9923e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 2.9037e-01 (2.9037e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.633 Acc@5 99.429\n",
            "Epoch: [6][  0/195]\tTime  0.138 ( 0.138)\tData  0.121 ( 0.121)\tLoss 3.6917e-01 (3.6917e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 3.9039e-01 (3.9039e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 85.927 Acc@5 99.209\n",
            "Epoch: [7][  0/195]\tTime  0.137 ( 0.137)\tData  0.122 ( 0.122)\tLoss 4.0667e-01 (4.0667e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.141 ( 0.141)\tLoss 2.8103e-01 (2.8103e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  98.83 ( 98.83)\n",
            " * Acc@1 89.874 Acc@5 99.409\n",
            "Epoch: [8][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 3.1381e-01 (3.1381e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 2.6755e-01 (2.6755e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.473 Acc@5 99.449\n",
            "Epoch: [9][  0/195]\tTime  0.160 ( 0.160)\tData  0.144 ( 0.144)\tLoss 4.7240e-01 (4.7240e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 3.0866e-01 (3.0866e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 87.780 Acc@5 99.319\n",
            "Epoch: [10][  0/195]\tTime  0.137 ( 0.137)\tData  0.121 ( 0.121)\tLoss 3.6805e-01 (3.6805e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 2.5833e-01 (2.5833e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 89.583 Acc@5 99.469\n",
            "Epoch: [11][  0/195]\tTime  0.144 ( 0.144)\tData  0.128 ( 0.128)\tLoss 2.5370e-01 (2.5370e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 3.1851e-01 (3.1851e-01)\tAcc@1  88.67 ( 88.67)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 87.500 Acc@5 99.279\n",
            "Epoch: [12][  0/195]\tTime  0.137 ( 0.137)\tData  0.121 ( 0.121)\tLoss 2.9434e-01 (2.9434e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  98.83 ( 98.83)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 2.2216e-01 (2.2216e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 90.254 Acc@5 99.509\n",
            "Epoch: [13][  0/195]\tTime  0.142 ( 0.142)\tData  0.126 ( 0.126)\tLoss 2.3614e-01 (2.3614e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 3.2374e-01 (3.2374e-01)\tAcc@1  88.67 ( 88.67)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 88.031 Acc@5 99.419\n",
            "Epoch: [14][  0/195]\tTime  0.147 ( 0.147)\tData  0.131 ( 0.131)\tLoss 2.7326e-01 (2.7326e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 2.6881e-01 (2.6881e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.293 Acc@5 99.459\n",
            "Epoch: [15][  0/195]\tTime  0.142 ( 0.142)\tData  0.125 ( 0.125)\tLoss 2.5425e-01 (2.5425e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 2.8657e-01 (2.8657e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 87.280 Acc@5 99.379\n",
            "Epoch: [16][  0/195]\tTime  0.133 ( 0.133)\tData  0.116 ( 0.116)\tLoss 3.2497e-01 (3.2497e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 3.0706e-01 (3.0706e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 87.370 Acc@5 99.499\n",
            "Epoch: [17][  0/195]\tTime  0.155 ( 0.155)\tData  0.139 ( 0.139)\tLoss 2.8481e-01 (2.8481e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 2.6781e-01 (2.6781e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.323 Acc@5 99.519\n",
            "Epoch: [18][  0/195]\tTime  0.143 ( 0.143)\tData  0.126 ( 0.126)\tLoss 2.9728e-01 (2.9728e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 2.1294e-01 (2.1294e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.905 Acc@5 99.690\n",
            "Epoch: [19][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 2.1024e-01 (2.1024e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 2.2537e-01 (2.2537e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.623 Acc@5 99.469\n",
            "Epoch: [20][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 3.0865e-01 (3.0865e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 2.4724e-01 (2.4724e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 89.914 Acc@5 99.489\n",
            "Epoch: [21][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 1.7805e-01 (1.7805e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 2.3012e-01 (2.3012e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            " * Acc@1 89.834 Acc@5 99.359\n",
            "Epoch: [22][  0/195]\tTime  0.133 ( 0.133)\tData  0.116 ( 0.116)\tLoss 1.7903e-01 (1.7903e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 2.2711e-01 (2.2711e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.365 Acc@5 99.519\n",
            "Epoch: [23][  0/195]\tTime  0.134 ( 0.134)\tData  0.118 ( 0.118)\tLoss 1.8804e-01 (1.8804e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 2.3705e-01 (2.3705e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.683 Acc@5 99.449\n",
            "Epoch: [24][  0/195]\tTime  0.137 ( 0.137)\tData  0.121 ( 0.121)\tLoss 2.1647e-01 (2.1647e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 2.8912e-01 (2.8912e-01)\tAcc@1  89.45 ( 89.45)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 88.892 Acc@5 99.399\n",
            "Epoch: [25][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 4.0560e-01 (4.0560e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 2.2256e-01 (2.2256e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.325 Acc@5 99.599\n",
            "Epoch: [26][  0/195]\tTime  0.159 ( 0.159)\tData  0.143 ( 0.143)\tLoss 2.1477e-01 (2.1477e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 2.0073e-01 (2.0073e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.266 Acc@5 99.529\n",
            "Epoch: [27][  0/195]\tTime  0.143 ( 0.143)\tData  0.126 ( 0.126)\tLoss 2.6184e-01 (2.6184e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.129 ( 0.129)\tLoss 3.2691e-01 (3.2691e-01)\tAcc@1  87.89 ( 87.89)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 87.179 Acc@5 99.559\n",
            "Epoch: [28][  0/195]\tTime  0.145 ( 0.145)\tData  0.128 ( 0.128)\tLoss 3.5172e-01 (3.5172e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 2.6899e-01 (2.6899e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.683 Acc@5 99.529\n",
            "Epoch: [29][  0/195]\tTime  0.139 ( 0.139)\tData  0.122 ( 0.122)\tLoss 3.2291e-01 (3.2291e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.153 ( 0.153)\tLoss 2.1945e-01 (2.1945e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.835 Acc@5 99.679\n",
            "Epoch: [30][  0/195]\tTime  0.153 ( 0.153)\tData  0.136 ( 0.136)\tLoss 3.1679e-01 (3.1679e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  98.44 ( 98.44)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 2.3297e-01 (2.3297e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 90.074 Acc@5 99.379\n",
            "Epoch: [31][  0/195]\tTime  0.167 ( 0.167)\tData  0.152 ( 0.152)\tLoss 2.8002e-01 (2.8002e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 2.4371e-01 (2.4371e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.046 Acc@5 99.569\n",
            "Epoch: [32][  0/195]\tTime  0.142 ( 0.142)\tData  0.125 ( 0.125)\tLoss 1.7009e-01 (1.7009e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.134 ( 0.134)\tLoss 2.2513e-01 (2.2513e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.895 Acc@5 99.619\n",
            "Epoch: [33][  0/195]\tTime  0.140 ( 0.140)\tData  0.123 ( 0.123)\tLoss 1.7682e-01 (1.7682e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 2.1262e-01 (2.1262e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.525 Acc@5 99.509\n",
            "Epoch: [34][  0/195]\tTime  0.147 ( 0.147)\tData  0.130 ( 0.130)\tLoss 1.8145e-01 (1.8145e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 2.6779e-01 (2.6779e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 88.812 Acc@5 99.509\n",
            "Epoch: [35][  0/195]\tTime  0.145 ( 0.145)\tData  0.129 ( 0.129)\tLoss 2.4707e-01 (2.4707e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  98.83 ( 98.83)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 2.3767e-01 (2.3767e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.543 Acc@5 99.299\n",
            "Epoch: [36][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 3.4028e-01 (3.4028e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 2.1896e-01 (2.1896e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5 100.00 (100.00)\n",
            " * Acc@1 89.583 Acc@5 99.579\n",
            "Epoch: [37][  0/195]\tTime  0.144 ( 0.144)\tData  0.127 ( 0.127)\tLoss 2.2110e-01 (2.2110e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 2.7072e-01 (2.7072e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 89.503 Acc@5 99.349\n",
            "Epoch: [38][  0/195]\tTime  0.155 ( 0.155)\tData  0.138 ( 0.138)\tLoss 2.0204e-01 (2.0204e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 2.0069e-01 (2.0069e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 91.176 Acc@5 99.519\n",
            "Epoch: [39][  0/195]\tTime  0.138 ( 0.138)\tData  0.121 ( 0.121)\tLoss 2.2501e-01 (2.2501e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.129 ( 0.129)\tLoss 2.7247e-01 (2.7247e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 88.532 Acc@5 99.279\n",
            "Epoch: [40][  0/195]\tTime  0.139 ( 0.139)\tData  0.122 ( 0.122)\tLoss 1.9144e-01 (1.9144e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 2.1949e-01 (2.1949e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.485 Acc@5 99.629\n",
            "Epoch: [41][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 2.5280e-01 (2.5280e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.129 ( 0.129)\tLoss 3.0247e-01 (3.0247e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 88.041 Acc@5 99.269\n",
            "Epoch: [42][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 2.7970e-01 (2.7970e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 2.2897e-01 (2.2897e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.914 Acc@5 99.379\n",
            "Epoch: [43][  0/195]\tTime  0.135 ( 0.135)\tData  0.119 ( 0.119)\tLoss 2.1805e-01 (2.1805e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.140 ( 0.140)\tLoss 2.5066e-01 (2.5066e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.583 Acc@5 99.690\n",
            "Epoch: [44][  0/195]\tTime  0.141 ( 0.141)\tData  0.124 ( 0.124)\tLoss 1.9729e-01 (1.9729e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 2.3001e-01 (2.3001e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.052 Acc@5 99.369\n",
            "Epoch: [45][  0/195]\tTime  0.132 ( 0.132)\tData  0.116 ( 0.116)\tLoss 2.7296e-01 (2.7296e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 2.1939e-01 (2.1939e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.495 Acc@5 99.599\n",
            "Epoch: [46][  0/195]\tTime  0.146 ( 0.146)\tData  0.130 ( 0.130)\tLoss 2.6580e-01 (2.6580e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 2.5577e-01 (2.5577e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 89.163 Acc@5 99.199\n",
            "Epoch: [47][  0/195]\tTime  0.138 ( 0.138)\tData  0.120 ( 0.120)\tLoss 2.2681e-01 (2.2681e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 2.8812e-01 (2.8812e-01)\tAcc@1  89.45 ( 89.45)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 88.772 Acc@5 99.389\n",
            "Epoch: [48][  0/195]\tTime  0.157 ( 0.157)\tData  0.141 ( 0.141)\tLoss 4.0116e-01 (4.0116e-01)\tAcc@1  88.67 ( 88.67)\tAcc@5  98.44 ( 98.44)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 4.1093e-01 (4.1093e-01)\tAcc@1  84.77 ( 84.77)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 85.236 Acc@5 99.199\n",
            "Epoch: [49][  0/195]\tTime  0.142 ( 0.142)\tData  0.126 ( 0.126)\tLoss 2.7605e-01 (2.7605e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 2.0502e-01 (2.0502e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            " * Acc@1 89.914 Acc@5 99.479\n",
            "Epoch: [50][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 3.0193e-01 (3.0193e-01)\tAcc@1  88.67 ( 88.67)\tAcc@5  98.83 ( 98.83)\n",
            "Test: [ 0/39]\tTime  0.121 ( 0.121)\tLoss 2.5350e-01 (2.5350e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 89.433 Acc@5 99.559\n",
            "Epoch: [51][  0/195]\tTime  0.136 ( 0.136)\tData  0.120 ( 0.120)\tLoss 2.6338e-01 (2.6338e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 2.6695e-01 (2.6695e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 88.952 Acc@5 99.389\n",
            "Epoch: [52][  0/195]\tTime  0.144 ( 0.144)\tData  0.128 ( 0.128)\tLoss 2.3109e-01 (2.3109e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 2.4492e-01 (2.4492e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 89.914 Acc@5 99.479\n",
            "Epoch: [53][  0/195]\tTime  0.168 ( 0.168)\tData  0.152 ( 0.152)\tLoss 2.9895e-01 (2.9895e-01)\tAcc@1  89.45 ( 89.45)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 2.9755e-01 (2.9755e-01)\tAcc@1  88.67 ( 88.67)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 88.081 Acc@5 98.788\n",
            "Epoch: [54][  0/195]\tTime  0.136 ( 0.136)\tData  0.120 ( 0.120)\tLoss 4.0554e-01 (4.0554e-01)\tAcc@1  87.11 ( 87.11)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 2.6438e-01 (2.6438e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5 100.00 (100.00)\n",
            " * Acc@1 88.812 Acc@5 99.489\n",
            "Epoch: [55][  0/195]\tTime  0.137 ( 0.137)\tData  0.121 ( 0.121)\tLoss 2.6819e-01 (2.6819e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 2.1308e-01 (2.1308e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 90.685 Acc@5 99.649\n",
            "Epoch: [56][  0/195]\tTime  0.153 ( 0.153)\tData  0.137 ( 0.137)\tLoss 3.0441e-01 (3.0441e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 2.1074e-01 (2.1074e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            " * Acc@1 90.745 Acc@5 99.679\n",
            "Epoch: [57][  0/195]\tTime  0.135 ( 0.135)\tData  0.118 ( 0.118)\tLoss 1.9843e-01 (1.9843e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 2.1910e-01 (2.1910e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            " * Acc@1 89.784 Acc@5 99.629\n",
            "Epoch: [58][  0/195]\tTime  0.162 ( 0.162)\tData  0.145 ( 0.145)\tLoss 1.2301e-01 (1.2301e-01)\tAcc@1  95.70 ( 95.70)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.9665e-01 (1.9665e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.186 Acc@5 99.619\n",
            "Epoch: [59][  0/195]\tTime  0.151 ( 0.151)\tData  0.134 ( 0.134)\tLoss 3.0435e-01 (3.0435e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 2.0669e-01 (2.0669e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.22 ( 99.22)\n",
            " * Acc@1 91.346 Acc@5 99.559\n",
            "Epoch: [60][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 1.9890e-01 (1.9890e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.9065e-01 (1.9065e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.627 Acc@5 99.720\n",
            "Epoch: [61][  0/195]\tTime  0.139 ( 0.139)\tData  0.122 ( 0.122)\tLoss 2.2806e-01 (2.2806e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.8843e-01 (1.8843e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.647 Acc@5 99.690\n",
            "Epoch: [62][  0/195]\tTime  0.136 ( 0.136)\tData  0.119 ( 0.119)\tLoss 2.5706e-01 (2.5706e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.8837e-01 (1.8837e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.567 Acc@5 99.710\n",
            "Epoch: [63][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 1.7872e-01 (1.7872e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.9158e-01 (1.9158e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.700\n",
            "Epoch: [64][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 1.3489e-01 (1.3489e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.9704e-01 (1.9704e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.546 Acc@5 99.760\n",
            "Epoch: [65][  0/195]\tTime  0.138 ( 0.138)\tData  0.121 ( 0.121)\tLoss 1.6789e-01 (1.6789e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.145 ( 0.145)\tLoss 1.9268e-01 (1.9268e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.777 Acc@5 99.740\n",
            "Epoch: [66][  0/195]\tTime  0.135 ( 0.135)\tData  0.118 ( 0.118)\tLoss 2.3913e-01 (2.3913e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9388e-01 (1.9388e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.637 Acc@5 99.730\n",
            "Epoch: [67][  0/195]\tTime  0.135 ( 0.135)\tData  0.120 ( 0.120)\tLoss 2.2330e-01 (2.2330e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9168e-01 (1.9168e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.710\n",
            "Epoch: [68][  0/195]\tTime  0.135 ( 0.135)\tData  0.119 ( 0.119)\tLoss 2.1531e-01 (2.1531e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.9697e-01 (1.9697e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.356 Acc@5 99.690\n",
            "Epoch: [69][  0/195]\tTime  0.144 ( 0.144)\tData  0.128 ( 0.128)\tLoss 2.3276e-01 (2.3276e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 1.9190e-01 (1.9190e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.657 Acc@5 99.720\n",
            "Epoch: [70][  0/195]\tTime  0.132 ( 0.132)\tData  0.116 ( 0.116)\tLoss 2.0141e-01 (2.0141e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.121 ( 0.121)\tLoss 1.9432e-01 (1.9432e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.637 Acc@5 99.649\n",
            "Epoch: [71][  0/195]\tTime  0.135 ( 0.135)\tData  0.120 ( 0.120)\tLoss 2.0769e-01 (2.0769e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.9172e-01 (1.9172e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.627 Acc@5 99.690\n",
            "Epoch: [72][  0/195]\tTime  0.154 ( 0.154)\tData  0.138 ( 0.138)\tLoss 2.5325e-01 (2.5325e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  98.83 ( 98.83)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 1.9232e-01 (1.9232e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.516 Acc@5 99.690\n",
            "Epoch: [73][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 2.1376e-01 (2.1376e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.9119e-01 (1.9119e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.750\n",
            "Epoch: [74][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 1.7909e-01 (1.7909e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.9333e-01 (1.9333e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.717 Acc@5 99.730\n",
            "Epoch: [75][  0/195]\tTime  0.177 ( 0.177)\tData  0.162 ( 0.162)\tLoss 2.2451e-01 (2.2451e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.9328e-01 (1.9328e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.677 Acc@5 99.679\n",
            "Epoch: [76][  0/195]\tTime  0.146 ( 0.146)\tData  0.130 ( 0.130)\tLoss 2.4616e-01 (2.4616e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.8809e-01 (1.8809e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.617 Acc@5 99.700\n",
            "Epoch: [77][  0/195]\tTime  0.139 ( 0.139)\tData  0.122 ( 0.122)\tLoss 2.4009e-01 (2.4009e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.8662e-01 (1.8662e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.516 Acc@5 99.700\n",
            "Epoch: [78][  0/195]\tTime  0.134 ( 0.134)\tData  0.118 ( 0.118)\tLoss 2.1452e-01 (2.1452e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.9626e-01 (1.9626e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.690\n",
            "Epoch: [79][  0/195]\tTime  0.135 ( 0.135)\tData  0.119 ( 0.119)\tLoss 2.3912e-01 (2.3912e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.134 ( 0.134)\tLoss 1.9302e-01 (1.9302e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.627 Acc@5 99.740\n",
            "Epoch: [80][  0/195]\tTime  0.140 ( 0.140)\tData  0.124 ( 0.124)\tLoss 1.5210e-01 (1.5210e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.135 ( 0.135)\tLoss 1.8917e-01 (1.8917e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.720\n",
            "Epoch: [81][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 1.4554e-01 (1.4554e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.8900e-01 (1.8900e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.827 Acc@5 99.710\n",
            "Epoch: [82][  0/195]\tTime  0.158 ( 0.158)\tData  0.143 ( 0.143)\tLoss 2.6679e-01 (2.6679e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8969e-01 (1.8969e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.720\n",
            "Epoch: [83][  0/195]\tTime  0.143 ( 0.143)\tData  0.127 ( 0.127)\tLoss 1.3929e-01 (1.3929e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9025e-01 (1.9025e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.787 Acc@5 99.740\n",
            "Epoch: [84][  0/195]\tTime  0.135 ( 0.135)\tData  0.119 ( 0.119)\tLoss 1.8662e-01 (1.8662e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8888e-01 (1.8888e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.837 Acc@5 99.700\n",
            "Epoch: [85][  0/195]\tTime  0.153 ( 0.153)\tData  0.137 ( 0.137)\tLoss 2.4938e-01 (2.4938e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8854e-01 (1.8854e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.817 Acc@5 99.690\n",
            "Epoch: [86][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 2.6578e-01 (2.6578e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 1.9154e-01 (1.9154e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.730\n",
            "Epoch: [87][  0/195]\tTime  0.155 ( 0.155)\tData  0.139 ( 0.139)\tLoss 1.6683e-01 (1.6683e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.8931e-01 (1.8931e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.787 Acc@5 99.710\n",
            "Epoch: [88][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 2.0858e-01 (2.0858e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 1.8962e-01 (1.8962e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.657 Acc@5 99.720\n",
            "Epoch: [89][  0/195]\tTime  0.144 ( 0.144)\tData  0.126 ( 0.126)\tLoss 1.8078e-01 (1.8078e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8909e-01 (1.8909e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.700\n",
            "Epoch: [90][  0/195]\tTime  0.144 ( 0.144)\tData  0.127 ( 0.127)\tLoss 2.1589e-01 (2.1589e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.8982e-01 (1.8982e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.697 Acc@5 99.730\n",
            "Epoch: [91][  0/195]\tTime  0.142 ( 0.142)\tData  0.126 ( 0.126)\tLoss 1.6571e-01 (1.6571e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.8844e-01 (1.8844e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.727 Acc@5 99.710\n",
            "Epoch: [92][  0/195]\tTime  0.142 ( 0.142)\tData  0.125 ( 0.125)\tLoss 1.8254e-01 (1.8254e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.121 ( 0.121)\tLoss 1.8657e-01 (1.8657e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.777 Acc@5 99.690\n",
            "Epoch: [93][  0/195]\tTime  0.142 ( 0.142)\tData  0.124 ( 0.124)\tLoss 1.7555e-01 (1.7555e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.8876e-01 (1.8876e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.777 Acc@5 99.710\n",
            "Epoch: [94][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 2.3820e-01 (2.3820e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 1.8990e-01 (1.8990e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.817 Acc@5 99.720\n",
            "Epoch: [95][  0/195]\tTime  0.140 ( 0.140)\tData  0.123 ( 0.123)\tLoss 1.7209e-01 (1.7209e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8940e-01 (1.8940e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.657 Acc@5 99.710\n",
            "Epoch: [96][  0/195]\tTime  0.153 ( 0.153)\tData  0.136 ( 0.136)\tLoss 2.0612e-01 (2.0612e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8991e-01 (1.8991e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.710\n",
            "Epoch: [97][  0/195]\tTime  0.131 ( 0.131)\tData  0.115 ( 0.115)\tLoss 2.0003e-01 (2.0003e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.9022e-01 (1.9022e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.827 Acc@5 99.720\n",
            "Epoch: [98][  0/195]\tTime  0.134 ( 0.134)\tData  0.119 ( 0.119)\tLoss 1.7732e-01 (1.7732e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.8987e-01 (1.8987e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.710\n",
            "Epoch: [99][  0/195]\tTime  0.143 ( 0.143)\tData  0.127 ( 0.127)\tLoss 2.2380e-01 (2.2380e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.8895e-01 (1.8895e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.817 Acc@5 99.690\n",
            "Epoch: [100][  0/195]\tTime  0.155 ( 0.155)\tData  0.139 ( 0.139)\tLoss 2.0026e-01 (2.0026e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.8906e-01 (1.8906e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.797 Acc@5 99.679\n",
            "Epoch: [101][  0/195]\tTime  0.135 ( 0.135)\tData  0.119 ( 0.119)\tLoss 2.5061e-01 (2.5061e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.8896e-01 (1.8896e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.677 Acc@5 99.720\n",
            "Epoch: [102][  0/195]\tTime  0.141 ( 0.141)\tData  0.124 ( 0.124)\tLoss 2.3097e-01 (2.3097e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8968e-01 (1.8968e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.737 Acc@5 99.720\n",
            "Epoch: [103][  0/195]\tTime  0.159 ( 0.159)\tData  0.143 ( 0.143)\tLoss 2.3745e-01 (2.3745e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8857e-01 (1.8857e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.737 Acc@5 99.679\n",
            "Epoch: [104][  0/195]\tTime  0.142 ( 0.142)\tData  0.127 ( 0.127)\tLoss 1.8226e-01 (1.8226e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.135 ( 0.135)\tLoss 1.8854e-01 (1.8854e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.720\n",
            "Epoch: [105][  0/195]\tTime  0.138 ( 0.138)\tData  0.121 ( 0.121)\tLoss 1.5066e-01 (1.5066e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 1.8963e-01 (1.8963e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.737 Acc@5 99.710\n",
            "Epoch: [106][  0/195]\tTime  0.148 ( 0.148)\tData  0.130 ( 0.130)\tLoss 1.8509e-01 (1.8509e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.8993e-01 (1.8993e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.720\n",
            "Epoch: [107][  0/195]\tTime  0.154 ( 0.154)\tData  0.139 ( 0.139)\tLoss 2.3091e-01 (2.3091e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.135 ( 0.135)\tLoss 1.9064e-01 (1.9064e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.700\n",
            "Epoch: [108][  0/195]\tTime  0.130 ( 0.130)\tData  0.115 ( 0.115)\tLoss 2.1393e-01 (2.1393e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8901e-01 (1.8901e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.767 Acc@5 99.690\n",
            "Epoch: [109][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 1.6335e-01 (1.6335e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.135 ( 0.135)\tLoss 1.9062e-01 (1.9062e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.837 Acc@5 99.720\n",
            "Epoch: [110][  0/195]\tTime  0.154 ( 0.154)\tData  0.137 ( 0.137)\tLoss 1.4612e-01 (1.4612e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.120 ( 0.120)\tLoss 1.9061e-01 (1.9061e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.807 Acc@5 99.730\n",
            "Epoch: [111][  0/195]\tTime  0.144 ( 0.144)\tData  0.126 ( 0.126)\tLoss 2.0593e-01 (2.0593e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8938e-01 (1.8938e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.710\n",
            "Epoch: [112][  0/195]\tTime  0.139 ( 0.139)\tData  0.122 ( 0.122)\tLoss 2.1127e-01 (2.1127e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.8859e-01 (1.8859e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.817 Acc@5 99.690\n",
            "Epoch: [113][  0/195]\tTime  0.140 ( 0.140)\tData  0.123 ( 0.123)\tLoss 1.1992e-01 (1.1992e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.8914e-01 (1.8914e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.827 Acc@5 99.710\n",
            "Epoch: [114][  0/195]\tTime  0.141 ( 0.141)\tData  0.124 ( 0.124)\tLoss 1.7534e-01 (1.7534e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8937e-01 (1.8937e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.817 Acc@5 99.720\n",
            "Epoch: [115][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 2.1786e-01 (2.1786e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.8931e-01 (1.8931e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.777 Acc@5 99.700\n",
            "Epoch: [116][  0/195]\tTime  0.142 ( 0.142)\tData  0.126 ( 0.126)\tLoss 2.4858e-01 (2.4858e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.151 ( 0.151)\tLoss 1.8977e-01 (1.8977e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.837 Acc@5 99.710\n",
            "Epoch: [117][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 2.0886e-01 (2.0886e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.8987e-01 (1.8987e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.720\n",
            "Epoch: [118][  0/195]\tTime  0.150 ( 0.150)\tData  0.133 ( 0.133)\tLoss 1.6789e-01 (1.6789e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.8972e-01 (1.8972e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.657 Acc@5 99.710\n",
            "Epoch: [119][  0/195]\tTime  0.141 ( 0.141)\tData  0.124 ( 0.124)\tLoss 1.9230e-01 (1.9230e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.147 ( 0.147)\tLoss 1.8911e-01 (1.8911e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.697 Acc@5 99.690\n",
            "Epoch: [120][  0/195]\tTime  0.152 ( 0.152)\tData  0.135 ( 0.135)\tLoss 2.1107e-01 (2.1107e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.8785e-01 (1.8785e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.787 Acc@5 99.700\n",
            "Epoch: [121][  0/195]\tTime  0.176 ( 0.176)\tData  0.161 ( 0.161)\tLoss 2.7624e-01 (2.7624e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.9043e-01 (1.9043e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.730\n",
            "Epoch: [122][  0/195]\tTime  0.143 ( 0.143)\tData  0.127 ( 0.127)\tLoss 2.1090e-01 (2.1090e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 1.8989e-01 (1.8989e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.710\n",
            "Epoch: [123][  0/195]\tTime  0.169 ( 0.169)\tData  0.152 ( 0.152)\tLoss 2.4198e-01 (2.4198e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.9123e-01 (1.9123e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.637 Acc@5 99.710\n",
            "Epoch: [124][  0/195]\tTime  0.173 ( 0.173)\tData  0.157 ( 0.157)\tLoss 2.1297e-01 (2.1297e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  98.83 ( 98.83)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.9017e-01 (1.9017e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.687 Acc@5 99.720\n",
            "Epoch: [125][  0/195]\tTime  0.135 ( 0.135)\tData  0.118 ( 0.118)\tLoss 2.1674e-01 (2.1674e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 1.8940e-01 (1.8940e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.607 Acc@5 99.720\n",
            "Epoch: [126][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 1.9576e-01 (1.9576e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.121 ( 0.121)\tLoss 1.9217e-01 (1.9217e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.730\n",
            "Epoch: [127][  0/195]\tTime  0.133 ( 0.133)\tData  0.116 ( 0.116)\tLoss 2.1646e-01 (2.1646e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.119 ( 0.119)\tLoss 1.8869e-01 (1.8869e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.767 Acc@5 99.700\n",
            "Epoch: [128][  0/195]\tTime  0.162 ( 0.162)\tData  0.146 ( 0.146)\tLoss 2.3630e-01 (2.3630e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8845e-01 (1.8845e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.827 Acc@5 99.679\n",
            "Epoch: [129][  0/195]\tTime  0.142 ( 0.142)\tData  0.125 ( 0.125)\tLoss 1.9189e-01 (1.9189e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8746e-01 (1.8746e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.690\n",
            "Epoch: [130][  0/195]\tTime  0.142 ( 0.142)\tData  0.125 ( 0.125)\tLoss 1.3223e-01 (1.3223e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9033e-01 (1.9033e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.607 Acc@5 99.720\n",
            "Epoch: [131][  0/195]\tTime  0.156 ( 0.156)\tData  0.139 ( 0.139)\tLoss 1.8618e-01 (1.8618e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.9039e-01 (1.9039e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.777 Acc@5 99.720\n",
            "Epoch: [132][  0/195]\tTime  0.141 ( 0.141)\tData  0.124 ( 0.124)\tLoss 2.2889e-01 (2.2889e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 1.8993e-01 (1.8993e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.697 Acc@5 99.710\n",
            "Epoch: [133][  0/195]\tTime  0.146 ( 0.146)\tData  0.130 ( 0.130)\tLoss 1.4687e-01 (1.4687e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 1.8759e-01 (1.8759e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.679\n",
            "Epoch: [134][  0/195]\tTime  0.150 ( 0.150)\tData  0.133 ( 0.133)\tLoss 1.6123e-01 (1.6123e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8912e-01 (1.8912e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.717 Acc@5 99.700\n",
            "Epoch: [135][  0/195]\tTime  0.197 ( 0.197)\tData  0.180 ( 0.180)\tLoss 2.0359e-01 (2.0359e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9038e-01 (1.9038e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.807 Acc@5 99.730\n",
            "Epoch: [136][  0/195]\tTime  0.155 ( 0.155)\tData  0.138 ( 0.138)\tLoss 1.7183e-01 (1.7183e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.9030e-01 (1.9030e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.697 Acc@5 99.710\n",
            "Epoch: [137][  0/195]\tTime  0.142 ( 0.142)\tData  0.126 ( 0.126)\tLoss 2.8743e-01 (2.8743e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8987e-01 (1.8987e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.737 Acc@5 99.700\n",
            "Epoch: [138][  0/195]\tTime  0.135 ( 0.135)\tData  0.118 ( 0.118)\tLoss 1.7592e-01 (1.7592e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.9116e-01 (1.9116e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.657 Acc@5 99.710\n",
            "Epoch: [139][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 1.9049e-01 (1.9049e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.8811e-01 (1.8811e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.737 Acc@5 99.720\n",
            "Epoch: [140][  0/195]\tTime  0.147 ( 0.147)\tData  0.131 ( 0.131)\tLoss 3.6372e-01 (3.6372e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8801e-01 (1.8801e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.677 Acc@5 99.700\n",
            "Epoch: [141][  0/195]\tTime  0.144 ( 0.144)\tData  0.128 ( 0.128)\tLoss 2.2775e-01 (2.2775e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8724e-01 (1.8724e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.697 Acc@5 99.679\n",
            "Epoch: [142][  0/195]\tTime  0.142 ( 0.142)\tData  0.126 ( 0.126)\tLoss 1.7298e-01 (1.7298e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 1.8921e-01 (1.8921e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.807 Acc@5 99.690\n",
            "Epoch: [143][  0/195]\tTime  0.135 ( 0.135)\tData  0.119 ( 0.119)\tLoss 2.0839e-01 (2.0839e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.9035e-01 (1.9035e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.717 Acc@5 99.720\n",
            "Epoch: [144][  0/195]\tTime  0.176 ( 0.176)\tData  0.159 ( 0.159)\tLoss 1.9754e-01 (1.9754e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 1.8848e-01 (1.8848e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.700\n",
            "Epoch: [145][  0/195]\tTime  0.137 ( 0.137)\tData  0.121 ( 0.121)\tLoss 2.6046e-01 (2.6046e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.134 ( 0.134)\tLoss 1.8985e-01 (1.8985e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.720\n",
            "Epoch: [146][  0/195]\tTime  0.146 ( 0.146)\tData  0.130 ( 0.130)\tLoss 2.1486e-01 (2.1486e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8915e-01 (1.8915e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.687 Acc@5 99.720\n",
            "Epoch: [147][  0/195]\tTime  0.153 ( 0.153)\tData  0.137 ( 0.137)\tLoss 2.6757e-01 (2.6757e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9063e-01 (1.9063e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.710\n",
            "Epoch: [148][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 1.3738e-01 (1.3738e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.121 ( 0.121)\tLoss 1.8964e-01 (1.8964e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.727 Acc@5 99.720\n",
            "Epoch: [149][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 2.7918e-01 (2.7918e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8900e-01 (1.8900e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.717 Acc@5 99.700\n",
            "Epoch: [150][  0/195]\tTime  0.152 ( 0.152)\tData  0.135 ( 0.135)\tLoss 2.6024e-01 (2.6024e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8910e-01 (1.8910e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.797 Acc@5 99.700\n",
            "Epoch: [151][  0/195]\tTime  0.136 ( 0.136)\tData  0.120 ( 0.120)\tLoss 2.3328e-01 (2.3328e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8787e-01 (1.8787e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.710\n",
            "Epoch: [152][  0/195]\tTime  0.150 ( 0.150)\tData  0.133 ( 0.133)\tLoss 2.3360e-01 (2.3360e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8812e-01 (1.8812e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.677 Acc@5 99.720\n",
            "Epoch: [153][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 1.5886e-01 (1.5886e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.9052e-01 (1.9052e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.677 Acc@5 99.730\n",
            "Epoch: [154][  0/195]\tTime  0.147 ( 0.147)\tData  0.131 ( 0.131)\tLoss 2.9819e-01 (2.9819e-01)\tAcc@1  90.23 ( 90.23)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 1.8963e-01 (1.8963e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.787 Acc@5 99.700\n",
            "Epoch: [155][  0/195]\tTime  0.137 ( 0.137)\tData  0.121 ( 0.121)\tLoss 2.2260e-01 (2.2260e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.9096e-01 (1.9096e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.647 Acc@5 99.720\n",
            "Epoch: [156][  0/195]\tTime  0.141 ( 0.141)\tData  0.124 ( 0.124)\tLoss 1.9786e-01 (1.9786e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.8925e-01 (1.8925e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.727 Acc@5 99.730\n",
            "Epoch: [157][  0/195]\tTime  0.153 ( 0.153)\tData  0.137 ( 0.137)\tLoss 2.4203e-01 (2.4203e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.8937e-01 (1.8937e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.710\n",
            "Epoch: [158][  0/195]\tTime  0.143 ( 0.143)\tData  0.126 ( 0.126)\tLoss 1.3469e-01 (1.3469e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.9048e-01 (1.9048e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.687 Acc@5 99.720\n",
            "Epoch: [159][  0/195]\tTime  0.146 ( 0.146)\tData  0.129 ( 0.129)\tLoss 1.6359e-01 (1.6359e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8830e-01 (1.8830e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.797 Acc@5 99.700\n",
            "Epoch: [160][  0/195]\tTime  0.140 ( 0.140)\tData  0.123 ( 0.123)\tLoss 2.5195e-01 (2.5195e-01)\tAcc@1  91.80 ( 91.80)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.8793e-01 (1.8793e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.637 Acc@5 99.720\n",
            "Epoch: [161][  0/195]\tTime  0.149 ( 0.149)\tData  0.133 ( 0.133)\tLoss 2.2543e-01 (2.2543e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8742e-01 (1.8742e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.807 Acc@5 99.690\n",
            "Epoch: [162][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 1.5931e-01 (1.5931e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.9029e-01 (1.9029e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.817 Acc@5 99.730\n",
            "Epoch: [163][  0/195]\tTime  0.150 ( 0.150)\tData  0.134 ( 0.134)\tLoss 2.1474e-01 (2.1474e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8989e-01 (1.8989e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.710\n",
            "Epoch: [164][  0/195]\tTime  0.163 ( 0.163)\tData  0.146 ( 0.146)\tLoss 1.8369e-01 (1.8369e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.9019e-01 (1.9019e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.730\n",
            "Epoch: [165][  0/195]\tTime  0.147 ( 0.147)\tData  0.130 ( 0.130)\tLoss 2.7264e-01 (2.7264e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8982e-01 (1.8982e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.837 Acc@5 99.720\n",
            "Epoch: [166][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 2.5198e-01 (2.5198e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8916e-01 (1.8916e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.787 Acc@5 99.690\n",
            "Epoch: [167][  0/195]\tTime  0.134 ( 0.134)\tData  0.118 ( 0.118)\tLoss 2.4526e-01 (2.4526e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 1.8831e-01 (1.8831e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.677 Acc@5 99.690\n",
            "Epoch: [168][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 2.1271e-01 (2.1271e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.8908e-01 (1.8908e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.727 Acc@5 99.690\n",
            "Epoch: [169][  0/195]\tTime  0.147 ( 0.147)\tData  0.130 ( 0.130)\tLoss 1.9849e-01 (1.9849e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.132 ( 0.132)\tLoss 1.8949e-01 (1.8949e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.787 Acc@5 99.720\n",
            "Epoch: [170][  0/195]\tTime  0.135 ( 0.135)\tData  0.119 ( 0.119)\tLoss 2.5117e-01 (2.5117e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 1.9096e-01 (1.9096e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.720\n",
            "Epoch: [171][  0/195]\tTime  0.138 ( 0.138)\tData  0.121 ( 0.121)\tLoss 1.5311e-01 (1.5311e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.140 ( 0.140)\tLoss 1.9064e-01 (1.9064e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.857 Acc@5 99.720\n",
            "Epoch: [172][  0/195]\tTime  0.137 ( 0.137)\tData  0.121 ( 0.121)\tLoss 2.2416e-01 (2.2416e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.9092e-01 (1.9092e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.717 Acc@5 99.720\n",
            "Epoch: [173][  0/195]\tTime  0.138 ( 0.138)\tData  0.122 ( 0.122)\tLoss 1.5293e-01 (1.5293e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8871e-01 (1.8871e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.720\n",
            "Epoch: [174][  0/195]\tTime  0.145 ( 0.145)\tData  0.128 ( 0.128)\tLoss 2.3470e-01 (2.3470e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.8809e-01 (1.8809e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.737 Acc@5 99.690\n",
            "Epoch: [175][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 1.8173e-01 (1.8173e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.129 ( 0.129)\tLoss 1.8804e-01 (1.8804e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.657 Acc@5 99.710\n",
            "Epoch: [176][  0/195]\tTime  0.143 ( 0.143)\tData  0.126 ( 0.126)\tLoss 2.1046e-01 (2.1046e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.8830e-01 (1.8830e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.667 Acc@5 99.679\n",
            "Epoch: [177][  0/195]\tTime  0.136 ( 0.136)\tData  0.119 ( 0.119)\tLoss 2.1248e-01 (2.1248e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8958e-01 (1.8958e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.727 Acc@5 99.690\n",
            "Epoch: [178][  0/195]\tTime  0.164 ( 0.164)\tData  0.148 ( 0.148)\tLoss 1.8076e-01 (1.8076e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 1.8826e-01 (1.8826e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.679\n",
            "Epoch: [179][  0/195]\tTime  0.144 ( 0.144)\tData  0.127 ( 0.127)\tLoss 2.3093e-01 (2.3093e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.8905e-01 (1.8905e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.690\n",
            "Epoch: [180][  0/195]\tTime  0.155 ( 0.155)\tData  0.139 ( 0.139)\tLoss 2.2865e-01 (2.2865e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9093e-01 (1.9093e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.717 Acc@5 99.710\n",
            "Epoch: [181][  0/195]\tTime  0.156 ( 0.156)\tData  0.140 ( 0.140)\tLoss 2.1063e-01 (2.1063e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8904e-01 (1.8904e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.777 Acc@5 99.690\n",
            "Epoch: [182][  0/195]\tTime  0.152 ( 0.152)\tData  0.136 ( 0.136)\tLoss 2.0497e-01 (2.0497e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.128 ( 0.128)\tLoss 1.8899e-01 (1.8899e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.777 Acc@5 99.700\n",
            "Epoch: [183][  0/195]\tTime  0.143 ( 0.143)\tData  0.127 ( 0.127)\tLoss 1.7225e-01 (1.7225e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.125 ( 0.125)\tLoss 1.8941e-01 (1.8941e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.687 Acc@5 99.690\n",
            "Epoch: [184][  0/195]\tTime  0.136 ( 0.136)\tData  0.120 ( 0.120)\tLoss 2.2852e-01 (2.2852e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.129 ( 0.129)\tLoss 1.8772e-01 (1.8772e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.677 Acc@5 99.690\n",
            "Epoch: [185][  0/195]\tTime  0.136 ( 0.136)\tData  0.119 ( 0.119)\tLoss 1.8237e-01 (1.8237e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.121 ( 0.121)\tLoss 1.8965e-01 (1.8965e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.647 Acc@5 99.710\n",
            "Epoch: [186][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 1.5358e-01 (1.5358e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.120 ( 0.120)\tLoss 1.9014e-01 (1.9014e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.700\n",
            "Epoch: [187][  0/195]\tTime  0.149 ( 0.149)\tData  0.133 ( 0.133)\tLoss 2.2280e-01 (2.2280e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.9005e-01 (1.9005e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.700\n",
            "Epoch: [188][  0/195]\tTime  0.149 ( 0.149)\tData  0.133 ( 0.133)\tLoss 2.3908e-01 (2.3908e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.8923e-01 (1.8923e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.797 Acc@5 99.700\n",
            "Epoch: [189][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 1.6520e-01 (1.6520e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.140 ( 0.140)\tLoss 1.9029e-01 (1.9029e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.720\n",
            "Epoch: [190][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 1.8852e-01 (1.8852e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.8938e-01 (1.8938e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.717 Acc@5 99.710\n",
            "Epoch: [191][  0/195]\tTime  0.162 ( 0.162)\tData  0.146 ( 0.146)\tLoss 1.7562e-01 (1.7562e-01)\tAcc@1  94.92 ( 94.92)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.8721e-01 (1.8721e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.679\n",
            "Epoch: [192][  0/195]\tTime  0.153 ( 0.153)\tData  0.136 ( 0.136)\tLoss 2.0515e-01 (2.0515e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.130 ( 0.130)\tLoss 1.8925e-01 (1.8925e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.707 Acc@5 99.720\n",
            "Epoch: [193][  0/195]\tTime  0.150 ( 0.150)\tData  0.134 ( 0.134)\tLoss 2.1618e-01 (2.1618e-01)\tAcc@1  92.58 ( 92.58)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.124 ( 0.124)\tLoss 1.9094e-01 (1.9094e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.697 Acc@5 99.730\n",
            "Epoch: [194][  0/195]\tTime  0.154 ( 0.154)\tData  0.138 ( 0.138)\tLoss 2.1568e-01 (2.1568e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.126 ( 0.126)\tLoss 1.9057e-01 (1.9057e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.807 Acc@5 99.720\n",
            "Epoch: [195][  0/195]\tTime  0.141 ( 0.141)\tData  0.125 ( 0.125)\tLoss 2.1036e-01 (2.1036e-01)\tAcc@1  94.14 ( 94.14)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.127 ( 0.127)\tLoss 1.9061e-01 (1.9061e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.767 Acc@5 99.710\n",
            "Epoch: [196][  0/195]\tTime  0.139 ( 0.139)\tData  0.123 ( 0.123)\tLoss 2.2817e-01 (2.2817e-01)\tAcc@1  91.02 ( 91.02)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.133 ( 0.133)\tLoss 1.9069e-01 (1.9069e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.767 Acc@5 99.710\n",
            "Epoch: [197][  0/195]\tTime  0.138 ( 0.138)\tData  0.121 ( 0.121)\tLoss 1.2581e-01 (1.2581e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.61 ( 99.61)\n",
            "Test: [ 0/39]\tTime  0.122 ( 0.122)\tLoss 1.8839e-01 (1.8839e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.787 Acc@5 99.710\n",
            "Epoch: [198][  0/195]\tTime  0.137 ( 0.137)\tData  0.120 ( 0.120)\tLoss 1.8380e-01 (1.8380e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n",
            "Test: [ 0/39]\tTime  0.123 ( 0.123)\tLoss 1.9009e-01 (1.9009e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.757 Acc@5 99.730\n",
            "Epoch: [199][  0/195]\tTime  0.153 ( 0.153)\tData  0.137 ( 0.137)\tLoss 2.9540e-01 (2.9540e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n",
            "Test: [ 0/39]\tTime  0.131 ( 0.131)\tLoss 1.8879e-01 (1.8879e-01)\tAcc@1  93.36 ( 93.36)\tAcc@5  99.61 ( 99.61)\n",
            " * Acc@1 91.747 Acc@5 99.700\n",
            "Best acc: tensor(91.8570, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import argparse\n",
        "import builtins\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "best_acc1_list = []\n",
        "epoch_list = []\n",
        "\n",
        "\n",
        "\n",
        "model_names = sorted(name for name in models.__dict__\n",
        "    if name.islower() and not name.startswith(\"__\")\n",
        "    and callable(models.__dict__[name]))\n",
        "\n",
        "\n",
        "\n",
        "def get_backbone(backbone_name, num_cls=10):\n",
        "    models = {'resnet18': ResNet18(low_dim=num_cls),\n",
        "              'resnet34': ResNet34(low_dim=num_cls),\n",
        "              'resnet50': ResNet50(low_dim=num_cls),\n",
        "              'resnet101': ResNet101(low_dim=num_cls),\n",
        "              'resnet152': ResNet152(low_dim=num_cls)}\n",
        "\n",
        "    return models[backbone_name]\n",
        "\n",
        "\n",
        "best_acc1 = 0\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to initiate training or evaluation.\"\"\"\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "        warnings.warn('You have chosen to seed training. '\n",
        "                      'This will turn on the CUDNN deterministic setting, '\n",
        "                      'which can slow down your training considerably! '\n",
        "                      'You may see unexpected behavior when restarting '\n",
        "                      'from checkpoints.')\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
        "                      'disable data parallelism.')\n",
        "\n",
        "    # Configure distributed training if applicable\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "    if args.multiprocessing_distributed:\n",
        "        # Since we have ngpus_per_node processes per node, the total world_size\n",
        "        # needs to be adjusted accordingly\n",
        "        args.world_size = ngpus_per_node * args.world_size\n",
        "        # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
        "        # main_worker process function\n",
        "        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
        "    else:\n",
        "        # Simply call main_worker function\n",
        "        main_worker(args.gpu, ngpus_per_node, args)\n",
        "\n",
        "\n",
        "def main_worker(gpu, ngpus_per_node, args):\n",
        "    \"\"\"Worker function for handling model creation and training.\"\"\"\n",
        "    global best_acc1\n",
        "    args.gpu = gpu\n",
        "\n",
        "    # suppress printing if not master\n",
        "    if args.multiprocessing_distributed and args.gpu != 0:\n",
        "        def print_pass(*args):\n",
        "            pass\n",
        "        builtins.print = print_pass\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    # Initialize distributed training\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            # For multiprocessing distributed training, rank needs to be the\n",
        "            # global rank among all the processes\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "    # create model\n",
        "    print(\"=> creating model '{}'\".format(args.arch))\n",
        "    # model = models.__dict__[args.arch]()\n",
        "    model = get_backbone(args.arch, args.num_cls)\n",
        "\n",
        "    # Freeze all layers except the last fully connected layer\n",
        "    for name, param in model.named_parameters():\n",
        "        if name not in ['fc.weight', 'fc.bias']:\n",
        "            param.requires_grad = False\n",
        "    # init the fc layer\n",
        "    model.fc.weight.data.normal_(mean=0.0, std=0.01)\n",
        "    model.fc.bias.data.zero_()\n",
        "\n",
        "    # Load pre-trained weights if provided\n",
        "    if args.pretrained:\n",
        "        if os.path.isfile(args.pretrained):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n",
        "            checkpoint = torch.load(args.pretrained, map_location=\"cpu\")\n",
        "            state_dict = checkpoint['state_dict']\n",
        "\n",
        "            # Adapt checkpoint state dict for backbone model\n",
        "            new_state_dict = dict()\n",
        "            for old_key, value in state_dict.items():\n",
        "                if old_key.startswith('backbone') and 'fc' not in old_key:\n",
        "                    new_key = old_key.replace('backbone.', '')\n",
        "                    new_state_dict[new_key] = value\n",
        "\n",
        "            args.start_epoch = 0\n",
        "            msg = model.load_state_dict(new_state_dict, strict=False)\n",
        "            assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n",
        "\n",
        "            print(\"=> loaded pre-trained model '{}'\".format(args.pretrained))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n",
        "\n",
        "    if args.distributed:\n",
        "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
        "        # should always set the single device scope, otherwise,\n",
        "        # DistributedDataParallel will use all available devices.\n",
        "        if args.gpu is not None:\n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            model.cuda(args.gpu)\n",
        "            # When using a single GPU per process and per\n",
        "            # DistributedDataParallel, we need to divide the batch size\n",
        "            # ourselves based on the total number of GPUs we have\n",
        "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
        "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        else:\n",
        "            model.cuda()\n",
        "            # DistributedDataParallel will divide and allocate batch_size to all\n",
        "            # available GPUs if device_ids are not set\n",
        "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        model = model.cuda(args.gpu)\n",
        "    else:\n",
        "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
        "        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
        "            model.features = torch.nn.DataParallel(model.features)\n",
        "            model.cuda()\n",
        "        else:\n",
        "            model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
        "\n",
        "    # optimize only the linear classifier\n",
        "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    assert len(parameters) == 2  # fc.weight, fc.bias\n",
        "    optimizer = torch.optim.SGD(parameters, args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay)\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "            if args.gpu is None:\n",
        "                checkpoint = torch.load(args.resume)\n",
        "            else:\n",
        "                # Map model to be loaded to specified single gpu.\n",
        "                loc = 'cuda:{}'.format(args.gpu)\n",
        "                checkpoint = torch.load(args.resume, map_location=loc)\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            best_acc1 = checkpoint['best_acc1']\n",
        "            if args.gpu is not None:\n",
        "                # best_acc1 may be from a checkpoint from a different GPU\n",
        "                best_acc1 = best_acc1.to(args.gpu)\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(args.resume, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Data loading code\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(32, scale=(0.8, 1.0),\n",
        "                                     ratio=(3.0 / 4.0, 4.0 / 3.0),\n",
        "                                     interpolation=Image.BICUBIC),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize(int(32 * (8 / 7)), interpolation=Image.BICUBIC),\n",
        "        transforms.CenterCrop(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10(args.data, train=True, transform=transform_train)\n",
        "    valset = datasets.CIFAR10(args.data, train=False, transform=transform_test)\n",
        "\n",
        "    if args.distributed:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n",
        "    else:\n",
        "        train_sampler = None\n",
        "\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=args.batch_size,\n",
        "                              shuffle=(train_sampler is None),\n",
        "                              num_workers=args.workers,\n",
        "                              sampler=train_sampler,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "    val_loader = DataLoader(valset,\n",
        "                            batch_size=args.batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=args.workers,\n",
        "                            pin_memory=True,\n",
        "                            drop_last=True)\n",
        "\n",
        "    if args.evaluate:\n",
        "        validate(val_loader, model, criterion, args)\n",
        "        return\n",
        "\n",
        "    # Training and validation loop\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        if args.distributed:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "        adjust_learning_rate(optimizer, epoch, args)\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, criterion, optimizer, epoch, args)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        acc1 = validate(val_loader, model, criterion, args)\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = acc1 > best_acc1\n",
        "        best_acc1 = max(acc1, best_acc1)\n",
        "        best_acc1_list.append(best_acc1)\n",
        "        epoch_list.append(epoch)\n",
        "\n",
        "        if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
        "                and args.rank % ngpus_per_node == 0):\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'arch': args.arch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_acc1': best_acc1,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, is_best)\n",
        "            if epoch == args.start_epoch:\n",
        "                sanity_check(model.state_dict(), args.pretrained)\n",
        "\n",
        "    print('Best acc:', best_acc1)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time, losses, top1, top5],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    \"\"\"\n",
        "    Switch to eval mode:\n",
        "    Under the protocol of linear classification on frozen features/models,\n",
        "    it is not legitimate to change any part of the pre-trained model.\n",
        "    BatchNorm in train mode may revise running mean/std (even if it receives\n",
        "    no gradient), which are part of the model parameters too.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        if args.gpu is not None:\n",
        "            images = images.cuda(args.gpu, non_blocking=True)\n",
        "        target = target.cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i)\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader),\n",
        "        [batch_time, losses, top1, top5],\n",
        "        prefix='Test: ')\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            if args.gpu is not None:\n",
        "                images = images.cuda(args.gpu, non_blocking=True)\n",
        "            target = target.cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "            # compute output\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            # loss_list.append(loss)\n",
        "\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "        # TODO: this should also be done with the ProgressMeter\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "# Save the model if it's the best so far\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "\n",
        "def sanity_check(state_dict, pretrained_weights):\n",
        "    \"\"\"\n",
        "    Linear classifier should not change any weights other than the linear layer.\n",
        "    This sanity check asserts nothing wrong happens (e.g., BN stats updated).\n",
        "    \"\"\"\n",
        "    print(\"=> loading '{}' for sanity check\".format(pretrained_weights))\n",
        "    checkpoint = torch.load(pretrained_weights, map_location=\"cpu\")\n",
        "    state_dict_pre = checkpoint['state_dict']\n",
        "\n",
        "    for k in list(state_dict.keys()):\n",
        "        # only ignore fc layer\n",
        "        if 'fc.weight' in k or 'fc.bias' in k:\n",
        "            continue\n",
        "\n",
        "        k_pre = 'backbone.' + k[len('module.'):] \\\n",
        "            if k.startswith('module.') else 'backbone.' + k\n",
        "\n",
        "        assert ((state_dict[k].cpu() == state_dict_pre[k_pre]).all()), \\\n",
        "            '{} is changed in linear classifier training.'.format(k)\n",
        "\n",
        "    print(\"=> sanity check passed.\")\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "    lr = args.lr\n",
        "    for milestone in args.schedule:\n",
        "        lr *= 0.1 if epoch >= milestone else 1.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1O4E-SnO09m",
        "outputId": "6096716e-d55d-4abb-a8b1-1549ca00fad1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert best accuracy list from GPU tensors to CPU float values for plotting\n",
        "best_acc1_list = [float(acc.cpu()) for acc in best_acc1_list]\n",
        "\n",
        "# Plotting the Best Accuracy Over Epochs\n",
        "plt.figure(figsize=(10, 5))  # Set figure size for better visualization\n",
        "plt.plot(epoch_list, best_acc1_list, label='Best Acc@1', marker=',')  # Plot accuracy values\n",
        "plt.xlabel('Epochs')  # Label for the x-axis\n",
        "plt.ylabel('Accuracy')  # Label for the y-axis\n",
        "plt.title('Best Accuracy Over Epochs')  # Title of the plot\n",
        "plt.legend()  # Add legend to indicate what the plot represents\n",
        "plt.grid(True)  # Enable grid for easier value interpretation\n",
        "plt.savefig('best_acc1_plot.png')  # Save the plot as a PNG file\n",
        "plt.show()  # Display the plot"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}