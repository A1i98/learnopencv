{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3mD91mNTRBJ"
   },
   "source": [
    "# Dependencies and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQ9SxiV2QRi5"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/tencent/HunyuanVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-19OAWkTfvb"
   },
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install -r requirements.txt\n",
    "!pip install ninja\n",
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLPoNu35UKbz"
   },
   "outputs": [],
   "source": [
    "%cd HunyuanVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BPjTv1CTXQa"
   },
   "source": [
    "# Download Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30qMXAvsT_li"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login #run this command in terminal and provide your huggingface token there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TPocO81UJDU"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download tencent/HunyuanVideo --local-dir ./ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GjEfTu5UTWM"
   },
   "outputs": [],
   "source": [
    "%cd HunyuanVideo/ckpts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PXt_XBSUZOv"
   },
   "source": [
    "As Hunyuan has not yet released their own MLLM(Multimodal Large Language Model), hence in their official repo they instructed to download llava-llama-3.8b transformers model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcZqPj_DUXXA"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download xtuner/llava-llama-3-8b-v1_1-transformers --local-dir ./llava-llama-3-8b-v1_1-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zlmvDiiU3OE"
   },
   "source": [
    "In order to save GPU memory resources for model loading, the authors in the official repo told to separate the language model parts of llava-llama-3-8b-v1_1-transformers into text_encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xPQEMyMU1C-"
   },
   "outputs": [],
   "source": [
    "%cd HunyuanVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnz5Akz8VCjf"
   },
   "outputs": [],
   "source": [
    "!python hyvideo/utils/preprocess_text_encoder_tokenizer_utils.py --input_dir ckpts/llava-llama-3-8b-v1_1-transformers --output_dir ckpts/text_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ5kkxTSVF-g"
   },
   "source": [
    "Hunyuan Video model also uses CLIP model, provided by OpenAI, as another text encoder. Hence, we will be downloading CLIP-ViT-Large-patch14 model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gsp_bTx1VESo"
   },
   "outputs": [],
   "source": [
    "%cd HunyuanVideo/ckpts\n",
    "!huggingface-cli download openai/clip-vit-large-patch14 --local-dir ./text_encoder_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1c9wC5-VqDd"
   },
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UY0O3kzoVpNf"
   },
   "outputs": [],
   "source": [
    "!python3 sample_video.py \\\n",
    "    --video-size 720 1280 \\ # if gpu constraint is present, we recommend you to do inferencing in 544 x 960 pixels with 129 total frames(video length) only.\n",
    "    --video-length 129 \\   # Can be extended. Max video length that we experimented with was 201 (always remember this number should be in 4n +1 format where n is fps) which utilized VRAM upto 69GB for (720, 1280) pixels.\n",
    "    --infer-steps 50 \\\n",
    "    --prompt \"A cat walks on the grass, realistic style.\" \\\n",
    "    --flow-reverse \\\n",
    "    --use-cpu-offload \\\n",
    "    --save-path ./results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xBLdOa3Z-VE"
   },
   "source": [
    "If more reduction in VRAM is required, you can go with the fp8 quantized version which will reduce upto 10GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q-1o50yaQpE"
   },
   "source": [
    "# Perform your own experiments here...\n",
    "## some test prompts are given below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. prompt_1: Under the sunset in the polar region, the sky takes on a warm, reddish-orange hue. The glaciers and snow-covered mountains appear even more serene and majestic under the sunlight. The camera slowly pulls back, revealing a vast expanse of the polar landscape in its entirety.\n",
    "2. prompt_2: A man stands on a bustling street making a phone call, dressed in a gray suit, holding a briefcase, pacing as he speaks on the phone. To his left stands a luxury car. Captured in a full shot.\n",
    "3. prompt_3: A time-lapse video capturing a lively city square from dawn to dusk, with crowds of people, changing light, and shifting shadows over historic architecture.\n",
    "4. prompt_4: Develop a narrative video where a traveler uses an AI-powered city guide to explore multiple cities. The guide offers suggestions based on real-time data and preferences, powered by AI models like recommendation engines and predictive analytics. Show how AI can recommend the best time to visit attractions, local hotspots, and hidden treasures. The video should end with a subtle pitch: â€˜Ready to create your own AI-powered applications? Join OpenCV University to start learning today."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
